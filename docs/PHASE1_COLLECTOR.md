# ğŸ§© FASE I â€” INGESTÃƒO E ARMAZENAMENTO
# ğŸ§± Etapa 1 â€” Coleta e Download (Collector)

## ğŸ“‹ VisÃ£o Geral

Sistema de coleta automatizada de documentos cientÃ­ficos de mÃºltiplas fontes (arXiv, Nature, Science, PubMed, etc.), organizados por origem e data, com persistÃªncia de metadados no banco de dados.

**Status:** âœ… arXiv implementado e funcional  
**LocalizaÃ§Ã£o:** `G:\Hive-Hub\News-main\downloads/<origem>/<YYYY-MM-DD>/`

## ğŸ¯ Objetivo

Conectar-se Ã s APIs das fontes configuradas, baixar arquivos originais (PDFs) e armazenÃ¡-los no sistema local com organizaÃ§Ã£o por origem e data.

## ğŸ”„ Fluxo

```
Scheduler â†’
  Collector Service â†’
    API Request (arXiv/Nature/Science) â†’
      Download PDF â†’
        downloads/<origem>/<YYYY-MM-DD>/<arquivo>.pdf â†’
          Metadados em raw_documents
```

## âœ… Estrutura de Downloads

### DiretÃ³rio Base
**LocalizaÃ§Ã£o:** `G:\Hive-Hub\News-main\downloads`

### OrganizaÃ§Ã£o
```
downloads/
â”œâ”€â”€ arxiv/              â† Origem: arXiv
â”‚   â””â”€â”€ 2025-10-27/
â”‚       â”œâ”€â”€ 2106.01234.pdf
â”‚       â”œâ”€â”€ 2106.01235.pdf
â”‚       â””â”€â”€ ... (10 papers)
â”œâ”€â”€ nature/             â† Origem: Nature
â”‚   â””â”€â”€ 2025-10-27/
â”‚       â””â”€â”€ article.pdf
â”œâ”€â”€ science/            â† Origem: Science
â”‚   â””â”€â”€ 2025-10-27/
â”‚       â””â”€â”€ article.pdf
â””â”€â”€ temp/               â† Arquivos temporÃ¡rios
    â””â”€â”€ arxiv_feed_<timestamp>.xml
```

## ğŸ”§ ImplementaÃ§Ã£o Atual: arXiv

### âœ… Status Implementado

**Fonte:** arXiv (https://arxiv.org)  
**Categoria:** cs.AI (Computer Science - Artificial Intelligence)  
**Quantidade:** 10 artigos mais recentes  
**Chave API:** NÃ£o necessÃ¡ria (API pÃºblica)  
**DiretÃ³rio:** `G:\Hive-Hub\News-main\downloads\arxiv\2025-10-27\`

### Como Funciona

1. **API OAI-PMH do arXiv**
   - URL: `https://export.arxiv.org/api/query`
   - Busca os 10 papers mais recentes de cs.AI
   - Ordena por data de submissÃ£o (mais recentes primeiro)

2. **XML TemporÃ¡rio**
   - Salvo em `downloads/temp/arxiv_feed_<timestamp>.xml`
   - Usado para parsing e auditoria

3. **Downloads PDF**
   - URL: `https://arxiv.org/pdf/<paper_id>.pdf`
   - Salvo em: `downloads/arxiv/<YYYY-MM-DD>/<paper_id>.pdf`

4. **Metadados no Banco**
   - TÃ­tulo, resumo, autores
   - URL do paper e do PDF
   - Timestamp de download

## ğŸ”‘ ConfiguraÃ§Ã£o de Chaves API

### Arquivo de ConfiguraÃ§Ã£o

**LocalizaÃ§Ã£o:** `news-backend/.env`

```env
# Database
DATABASE_URL=postgresql://postgres:postgres@localhost:5432/news_system

# Collector
COLLECTOR_TIMEOUT_SECONDS=30
COLLECTOR_MAX_RETRIES=3
COLLECTOR_DOWNLOAD_DIR=./downloads

# ==========================================
# API Keys for Document Collection Sources
# ==========================================

# arXiv - NO KEY REQUIRED âœ… (Implementado)
ARXIV_BASE_URL=https://export.arxiv.org/api/query
ARXIV_CATEGORY=cs.AI
ARXIV_MAX_RESULTS=10
ARXIV_RATE_LIMIT=120

# Nature Publishing Group
NATURE_API_KEY=your_nature_api_key_here
NATURE_BASE_URL=https://api.nature.com
NATURE_RATE_LIMIT=60

# Science (AAAS)
SCIENCE_API_KEY=your_science_api_key_here
SCIENCE_BASE_URL=https://api.science.org
SCIENCE_RATE_LIMIT=60

# PubMed / NCBI
PUBMED_BASE_URL=https://eutils.ncbi.nlm.nih.gov/entrez/eutils
PUBMED_RATE_LIMIT=100

# IEEE Xplore
IEEE_API_KEY=your_ieee_api_key_here
IEEE_BASE_URL=https://ieeexploreapi.ieee.org/api/v1
IEEE_RATE_LIMIT=60

# Springer Nature
SPRINGER_API_KEY=your_springer_api_key_here
SPRINGER_BASE_URL=https://api.springernature.com
SPRINGER_RATE_LIMIT=60

# Elsevier ScienceDirect
ELSEVIER_API_KEY=your_elsevier_api_key_here
ELSEVIER_BASE_URL=https://api.elsevier.com/content
ELSEVIER_RATE_LIMIT=60

# File Management
TEMP_FILE_RETENTION_DAYS=7
MAX_DOWNLOAD_SIZE_MB=50
```

### Como Configurar

```bash
cd G:\Hive-Hub\News-main\news-backend

# 1. Copiar template
copy .env.example .env

# 2. Editar com suas chaves
notepad .env

# 3. Executar
cargo run
```

## ğŸ“Š Fontes DisponÃ­veis

### âœ… arXiv (IMPLEMENTADO)

**Status:** Funcional  
**Chave API:** NÃ£o necessÃ¡ria  
**Categoria:** cs.AI (configurÃ¡vel)  
**Quantidade:** 10 papers  
**Rate Limit:** 120 requests/min

**Categorias DisponÃ­veis:**
- `cs.AI` - Artificial Intelligence
- `cs.LG` - Machine Learning
- `cs.CV` - Computer Vision
- `cs.NE` - Neural and Evolutionary Computing
- `cs.CL` - Computation and Language
- `stat.ML` - Statistics - Machine Learning

**API:** https://export.arxiv.org/api/query  
**URL PDF:** https://arxiv.org/pdf/\<paper_id\>.pdf  
**DocumentaÃ§Ã£o:** https://arxiv.org/help/api

### â³ Nature Publishing Group

**Status:** Em desenvolvimento  
**Chave API:** NecessÃ¡ria  
**Registro:** https://go.nature.com/api-keys  
**Rate Limit:** 60 requests/min  
**DocumentaÃ§Ã£o:** https://www.nature.com/documents/nr-API-Developer-Guide.pdf

### â³ Science (AAAS)

**Status:** Em desenvolvimento  
**Chave API:** NecessÃ¡ria  
**Registro:** https://science.sciencemag.org/api  
**Rate Limit:** 60 requests/min  
**DocumentaÃ§Ã£o:** https://www.science.org/content/page/science-api

### â³ PubMed / NCBI

**Status:** Planejado  
**Chave API:** NÃ£o necessÃ¡ria  
**Rate Limit:** 100 requests/min  
**DocumentaÃ§Ã£o:** https://www.ncbi.nlm.nih.gov/books/NBK25497/

### â³ IEEE Xplore

**Status:** Planejado  
**Chave API:** NecessÃ¡ria  
**Registro:** https://developer.ieee.org/register  
**Rate Limit:** 60 requests/min  
**DocumentaÃ§Ã£o:** https://developer.ieee.org/

### â³ Springer Nature

**Status:** Planejado  
**Chave API:** NecessÃ¡ria  
**Registro:** https://dev.springernature.com/signup  
**Rate Limit:** 60 requests/min  
**DocumentaÃ§Ã£o:** https://dev.springernature.com/

### â³ Elsevier ScienceDirect

**Status:** Planejado  
**Chave API:** NecessÃ¡ria  
**Registro:** https://dev.elsevier.com/user/login  
**Rate Limit:** 60 requests/min  
**DocumentaÃ§Ã£o:** https://dev.elsevier.com/

## ğŸ“Š Schema do Banco de Dados

```sql
CREATE TABLE raw_documents (
    id SERIAL PRIMARY KEY,
    portal_id INT REFERENCES pages_config(id) ON DELETE CASCADE,
    title TEXT NOT NULL,
    source_url TEXT NOT NULL,
    file_path TEXT NOT NULL,
    file_type TEXT NOT NULL,
    file_size BIGINT,
    metadata JSONB DEFAULT '{}',
    downloaded_at TIMESTAMP DEFAULT NOW(),
    processed BOOLEAN DEFAULT false,
    created_at TIMESTAMP DEFAULT NOW()
);

CREATE INDEX idx_raw_documents_portal_id ON raw_documents(portal_id);
CREATE INDEX idx_raw_documents_processed ON raw_documents(processed);
CREATE INDEX idx_raw_documents_downloaded_at ON raw_documents(downloaded_at);
```

## ğŸ§° Tecnologias

- **reqwest**: HTTP client assÃ­ncrono
- **tokio**: ConcorrÃªncia assÃ­ncrona e runtime
- **chrono**: Controle temporal e formataÃ§Ã£o de datas
- **serde_json**: SerializaÃ§Ã£o e parsing de JSON
- **tracing**: Sistema de logs estruturados
- **sqlx**: InteraÃ§Ãµes com PostgreSQL
- **tokio-cron-scheduler**: Agendamento de tarefas
- **regex**: Parsing de XML do arXiv

## ğŸ”§ CollectorService

### Estrutura

```rust
pub struct CollectorService {
    db: PgPool,
    client: reqwest::Client,
    download_dir: PathBuf,
    arxiv_collector: ArxivCollector,
}

impl CollectorService {
    // Coleta artigos para um portal
    pub async fn collect_for_portal(&self, portal_id: i32) -> Result<CollectionResult>
    
    // Faz download de um artigo
    pub async fn download_article(&self, article, portal, source) -> Result<PathBuf>
    
    // Salva metadados no banco
    pub async fn save_document(&self, doc: &CreateRawDocument) -> Result<i32>
    
    // Busca documentos nÃ£o processados
    pub async fn get_unprocessed_documents(&self) -> Result<Vec<RawDocument>>
    
    // Marca documento como processado
    pub async fn mark_as_processed(&self, document_id: i32) -> Result<()>
}
```

## ğŸš€ Como Usar

### Via start.rs

```bash
cd G:\Hive-Hub\News-main

# Ver status do collector
cargo run --bin start collector

# Iniciar sistema completo
cargo run --bin start start
```

### Diretamente no Backend

```bash
cd G:\Hive-Hub\News-main\news-backend

# Carregar .env e executar
cargo run
```

## ğŸ“ Endpoints REST

### POST /api/collector/start

Inicia coleta para um portal especÃ­fico.

**Request:**
```json
{
  "portal_id": 1
}
```

**Response:**
```json
{
  "success": true,
  "documents_collected": 12,
  "duration_ms": 1523,
  "errors": []
}
```

### GET /api/collector/status/:portal_id

Retorna status da Ãºltima coleta.

**Response:**
```json
{
  "portal_id": 1,
  "last_collection": "2025-10-27T14:30:00Z",
  "status": "success",
  "articles_collected": 12,
  "next_collection": "2025-10-27T15:30:00Z"
}
```

### GET /api/collector/logs

Retorna logs de coletas.

**Query params:** `portal_id`, `limit`, `offset`

## ğŸ§ª Testes

### Unit Tests

- âœ… Download de arquivo simples
- âœ… SanitizaÃ§Ã£o de nome de arquivo
- âœ… DeduplicaÃ§Ã£o de downloads
- âœ… CriaÃ§Ã£o de estrutura de diretÃ³rios
- âœ… Parsing de XML do arXiv

### Integration Tests

- â³ Coleta completa para um portal
- â³ PersistÃªncia no banco de dados
- â³ VerificaÃ§Ã£o de arquivos baixados
- â³ Scheduler de tarefas

## ğŸ“Š Monitoramento

### MÃ©tricas Coletadas

- âœ… Total de documentos coletados
- âœ… Taxa de sucesso vs falhas
- âœ… Tempo mÃ©dio de download
- âœ… Tamanho mÃ©dio de arquivos
- âœ… Ãšltima coleta por portal
- âœ… Rate limiting por API

### Logs Estruturados

```rust
tracing::info!(
    source = %source,
    paper_id = %paper_id,
    pdf_url = %pdf_url,
    path = %file_path.display(),
    size = size,
    duration_ms = %duration.as_millis(),
    "Article downloaded successfully"
);
```

## ğŸš¨ Tratamento de Erros

### CenÃ¡rios Comuns

**1. API IndisponÃ­vel**
- Retry com backoff exponencial
- Log estruturado
- Continua para prÃ³xima fonte

**2. Arquivo Corrompido**
- ValidaÃ§Ã£o de checksum (opcional)
- Retry do download
- Log de erro especÃ­fico

**3. EspaÃ§o em Disco Insuficiente**
- VerificaÃ§Ã£o prÃ©via de espaÃ§o
- Limpeza de arquivos antigos (opcional)
- NotificaÃ§Ã£o de erro crÃ­tico

**4. Rate Limit Excedido**
- Aguarda tempo necessÃ¡rio
- Retry apÃ³s rate limit reset
- Log de rate limiting

## ğŸ”’ SeguranÃ§a

### âš ï¸ IMPORTANTE

1. **NUNCA commite arquivo `.env`** com chaves reais
2. **Sempre use `.env.example`** como template
3. **Adicione `.env` ao `.gitignore`**
4. **Use variÃ¡veis de ambiente** em produÃ§Ã£o
5. **Rote as chaves** quando possÃ­vel

### .gitignore

```gitignore
# Environment variables
.env
.env.local
.env.*.local
```

## ğŸ“ˆ Status da ImplementaÃ§Ã£o

### âœ… ConcluÃ­do

- [x] Estrutura de diretÃ³rios por origem
- [x] OrganizaÃ§Ã£o por data (YYYY-MM-DD)
- [x] SanitizaÃ§Ã£o de nomes de arquivo
- [x] DeduplicaÃ§Ã£o de downloads
- [x] PersistÃªncia no banco de dados
- [x] CollectorService implementado
- [x] arXiv API integrada (10 papers)
- [x] XML temporÃ¡rio salvo
- [x] Sistema de configuraÃ§Ã£o de chaves API
- [x] Rate limiting por API
- [x] Logs estruturados

### â³ Em Progresso

- [ ] Busca de configuraÃ§Ã£o de portal no banco
- [ ] Busca de sources configuradas no banco
- [ ] Scheduler automÃ¡tico com tokio-cron-scheduler
- [ ] Retry logic com backoff exponencial

### ğŸ¯ PrÃ³ximos Passos

**Implementar Outras Fontes:**
- Nature Publishing Group
- Science (AAAS)
- PubMed / NCBI
- IEEE Xplore
- Springer Nature
- Elsevier ScienceDirect

**Funcionalidades Adicionais:**
- [x] **Limpeza automÃ¡tica de arquivos temporÃ¡rios** - Implementada âœ…
- [x] **Coleta incremental anti-duplicaÃ§Ã£o** - Implementada âœ…
- [ ] ValidaÃ§Ã£o de arquivos baixados
- [ ] Download paralelo de mÃºltiplos PDFs
- [ ] EstatÃ­sticas de uso de disco
- [ ] Webhooks para notificaÃ§Ãµes

## ğŸ‰ Funcionalidades Implementadas

### 1. Coleta Incremental e Anti-DuplicaÃ§Ã£o

O sistema busca automaticamente **artigos novos nÃ£o duplicados**:

**Como funciona:**
```rust
// Loop atÃ© baixar 10 novos artigos
while downloaded_count < 10 {
    // Busca batch com offset dinÃ¢mico (0, 10, 20...)
    let url = format!("https://export.arxiv.org/api/query?search_query=cat:cs.AI&start={}&max_results=20", offset);
    
    // Para cada artigo
    if file_already_downloaded(paper_id, base_dir) {
        println!("  â­ï¸  (already exists)");
        continue; // Pula duplicados
    } else {
        download_pdf(); // Baixa apenas novos
        downloaded_count++;
    }
    
    // Incrementa offset se nÃ£o encontrou novos
    if downloaded_count == 0 {
        offset += 10;
    }
}
```

**VerificaÃ§Ã£o em 3 locais:**
```rust
fn file_already_downloaded(paper_id: &str, base_dir: &Path) -> bool {
    // 1. Verifica em downloads/arxiv/ (todas as datas)
    // 2. Verifica em downloads/filtered/<categoria>/
    // 3. Verifica em downloads/rejected/
    
    // Retorna true se encontrado em qualquer lugar
}
```

**CaracterÃ­sticas:**
- âœ… Verifica existÃªncia antes de baixar
- âœ… **Verifica em 3 locais**: arxiv/, filtered/, rejected/
- âœ… Offset dinÃ¢mico (auto-incrementa atÃ© 100)
- âœ… Loop inteligente atÃ© completar 10 novos artigos
- âœ… Logs claros: `â­ï¸ (already exists)` vs `âœ… NEW`
- âœ… Safety limit para evitar loops infinitos
- âœ… **NOVO**: Detecta arquivos jÃ¡ movidos pelo filtro

**Output exemplo:**
```
ğŸ“¡ Fetching batch starting from offset 0...
  Found 20 papers in this batch
  [1/10]: 2510.21695v1... â­ï¸  (already exists)
  [1/10]: 2510.21689v1... â­ï¸  (already exists)
  ...
  [1/10]: 2510.21443v1... âœ… NEW
  [2/10]: 2510.21436v1... âœ… NEW
  ...
```

### 2. Limpeza AutomÃ¡tica de Arquivos TemporÃ¡rios

**Implementado:** O sistema **remove automaticamente** arquivos XML temporÃ¡rios apÃ³s cada coleta:

```rust
async fn cleanup_temp_files(temp_dir: &Path) -> Result<()> {
    for entry in fs::read_dir(temp_dir)? {
        let path = entry.path();
        
        // Deletar apenas XMLs
        if path.is_file() && path.extension() == "xml" {
            fs::remove_file(&path)?;
            println!("  âœ“ Deleted: {}", filename);
        }
    }
}
```

**ExecuÃ§Ã£o automÃ¡tica:**
```rust
println!("\nâœ… Collection completed!");
println!("   New papers downloaded: 10/10");

// Limpar arquivos temporÃ¡rios
println!("\nğŸ§¹ Cleaning temporary files...");
cleanup_temp_files(&temp_dir).await?;
```

**Output exemplo:**
```
âœ… Collection completed!
   New papers downloaded: 10/10
   Location: G:/Hive-Hub/News-main/downloads\arxiv\2025-10-27

ğŸ§¹ Cleaning temporary files...
  âœ“ Deleted: arxiv_feed_1761543231.xml
  âœ“ Deleted: arxiv_feed_1761543107.xml
  Cleaned 2 temporary file(s)
```

**Resultado:** DiretÃ³rio `downloads/temp/` sempre limpo apÃ³s execuÃ§Ã£o.

## ğŸ“š PrÃ³ximas Etapas do Pipeline

ApÃ³s implementar o Collector:

1. **Etapa 2**: ExtraÃ§Ã£o e chunking de texto (`ExtractorService`)
2. **Etapa 3**: Embedding e indexaÃ§Ã£o vetorial (`EmbedderService`)
3. **Etapa 4**: Rankeamento e seleÃ§Ã£o de artigos (`RankerService`)
4. **Etapa 5**: GeraÃ§Ã£o de conteÃºdo (`PublisherService`)

## ğŸ“š ReferÃªncias

- [reqwest documentation](https://docs.rs/reqwest/)
- [tokio-cron-scheduler](https://docs.rs/tokio-cron-scheduler/)
- [sqlx migrations](https://docs.rs/sqlx/0.7/sqlx/macro.migrate.html)
- [tracing crate](https://docs.rs/tracing/)
- [arXiv API](https://arxiv.org/help/api)
- [Nature API](https://www.nature.com/documents/nr-API-Developer-Guide.pdf)

---

**ğŸ¯ Collector implementado com arXiv e pronto para expansÃ£o com outras fontes!**

**LocalizaÃ§Ã£o:** `G:\Hive-Hub\News-main\downloads/<origem>/<YYYY-MM-DD>/`
