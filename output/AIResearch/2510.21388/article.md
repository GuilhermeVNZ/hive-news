Artificial intelligence systems that process sound—from music to environmental noises—are becoming essential in smart devices, but their high computational demands limit deployment on phones and embedded systems. A new study demonstrates how to compress these AI models dramatically, cutting energy use and carbon emissions while maintaining performance, paving the way for more sustainable and accessible audio AI.

Researchers found that quaternion convolutional neural networks (QCNNs), a type of AI model designed to handle multi-channel audio data, can be significantly compressed using pruning techniques. This approach removes less important components from the network, reducing its size and computational needs without substantially harming accuracy. On the AudioSet dataset, which includes millions of audio clips, compressed QCNNs achieved similar performance to larger models while using 50% less computational power and 80% fewer parameters—the internal values the AI learns during training.

The method involves evaluating the importance of filters within the QCNN using metrics like the operator-norm, which identifies which parts contribute least to the model's output. These low-importance filters are then pruned away, and the remaining model is fine-tuned on the original data to recover any lost performance. This process contrasts with knowledge distillation, another compression technique where a smaller model learns from a larger one; pruning outperformed distillation in efficiency, requiring less computational effort for similar results.

Results show that pruned QCNNs maintain competitive accuracy across diverse audio tasks. For music genre classification on the GTZAN dataset, a pruned QCNN achieved 94.9% accuracy with only 8.95 million parameters and 6 billion multiply-accumulate operations (MACs), a measure of computational cost. This outperformed larger models like genreMERT, which uses over 80 million parameters and 100 billion MACs. In environmental sound classification on ESC-50, pruned models reached up to 97.6% accuracy, surpassing some Transformer-based architectures while using far fewer resources. Similarly, for speech emotion recognition on RAVDESS, they matched or exceeded the performance of conventional CNNs with reduced computational loads.

This compression has real-world implications for deploying AI in energy-sensitive environments, such as mobile devices and Internet of Things (IoT) systems. By lowering inference time and energy consumption, the approach reduces the environmental footprint; for instance, a compressed QCNN14 model cut inference time by 55% and carbon emissions by 53% compared to its uncompressed counterpart, with a performance drop of less than 1 percentage point. This makes AI more feasible for everyday applications like smart assistants, security systems, and healthcare monitoring, where efficiency is critical.

Limitations include the trade-off between compression and performance, as higher pruning ratios can lead to slight accuracy declines. The study also notes that the advantages of QCNNs stem from their ability to capture inter-channel dependencies in audio data, but further research is needed to optimize pruning for other AI architectures, such as Transformers, and to explore the combined effects of pruning with other techniques like knowledge distillation.