In many real-world applications, from medical diagnosis to content moderation, obtaining fully labeled training data is prohibitively expensive or practically impossible. Researchers now report a method that enables artificial intelligence systems to learn effectively even when most data lacks labels—a common scenario where traditional machine learning fails.

The key finding is a cost-sensitive framework for multi-class positive-unlabeled (MPU) learning, where AI systems learn to distinguish known categories of interest from everything else without requiring comprehensive labeling. This approach specifically addresses the challenge of class imbalance, where some categories appear much less frequently than others, by assigning appropriate penalties to different types of classification errors.

The methodology builds on risk minimization principles, combining class-dependent cost weighting with unbiased risk estimation. The researchers formalized the data-generating process where only some classes have labeled examples while others remain completely unlabeled. Their approach assigns distinct, data-dependent weights to different components of the learning objective, ensuring the resulting empirical risk estimator remains unbiased while maintaining stability during optimization.

Experimental results across eight benchmark datasets—including handwritten digit recognition (MNIST), fashion item classification (Fashion-MNIST), and real-world street view house numbers (SVHN)—demonstrate consistent performance advantages. The method achieved classification accuracies ranging from 80% to over 97% across different datasets and class-prior settings, substantially outperforming existing approaches. For example, on MNIST with six classes and 20% negative class prior, the method attained 92.73% accuracy compared to 79.03% for the next best baseline. The framework showed particular strength in handling heavily imbalanced scenarios where competing methods often degraded to near-chance performance.

This work matters because it addresses a fundamental limitation in applying machine learning to practical problems where comprehensive data labeling is infeasible. In medical imaging, for instance, obtaining verified negative examples for rare diseases can be extremely difficult. Similarly, in content moderation, identifying all possible harmful content categories in advance is impractical. The method enables reliable detection of known categories of interest while suppressing everything else, without requiring complete knowledge of all possible classes.

The approach does have limitations. Performance depends on accurate estimation of class priors—the relative frequencies of different categories in the data. While the paper provides robust estimation procedures, misspecification of these priors can degrade performance, particularly in highly imbalanced scenarios. Additionally, the theoretical guarantees assume the learning model has sufficient capacity to represent the underlying patterns, which may not always hold in practice with limited data or overly complex relationships.