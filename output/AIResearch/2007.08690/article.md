Hybrid electric vehicles (HEVs) offer a promising solution to reduce fossil fuel consumption and emissions, but their energy management systems often require lengthy training times to optimize performance. Researchers have now developed an AI approach that dramatically accelerates this process, making it feasible to deploy efficient energy controllers in real-world driving environments. This breakthrough addresses a critical bottleneck in applying deep reinforcement learning (DRL) to hybrid vehicles, where traditional methods can take hours to train.

The key finding is that combining deep reinforcement learning with transfer learning enables an adaptive energy management strategy for hybrid tracked vehicles (HETVs). This method divides driving cycles into speed intervals—low, medium, and high—and uses pre-trained neural networks to quickly adapt to new conditions. By reusing learned parameters, the system reduces training time from over 21 hours to under 0.8 hours while maintaining near-optimal fuel efficiency.

The methodology involves a bi-level framework. At the upper level, driving cycles are classified into speed intervals, and DRL algorithms—specifically Deep Q-Network (DQN) and Deterministic Policy Gradient (DDPG)—train energy management strategies for each interval. At the lower level, transfer learning applies these pre-trained networks to novel driving cycles, requiring only the output layer to be retrained. This approach leverages similarities in driving tasks across intervals to avoid starting from scratch, much like reusing a trained chef's skills to quickly master a new recipe.

Results from simulation experiments show that the DDPG with transfer learning method achieves fuel consumption of 2.8673 L/100km, nearly matching the global optimum of 2.8673 L/100km obtained via dynamic programming (DP) and outperforming common DDPG at 3.2896 L/100km. The state of charge (SOC) trajectories and power distribution between the engine and battery closely align with DP, indicating high optimality. Training time is reduced by over 96%, with cumulative rewards and mean Q-value errors converging faster, demonstrating improved learning efficiency.

This innovation matters because it makes AI-based energy management practical for everyday use in hybrid vehicles, potentially lowering fuel costs and emissions without lengthy recalibration. For regular drivers, it means vehicles could adapt more swiftly to different driving styles and conditions, enhancing efficiency and reliability. In broader terms, it showcases how transfer learning can overcome time-consuming training in AI applications, with implications for other real-time control systems.

Limitations noted in the paper include that the method's performance depends on the similarity between source and target driving cycles; if they are uncorrelated, negative transfer could occur, reducing effectiveness. Future work will focus on online implementation and hardware-in-the-loop tests to validate the framework in real-world scenarios.