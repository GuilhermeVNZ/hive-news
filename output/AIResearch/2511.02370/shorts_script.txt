[VOICEOVER 0:00-0:05] What if an AI label could override your trust in established institutions? [VISUAL DIRECTION] Animated text: "AI CREDIBILITY > INSTITUTIONAL TRUST" with question mark pulsing

[VOICEOVER 0:05-0:20] As artificial intelligence becomes embedded in our information ecosystems, new research reveals a startling finding about how AI-generated signals shape what people believe. [VISUAL DIRECTION] Show Figure 1 from paper with zoom on experimental design diagram

[VOICEOVER 0:20-0:40] Researchers conducted a carefully designed experiment testing how different credibility signals—including AI labels from ChatGPT—influence people's judgments of news content. [VISUAL DIRECTION] Fast cuts between different signal conditions: control, GroundNews labels, reversed labels, ChatGPT labels

[VOICEOVER 0:40-1:00] The discovery: ChatGPT labels produced the largest overall increase in credibility ratings compared to control conditions, operating uniformly across political groups. [VISUAL DIRECTION] Show Figure 2 with dramatic zoom on peak distribution for ChatGPT condition

[VOICEOVER 1:00-1:30] This cross-partisan persuasiveness makes AI potentially powerful for improving information quality, but raises concerns about over-reliance on opaque systems that may override human judgment. [VISUAL DIRECTION] Animated text overlay: "Conservative participants showed strongest responsiveness to ChatGPT feedback"

[VOICEOVER 1:30-1:50] The findings highlight the urgent need for transparent, explainable AI systems as they become integrated into our information environments. [VISUAL DIRECTION] Transition to ethical implications graphic with key statistics from paper

[VOICEOVER 1:50-2:00] AI credibility signals now outrank institutional trust—how will we ensure these systems enhance rather than override our judgment? [VISUAL DIRECTION] Final text: "AI CREDIBILITY: FRIEND OR FOE?" with paper citation