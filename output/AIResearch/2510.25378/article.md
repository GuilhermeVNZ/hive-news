When artificial intelligence systems recommend scientific papers, they often invent fake references that look convincingly real—revealing fundamental limitations in how these models learn from data. A new study shows this problem isn't random but systematically linked to how frequently papers appear in training data, raising concerns about AI reliability in academic and research applications.

The key finding demonstrates that large language models like GPT-4.1 generate fabricated bibliographic references at dramatically different rates depending on how often the original papers were cited. Researchers discovered that papers with high citation counts—meaning they appear frequently across scientific literature—are reproduced accurately by AI systems. However, papers cited less than about 300 times suffer from increasing rates of hallucination, where the AI invents plausible-looking but completely fake references.

The methodology involved testing GPT-4.1's ability to generate accurate bibliographic recommendations across computer science domains. Researchers prompted the model to produce paper recommendations in specific fields, then manually verified each output against Google Scholar records. They scored references on a three-point scale: completely accurate (score 2), partially accurate with some incorrect metadata (score 1), or completely fabricated (score 0). Citation frequency data came from Google Scholar as of October 2025.

Results analysis revealed a clear pattern: average accuracy scores dropped significantly for low-citation papers compared to high-citation ones. The study found that papers with citation counts below the median (818 citations) showed substantially lower accuracy (average score 0.725) compared to highly-cited papers (average score 1.245). Figure 2 in the paper illustrates this relationship, showing that once citation counts exceed approximately 1,000 (log(citation) ≈ 7), models begin reproducing papers verbatim rather than generating fabricated content. The research also identified domain-specific variations, with some computer science subfields showing markedly different hallucination rates.

This matters because AI systems are increasingly used for literature review assistance, academic research, and knowledge discovery. The findings suggest that AI recommendations in emerging fields or niche research areas—where papers have lower citation counts—may be particularly unreliable. For researchers relying on AI tools to survey literature or identify relevant studies, this systematic bias could lead to missed important work or inclusion of non-existent references in their own publications.

Limitations include the study's focus on computer science domains and a single AI model (GPT-4.1). The research didn't explore whether similar patterns occur in other disciplines or with different large language models. Additionally, the study examined only bibliographic hallucinations rather than factual inaccuracies in paper content summaries. The authors note that future work should extend to multilingual contexts and diverse academic fields to assess the generalizability of these findings.