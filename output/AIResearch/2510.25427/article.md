Artificial intelligence systems that can solve mathematical problems have shown impressive results on standard tests, but a new evaluation reveals they struggle significantly when faced with the complex proofs found in actual research mathematics. The RLMEval benchmark, developed by researchers at EPFL, demonstrates that even the best AI models achieve only modest success rates when tackling genuine mathematical challenges, highlighting a critical gap between artificial benchmarks and real-world mathematical reasoning.

Researchers discovered that current large language models perform dramatically worse on research-level mathematics compared to established benchmarks. The best-performing model, DeepSeek-Prover-V2-7B, achieved only a 10.3% success rate on autoformalization tasks and 8.8% on theorem proving in the normal evaluation mode. This represents a substantial drop from the 88.9% success rates reported on the MiniF2F benchmark, indicating that progress on curated problems does not readily translate to realistic mathematical settings.

The research team created RLMEval by selecting 613 theorems from six ongoing mathematical research projects, focusing specifically on theorems that represent significant conceptual advances rather than routine deductions. The benchmark includes two main tasks: neural theorem proving, where models must generate complete verifiable proofs given formal statements, and autoformalization, where models translate natural language mathematical statements into formal proofs. The evaluation was conducted using a dedicated Python interface called LeanInteract that enables robust communication with the Lean proof assistant.

Analysis of the results reveals several critical patterns. Performance varies substantially across different mathematical domains, with some projects yielding success rates as high as 32.1% while others remain below 1%. The researchers found that proof length serves as a meaningful indicator of difficulty, with longer proofs consistently correlating with lower success rates. When examining the proofs that models did generate successfully, they tended to be significantly shorter than human-written counterpartsâ€”averaging only 2.5-6.0 lines compared to the human average of 16.6 lines. This suggests models primarily succeed on theorems that admit concise proof strategies.

The practical implications are significant for mathematicians and computer scientists working on automated reasoning. The stark performance gap indicates that current AI systems cannot reliably assist with ongoing mathematical research, despite their impressive results on standardized tests. The researchers identified that access to project-specific intermediate lemmas provides some benefit, improving success rates by approximately 6%, but this limited improvement underscores the fundamental challenges AI faces with complex mathematical reasoning.

Several limitations remain. The researchers note potential contamination concerns, as some models were trained on mathematical content that might overlap with RLMEval problems. Additionally, the evaluation used smaller model versions due to computational constraints, meaning the results likely underestimate achievable performance. The current setup also provides models with only limited context preceding target theorems, which may disadvantage systems trained on self-contained problems. Future work should investigate optimal retrieval strategies and context provision methods to better support research-level mathematical reasoning.