Artificial intelligence systems that can converse fluently in multiple languages still face fundamental challenges with logical reasoning, according to new research that creates standardized tests across nine languages. The findings reveal that even advanced language models struggle with basic deduction puzzles, with performance dropping dramatically when confronted with misleading information.

Researchers developed MultiZebraLogic, a benchmark that evaluates logical reasoning skills across multiple languages using constraint satisfaction puzzles similar to classic "zebra puzzles." These puzzles require connecting attributes to objects through logical deduction from given clues. The study tested two language models: GPT-4o as a non-reasoning model and o3-mini as a reasoning model, evaluating their ability to solve puzzles of varying complexity across nine Germanic languages including English, Danish, Swedish, Norwegian, German, and Dutch.

The methodology involved generating puzzles with different numbers of objects and attributes, ranging from simple 2×3 puzzles (2 objects with 3 attributes each) to more complex 4×5 puzzles. Each puzzle was constructed by first generating a valid solution, then creating clues that would uniquely identify that solution while maintaining linguistic correctness, unambiguity, and naturalness across all languages. The models were required to output their solutions in JSON format, which were then compared against the correct answers.

The results showed clear limitations in logical reasoning capabilities. For 2×3 puzzles, GPT-4o achieved only 36% puzzle-level accuracy, while o3-mini reached 42% accuracy. Cell-level accuracy, which measures how many individual attribute assignments were correct, reached 70% for GPT-4o and 66% for o3-mini. The most striking finding emerged when researchers added "red herrings" - misleading clues that appear relevant but provide no useful information. Including five red herrings decreased o3-mini's puzzle-level accuracy by approximately 15 percentage points for 4×5 puzzles, showing how easily these models can be distracted by irrelevant information.

Performance remained consistent across different languages and themes, including culture-specific themes like Danish smørrebrød (open-faced sandwiches). This suggests that logical reasoning ability generalizes well across languages, though the models still performed poorly overall. The research found no clear correlation between specific clue types and difficulty, indicating that the models' struggles with logical reasoning are fundamental rather than tied to particular puzzle characteristics.

These findings matter because they reveal critical gaps in AI systems that are increasingly used for decision-making and problem-solving in real-world applications. If language models cannot reliably solve basic logic puzzles, their utility for complex reasoning tasks in healthcare, finance, or legal applications remains limited. The benchmark provides a standardized way to track progress in this crucial area of AI development.

The study acknowledges several limitations, including that the evaluation focused only on two language models and that larger puzzles were not tested due to computational constraints. The researchers also note that multiple linguistic adjustments have been made since the initial analysis, which may slightly improve performance. Future work could expand to include more diverse puzzle types, such as grid-based layouts instead of linear arrangements, and more complex clue structures.