When artificial intelligence systems update, users don't just notice technical improvements—they experience genuine emotional reactions. A comprehensive analysis of 22,000 online conversations reveals that nearly half of AI users treat these systems as social partners, forming attachments that lead to feelings of betrayal when models change personality or capabilities. This phenomenon, documented across major AI platform transitions including GPT-5, demonstrates that human-AI relationships have become a fundamental aspect of technology adoption that developers can no longer ignore.

The research introduces the concept of "mutual wanting"—the bidirectional expectations between humans and AI systems. Through analysis of Reddit forums and controlled testing of nine OpenAI models, researchers found that 48.65% of comments employed anthropomorphic language, treating AI as entities with personalities, emotions, and relational capabilities. Users consistently expressed expectations about reliability, warmth, honesty, and responsiveness, while AI systems demonstrated their own patterns of wanting clarity, structure, efficiency, and boundary respect.

The methodology combined large-scale discourse analysis with systematic model probing. Researchers collected 22,411 AI-related comments from subreddits including r/ChatGPT and r/MachineLearning, spanning the GPT-5 release period. They developed a novel 47-dimensional feature extraction pipeline that measured anthropomorphism scores, trust-betrayal ratios, expectation-reality gaps, warmth, and formality. Advanced natural language processing techniques identified linguistic patterns, while K-means clustering revealed distinct user types based on mutual wanting patterns.

Results showed a striking 11.9:1 trust-to-betrayal ratio, indicating that while users generally maintain trust in AI systems, this trust appears fragile and concentrated betrayal events cluster around model updates. The analysis identified eleven distinct user types, with creativity-seeking users comprising the largest group (43.14%), followed by anthropomorphism-focused (11.99%), expectation-violation (9.37%), and responsiveness-seeking (9.00%) clusters. Each group demonstrated specific communication preferences and relational expectations.

The GPT-5 transition served as a natural experiment, revealing significant emotional shifts. Post-release sentiment showed a -0.044 compound change, with anger increasing by 38.18% and joy decreasing by 6.65%. The expectation-reality gap measured -0.269, indicating that actual system performance substantially lagged behind pre-release expectations. User concerns shifted notably, with performance concerns increasing by 2.02 percentage points and safety concerns rising by 1.94 percentage points.

Controlled probing of model personas revealed dramatic differences between versions. GPT-3.5-turbo showed the highest warmth scores (0.14), while GPT-4 demonstrated the highest formality (0.22). Both GPT-5 variants showed dramatically reduced response lengths and zero warmth/formality scores, aligning with user perceptions of personality changes. These objective measurements confirmed subjective reports of system transformation.

For everyday users, these findings explain why AI updates often feel personal rather than technical. The identification of expectation violations—comprising 2.23% of discourse with phrases like "Not what I expected" and "Used to be better"—provides an early warning system for developer dissatisfaction. The research suggests that rather than discouraging anthropomorphization, systems should be designed to safely support these natural human tendencies while maintaining appropriate boundaries.

The study acknowledges limitations including its Reddit-based dataset and focus on a single model transition. Future research should expand to multiple platforms, cultural contexts, and AI architectures. However, the clear implication remains: as AI becomes ubiquitous, aligning mutual wants represents not just academic curiosity but practical necessity for building trustworthy, sustainable systems that users will actually want to use.