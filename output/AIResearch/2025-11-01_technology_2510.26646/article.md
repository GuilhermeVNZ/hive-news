Robots in warehouses, search-and-rescue missions, and security patrols often struggle with dynamic environments where obstacles move and conditions change rapidly. Traditional navigation methods rely on pre-built maps and fixed algorithms, making them slow and inflexible in unpredictable settings. A new hybrid artificial intelligence approach combines two learning techniques to enhance robot decision-making, enabling more adaptive and efficient navigation without constant human intervention.

The researchers developed a framework that integrates Deep Q-Network (DQN) for high-level strategic choices, like selecting subgoals and directions, with Twin Delayed Deep Deterministic Policy Gradient (TD3) for precise, low-level control actions such as movement and obstacle avoidance. This hybrid design addresses the limitations of using either method alone: DQN excels at discrete decisions but lacks fine control, while TD3 handles continuous actions effectively but struggles with complex planning. By merging these, the system aims to improve overall navigation accuracy and robustness in uncertain environments.

Methodology involved simulating the hybrid algorithm in ROS and Gazebo platforms, using PyBullet for physics modeling and a custom Gym environment for training. The robot agents learned through reinforcement learning, where rewards guided behaviorâ€”for instance, positive rewards for moving toward goals and avoiding obstacles, and penalties for collisions or excessive time usage. Key reward components included direction alignment, distance to target, obstacle avoidance, path smoothness, and time efficiency, with specific weights assigned to balance these objectives. Training spanned approximately 10,000 episodes (5 million timesteps), with evaluations based on success rates, collision rates, time to goal, path efficiency, and trajectory smoothness.

Results from the simulation showed that the hybrid approach began learning effectively early in training, with reward curves rising from negative to positive values. Mid-training phases exhibited fluctuations as the agent balanced exploration and exploitation, stabilizing in later stages with consistent performance. Qualitative observations indicated that the high-level DQN agent generated subgoals visible in RViz, though initial rotations and instability were noted. The TD3 component demonstrated gradual convergence in loss metrics, approaching near-zero values after extensive training, suggesting improved control stability. However, the combined DQN-TD3 framework faced challenges, including policy misalignment and hyperparameter sensitivity, leading to unstable behavior that requires further tuning for reliable deployment.

This advancement matters because it could enable robots to operate more autonomously in real-world scenarios like disaster response or logistics, where environments are cluttered and unpredictable. By reducing dependency on pre-mapped routes and enhancing real-time adaptation, the method supports safer and more efficient robotic systems. Limitations include the current instability of the hybrid framework, limited hyperparameter exploration, and validation only in simulation without physical robot tests. Future work will focus on stabilizing the system, extending it to multi-robot coordination, and applying it to high-dimensional 3D environments for broader practical use.