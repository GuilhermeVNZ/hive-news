Autonomous vehicles are getting better at understanding and following traffic regulations, moving beyond simple rule-following to handle the complex reasoning human drivers use every day. A comprehensive survey reveals how researchers are teaching self-driving systems to interpret laws, predict violations by other drivers, and make legally defensible decisions in ambiguous situations—critical steps toward widespread adoption of automated driving technology.

The central finding shows that significant progress occurs at the intersection of perceptual reliability, legal compliance, and decision justifiability. Researchers have developed methods that enable autonomous vehicles to not only detect objects but also understand traffic rules and make decisions that can be legally defended if questioned. This represents a fundamental shift from purely technical driving algorithms to systems that must operate within legal frameworks.

Methodologically, researchers are taking multiple approaches to integrate legal reasoning into autonomous systems. Neural-symbolic integration combines deep learning with symbolic logic, allowing vehicles to apply domain knowledge like traffic laws directly during perception tasks. For traffic sign recognition, Robust Logic-infused Learning automatically extracts high-level features through inductive logic programming, transforming them into constraints that improve system reliability. Energy-Based Modeling Frameworks establish three distinct paradigms for scoring compatibility between neural network predictions and logical constraints, enabling direct enforcement of traffic rules and physical laws.

Results demonstrate practical improvements across multiple domains. In perception systems, logic-infused approaches show significant robustness gains—maintaining comparable performance on normal images while outperforming baseline systems when subjected to adversarial attacks. Knowledge-Refined Sets reduce prediction set sizes by up to 80% while increasing semantic consistency by 30%, providing more reliable uncertainty quantification. For object detection, Multi-label Object Detection Constrained Loss frameworks support multiple labels per bounding box while ensuring predefined requirement satisfaction, showing improvements in precision, recall, and F1-score compared to baseline methods.

The real-world implications extend beyond technical performance. These developments address critical legal and safety concerns as autonomous vehicles approach broader deployment. Systems that can explain their decisions in human-understandable terms—such as "I stopped because the traffic light was red"—build public trust and provide defensible explanations in accident scenarios. Rule-based planners and formal verification methods ensure that vehicle behavior aligns with traffic regulations, while runtime monitoring continuously checks compliance during execution.

However, significant limitations remain. Most current systems handle only subsets of traffic regulations rather than comprehensive legal codes. Encoding the full traffic laws of even one jurisdiction remains daunting, and maintaining updates as laws change presents ongoing challenges. Scenario validation faces state-space explosion issues—while systems perform well in designed scenarios, rare combinations of events may expose gaps in legal reasoning. Adaptation to different driving cultures and jurisdictions also presents unresolved tensions between strict compliance and socially acceptable behavior.

The survey highlights that making autonomous vehicles law-compliant isn't just a technical goal but a sociotechnical challenge requiring collaboration between roboticists, legal scholars, and regulators. As these systems evolve, they must navigate the delicate balance between following rules precisely and adapting to the nuanced, sometimes ambiguous nature of real-world driving—much like human drivers do every day.