As 5G networks expand and 6G emerges, wireless communication faces a critical bottleneck: the massive amount of data needed to optimize signal quality while protecting user privacy. Current methods for channel state information feedback—the process that helps base stations understand signal conditions—require either collecting sensitive user data centrally or transmitting enormous model updates that strain network capacity. Researchers at Shanghai Jiao Tong University have developed Fed-PELAD, a federated learning approach that reduces communication costs by 57% while maintaining signal reconstruction accuracy across diverse environments.

The key innovation lies in how Fed-PELAD handles the trade-off between personalization and efficiency. Unlike conventional methods that either share raw data or transmit full model updates, this system keeps personalized encoders on user equipment to capture device-specific characteristics while using a shared decoder updated through Low-Rank Adaptation (LoRA). This means only compact adapter parameters—representing just 42.97% of conventional communication costs—need transmission rather than full model weights.

The methodology employs a four-stage process that begins with a pre-trained base model. During local updates, each user device trains its personalized encoder on private channel data while updating LoRA adapters for the shared decoder. These adapters are then aggregated at the base station using an alternating-freeze mechanism that stabilizes training across heterogeneous devices. The system alternates between updating A and B adapter matrices while keeping the other frozen, with separate learning rates (ηA and ηB) to control update directions and step sizes effectively.

Results from extensive simulations using 3GPP-standard channel models demonstrate significant improvements. Fed-PELAD achieved normalized mean squared error (NMSE) performance of -12.36 dB in urban micro environments while requiring only 42.97% of the communication overhead compared to conventional federated learning. The system maintained robust performance across four distinct scenarios—outdoor LOS, outdoor NLOS, indoor LOS, and indoor NLOS—with the alternating-freeze mechanism reducing performance variance from ±4.44 to ±4.19 in the most challenging conditions.

This approach matters because it enables more efficient wireless networks without compromising user privacy. By keeping sensitive channel information on user devices and transmitting only essential adapter parameters, Fed-PELAD addresses both the scalability challenges of massive MIMO systems and growing privacy concerns in distributed learning. The method's adaptability to different communication budgets—achieving -11.12 dB NMSE with just 21.49% overhead in constrained scenarios—makes it practical for real-world deployment where bandwidth varies.

The research acknowledges limitations in how the system handles extreme heterogeneity across user devices and environments. While the alternating-freeze mechanism improves stability, performance gaps remain between ideal centralized training and distributed approaches. The study also notes that optimal hyperparameter settings depend on specific deployment scenarios, requiring careful tuning of LoRA rank and learning rate ratios for different network conditions.