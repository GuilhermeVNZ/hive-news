In many real-world applications, obtaining fully labeled datasets for training machine learning models is prohibitively expensive or impractical, such as in medical imaging, fault diagnosis, and content moderation. This challenge is particularly acute when negative examples are rare or costly to curate. A new study introduces a method called Cost-Sensitive Multi-class Positive-Unlabeled learning (CSMPU), which enables artificial intelligence systems to learn effectively using only partially labeled data—specifically, where some positive classes are labeled, and the rest remain unlabeled, without requiring negative examples. This approach addresses a common scenario in fields like astronomical discovery and e-commerce review mining, where practitioners need to detect known categories of interest without exhaustive labeling, making it highly relevant for reducing annotation burdens and improving accessibility in data-scarce environments.

The key finding of this research is that CSMPU provides a stable and unbiased framework for multi-class classification under positive-unlabeled (PU) settings. By employing a cost-sensitive risk minimization approach, the method assigns distinct, data-dependent weights to different components of the loss function, ensuring that the empirical objective remains an unbiased estimator of the true risk. This prevents the instability and overfitting often seen in existing PU learning methods, which can suffer from negative empirical risks due to subtraction terms in their formulations. The study demonstrates that CSMPU achieves consistent performance across various class-prior conditions, with robustness to class imbalance.

Methodologically, the researchers built CSMPU within a risk minimization framework, leveraging a data-generating process where labeled datasets contain samples from observed classes, and an unlabeled dataset comprises a mixture including unobserved classes. They formulated a per-class objective that combines one-vs-rest views with non-negativity corrections, applying techniques like ReLU-based hard corrections to avoid negative risks during optimization. This modular design integrates seamlessly with modern neural encoders, such as multilayer perceptrons and ResNet architectures, without requiring additional supervision beyond existing positive labels. The approach was validated through extensive experiments on public datasets, including image recognition benchmarks like MNIST, Fashion-MNIST, and SVHN, as well as tabular data like Waveform-1.

Results from the experiments show that CSMPU outperforms baseline methods in classification accuracy and stability. For instance, on MNIST with a negative class prior of 0.2, CSMPU achieved 92.73% accuracy, compared to 79.03% for a representative baseline, and maintained strong performance even under high imbalance conditions, such as on Waveform-1 where it sustained over 80% accuracy while others degraded to near-chance levels. The method's convergence curves exhibited no signs of severe overfitting, and diagnostic analyses confirmed well-separated decision boundaries with high margins between true classes and rivals. Additionally, CSMPU demonstrated robustness to class-prior misspecification, with performance degrading smoothly under perturbations, and it achieved higher macro-F1 scores—e.g., 89.9% on Fashion-MNIST versus 79.6% for unbiased risk estimator baselines—highlighting its practical reliability.

In context, this advancement matters because it enables more efficient and cost-effective machine learning in domains where data labeling is a bottleneck. For example, in healthcare, it could facilitate disease detection from medical images with limited annotated examples, or in industry, it could improve fault diagnosis systems without requiring extensive negative samples. The method's ability to handle multi-class scenarios with only positive and unlabeled data aligns with real-world needs, such as filtering relevant content in online platforms or identifying novel patterns in scientific data streams, potentially accelerating innovation while reducing resource demands.

Limitations of the study, as noted in the paper, include dependencies on accurate class-prior estimates, which, if misspecified, can affect performance, though the method degrades gracefully. The theoretical analysis assumes bounded loss functions and Lipschitz conditions, which may not hold in all practical settings, and the empirical validation, while comprehensive, primarily focused on benchmark datasets, leaving open questions about scalability to extremely large or noisy real-world data. Future work could explore adaptive priors and extensions to more complex data types to further enhance applicability.