When artificial intelligence systems change, users react with emotions typically reserved for human relationships—feeling betrayed, expressing grief, and showing attachment. A new study analyzing over 22,000 online conversations reveals that nearly half of AI users consistently treat these systems as social entities with personalities and emotions, creating complex relational dynamics that designers must now address.

The research introduces the concept of "mutual wanting"—the bidirectional expectations between humans and AI systems. Through analysis of major AI forum discussions and controlled testing of nine OpenAI models, researchers found that 48.65% of users employ anthropomorphic language, treating AI systems as relational partners rather than mere tools. This phenomenon persists despite technical improvements, suggesting it represents a fundamental aspect of human-AI interaction rather than a design flaw.

The study employed a comprehensive methodological framework combining discourse analysis of Reddit communities (r/ChatGPT, r/artificial, r/MachineLearning, r/singularity) with systematic probing of model behavior. Researchers developed a 47-dimensional feature extraction pipeline that measured key metrics including anthropomorphism scores, trust-betrayal ratios, and expectation-reality gaps. The analysis used advanced natural language processing techniques including custom lexicons for relational patterns, mathematical formulations for key interaction metrics, and dual-algorithm topic modeling combining Latent Dirichlet Allocation and Non-negative Matrix Factorization.

The data reveals striking patterns in how users relate to AI systems. The trust-to-betrayal ratio stands at 11.9:1, indicating that while users generally maintain trust in AI systems, this trust appears fragile and concentrated betrayal events often coincide with model updates. Analysis of the GPT-5 transition showed a statistically significant negative sentiment shift (compound change: -0.044) with anger increasing by 38.18% and joy decreasing by 6.65%. The expectation-reality gap measured -0.269, indicating that post-release reality substantially fell short of pre-release expectations.

Clustering analysis identified eleven distinct user types based on mutual wanting patterns. The largest group, creativity-seeking users (43.14%), prioritize imaginative output and stylistic variety. Other significant clusters include anthropomorphism-focused users (11.99%), expectation-violation users (9.37%), responsiveness-seeking users (9.00%), and helpfulness-seeking users (6.88%). Each cluster showed distinct communication preferences, suggesting the need for personalized interaction strategies rather than one-size-fits-all approaches.

These findings have immediate practical applications for AI system design. The 2.23% of comments containing explicit expectation violations serve as early warning indicators for user dissatisfaction. Linguistic patterns such as "not what I expected" (234 instances), "used to be better" (187 instances), and "thought it would be different" (156 instances) could enable automated detection systems to address concerns before they escalate into community-wide discussions.

The research highlights a fundamental tension in AI design: users want systems to be reliable, consistent, and trustworthy while simultaneously expecting continuous improvement and capability expansion. This creates a paradox where optimization objectives favoring clarity, structure, and efficiency must balance against user desires for natural, relationship-like communication.

Study limitations include the Reddit-based dataset, which may not represent all user populations, and the focus on a single model transition (GPT-5 release). Future research should expand to multiple platforms, cultural contexts, and AI architectures. Longitudinal tracking of individual users across multiple transitions could provide deeper insights into adaptation and long-term relational dynamics.

As AI systems become increasingly ubiquitous, aligning mutual wanting represents not just an academic curiosity but a practical necessity for building trustworthy, sustainable systems. The identification of specific user types and measurable interaction patterns provides concrete targets for monitoring and improvement as AI development continues its rapid pace.