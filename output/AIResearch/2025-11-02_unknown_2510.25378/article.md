When AI systems generate fake academic references, it undermines trust in automated research tools used by students, professionals, and scientists. A new study reveals that these errors are not random but stem from how often a paper appears in training data, highlighting a critical flaw in AI knowledge systems.

Researchers discovered that large language models like GPT-4.1 frequently invent non-existent bibliographic entries when asked to recommend scientific papers. These hallucinations include plausible but fabricated author names, journal titles, and publication details, merging elements from real sources into fictitious citations. For example, when prompted for a recent RFM analysis paper, the model produced a citation that followed correct formatting but referred to a non-existent paper by Raghunathan et al. in a made-up journal.

The study manually verified bibliographic records generated by GPT-4.1 for computer-science topics, scoring each on correctness. Citation counts from Google Scholar were used as a proxy for how frequently a paper appeared in the AI's training data. The team analyzed the relationship between citation frequency and hallucination rates using statistical tests and semantic similarity measures with Sentence-BERT embeddings to assess how closely AI outputs matched real metadata.

Data shows that highly cited papers—those with over about 1,000 citations—are almost always reproduced accurately, with near-perfect scores. In contrast, low-citation papers (below the median of 818 citations) had significantly higher hallucination rates, as shown by a one-tailed t-test result (t(98) = -5.12, p < .001). Figure 2 illustrates a log-linear correlation: as citation counts increase, factual accuracy improves, with verbatim memorization occurring beyond roughly 7 on the log(citation) scale. This indicates that AI models struggle with sparsely represented knowledge, fabricating references for less redundant works.

This finding matters because AI is increasingly used in academic and professional settings for tasks like literature reviews and recommendation systems. If models invent references for niche or emerging topics, it could lead to misinformation in research, education, and decision-making. The study suggests that improving AI reliability requires addressing imbalances in training data to reduce errors in less familiar domains.

Limitations include the focus on a single model (GPT-4.1) and computer-science topics, leaving open questions about whether these patterns hold for other disciplines, models, or multilingual contexts. The research did not explore solutions like retrieval-augmented generation but calls for future work to assess generality and develop methods to mitigate hallucinations.