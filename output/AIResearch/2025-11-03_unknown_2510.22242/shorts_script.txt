[VOICEOVER 0:00-0:05] What if your AI assistant fails basic information tasks nearly every time?
[VISUAL DIRECTION] Animated text: "AI FAILURE RATES: 48-98%" with shocking red numbers

[VOICEOVER 0:05-0:20] Researchers systematically tested popular AI models on core operations we rely on daily
[VISUAL DIRECTION] Show rotating icons of GPT-4o, GPT-5, Gemini-2.5-Flash with failure percentages

[VOICEOVER 0:20-0:40] They created PaperAsk benchmark to evaluate real-world performance across physics, biology, medicine
[VISUAL DIRECTION] Animated diagram showing four operations: retrieval, extraction, answering, verification

[VOICEOVER 0:40-1:00] The results are alarming: extraction tasks failed 72-91%, verification succeeded only 60% of the time
[VISUAL DIRECTION] Show Figure 2 with zoom on peak failure distribution across different task types

[VOICEOVER 1:00-1:30] Systems preferred search snippets over full documents, introducing conflicting information
[VISUAL DIRECTION] Side-by-side comparison: full document vs search snippet with accuracy percentages

[VOICEOVER 1:30-1:50] This reveals architectural trade-offs where speed compromises accuracy in current implementations
[VISUAL DIRECTION] Animated scale showing speed vs accuracy trade-off with current AI performance

[VOICEOVER 1:50-2:00] Critical implications: AI tools may mislead when you need reliable scientific information most