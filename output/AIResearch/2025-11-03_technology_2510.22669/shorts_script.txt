[VOICEOVER 0:00-0:05] What if AI could see the world in 3D with human-like precision, even when everything is moving around it?
[VISUAL DIRECTION] [Animated text: "AI SEES IN 3D" with rotating 3D city model]

[VOICEOVER 0:05-0:20] Current 3D mapping systems struggle with dynamic scenes—cars, pedestrians, changing environments. This limitation has held back autonomous navigation.
[VISUAL DIRECTION] [Show blurry 3D reconstruction with moving objects causing artifacts]

[VOICEOVER 0:20-0:40] Researchers asked: Can we create a system that maintains 3D consistency while accurately capturing motion?
[VISUAL DIRECTION] [Show research paper title "LVD-GS" with question mark animation]

[VOICEOVER 0:40-1:00] They discovered that combining hierarchical representations with LiDAR and visual data achieves state-of-the-art reconstruction.
[VISUAL DIRECTION] [Show Figure 2 with zoom on hierarchical representation layers]

[VOICEOVER 1:00-1:30] The system uses explicit-implicit joint modeling to filter transient objects while preserving important features. It selectively focuses on relevant elements, similar to human perception.
[VISUAL DIRECTION] [Animated diagram showing filtering process with moving objects being removed while structures remain]

[VOICEOVER 1:30-1:50] Results show 32.1 PSNR on nuScenes dataset—15% improvement. Tracking error dropped to 1.27 ATE-RMSE. This enables photorealistic 3D mapping of urban, highway, and campus environments.
[VISUAL DIRECTION] [Show side-by-side comparison: old method vs LVD-GS with clear improvement in vehicle contours and architectural details]

[VOICEOVER 1:50-2:00] Autonomous navigation just took a massive leap forward. The future of 3D perception is here.
[VISUAL DIRECTION] [Final shot: Clean 3D reconstruction of dynamic city scene with text: "LVD-GS: Seeing the World in Motion"]