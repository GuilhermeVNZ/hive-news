As global populations age, the demand for robots that can assist with caregiving tasks like repositioning and wiping patients is surging, but current systems often fail in unpredictable real-world settings. A new study introduces a brain-inspired artificial intelligence framework that enables robots to learn and perform multiple tasks without specialized programming, offering a path to more versatile and reliable automation in healthcare and beyond.

Researchers developed a predictive neural network, called PV-RNN, that integrates high-dimensional sensory inputs—such as vision and proprioception—to handle diverse tasks like repositioning a mannequin and wiping surfaces. This model learns directly from raw data, avoiding the need for handcrafted features, and demonstrated the ability to generalize across different scenarios in simulations. Key findings include the network's capacity to self-organize task representations, estimate uncertainty, infer hidden states, and maintain robustness even with degraded sensory inputs.

The methodology builds on the free-energy principle from neuroscience, where the brain minimizes prediction errors to guide behavior. The PV-RNN processes over 30,000 dimensions of visual and proprioceptive data through a hierarchical structure, updating its internal states to predict and adapt to sensory inputs. During training, the network was exposed to sequences of teleoperated robot actions, learning to reconstruct and generate behaviors for tasks involving rigid-body manipulation and flexible-object interactions. Evaluation involved simulation-based tests with multiple network initializations to ensure reliability.

Analysis of the results showed that the network successfully reconstructed visuo-proprioceptive sequences, with modules specializing in different aspects like continuous attention and task transitions. For instance, in repositioning tasks, the executive module regulated shifts between subtasks, while in wiping tasks, it handled smoother transitions. The model also inferred occluded body parts and adapted to varying uncertainty levels, with repositioning tasks showing higher volatility but less interference from other tasks. In robustness tests, reducing visual resolution to 16×16 pixels did not significantly impair performance when proprioceptive data was available, highlighting the importance of multimodal integration.

This approach matters because it addresses critical limitations in current robotics, such as the inability to handle heterogeneous tasks without retraining. By mimicking human cognitive flexibility, the framework could lead to robots that assist in caregiving, reducing physical strain on human workers and improving safety in environments like hospitals. The findings also provide insights into how biological systems process uncertainty and adapt, bridging AI and neuroscience.

Limitations include that the evaluation was restricted to simulations and a simplified mannequin, lacking real-world robot testing. The computational demands are high, with the model running at approximately 1 Hz in experiments, and the setup may not fully capture the complexity of human-robot interactions. Future work should focus on optimizing efficiency and scaling to more realistic scenarios to validate these promising results.