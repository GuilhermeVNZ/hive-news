Large language models like GPT-4 and LLaMA have transformed how we interact with technology, but their training has been guided by assumptions that may be limiting their potential. A new study from EPFL researchers challenges fundamental beliefs about how to best train AI systems across multiple languages, with implications for creating more capable and inclusive artificial intelligence.

The research team trained 1.1 billion and 3 billion parameter models on diverse multilingual corpora spanning up to 1,834 languages, systematically testing four key assumptions that have guided AI development. Their findings overturn conventional wisdom about language model training, revealing that common practices may be unnecessarily restrictive.

Using carefully designed experiments with the mC4 and FineWeb2 datasets, the researchers employed two training regimes: one with a fixed total budget where increasing multilingual data came at the expense of English content, and another with a fixed multilingual budget where English data was added on top. This approach allowed them to isolate the effects of multilingual training from simple data volume changes.

The results show that combining multiple languages does not inherently degrade performance in any single language. When researchers maintained a fixed multilingual budget and added English data separately, performance remained stable even when English comprised only 10% of the training mixture. This contradicts the assumption that multilingual training necessarily comes at the cost of English capability.

Perhaps most surprisingly, the study found that language family boundaries do not serve as effective barriers for knowledge transfer. Training with Russian as a pivot language proved equally beneficial for Slavic languages like Polish and Czech as using languages from the same specific family. The research suggests that selecting languages based on family relationships provides no clear advantage over choosing languages with abundant, high-quality data available.

The team also investigated curriculum learning approaches, where languages are introduced in stages rather than all at once. They tested four strategies: training all languages simultaneously, starting with English only, beginning with English plus major languages, and starting with major languages alone. While different curricula affected learning trajectories, none reduced negative interference between languages. The observed benefits stemmed from increased data exposure rather than the order of introduction.

Finally, the research revisits the 'curse of multilinguality'—the idea that adding more languages eventually degrades performance. The study demonstrates that this phenomenon arises not from language count itself but from data quality and model capacity limitations. When languages are added while maintaining data quality and sufficient model size, performance does not degrade, even when scaling from 25 to 400 languages.

These findings have immediate practical implications for AI development. Companies training multilingual models can be more aggressive in including diverse languages without fearing performance trade-offs. The research suggests that efforts should focus on data quality and cleaning rather than costly balancing operations between languages.

For regular technology users, this means future AI assistants could become more capable across a wider range of languages without sacrificing performance in commonly used languages like English. The study points toward more inclusive AI systems that better serve global populations.

The research does have limitations—the models, while substantial, are smaller than frontier systems like GPT-4, and the study couldn't explore post-training strategies. However, the systematic approach and clear results provide strong evidence that current multilingual training practices may be overly conservative. As AI continues to transform global communication, these findings could help build systems that truly understand the world's linguistic diversity.