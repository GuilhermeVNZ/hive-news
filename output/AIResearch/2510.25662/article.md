As artificial intelligence coding tools become increasingly popular, many programmers are developing mistaken beliefs about what these systems can actually do. A new study reveals that users frequently overestimate AI assistants' capabilities, leading to unproductive work habits and potential security risks in software development.

Researchers discovered that programmers commonly misunderstand basic limitations of AI coding tools like ChatGPT and GitHub Copilot. The most frequent misconception involves expecting these systems to access the internet or specific websites, despite most versions lacking this functionality. Users frequently asked AI assistants to scrape data from Wikipedia pages, analyze GitHub repositories, or pull real-time stock market information—tasks requiring web access the tools simply don't possess.

The research team used a two-phase approach to identify these misunderstandings. First, they brainstormed potential misconceptions based on existing literature and teaching experiences. Then, they analyzed 500 real programming conversations from an open dataset of interactions with an AI chatbot. The analysis focused specifically on Python programming tasks, the most common language in the dataset.

Methodology involved examining how users interacted with AI assistants during natural programming sessions. Researchers looked for instances where users requested features the chatbot couldn't provide, such as executing code, generating non-text outputs like graphs, or remembering information across separate conversations. The team developed a coding system to categorize different types of misunderstandings based on the chatbot's actual capabilities.

Results showed that beyond web access issues, users frequently misunderstood other fundamental limitations. Many expected AI assistants to execute code and report back results, despite these being text-only systems. Others requested algebraic computations, assuming the AI could function like a calculator. Some users even asked the chatbot to "clear the chat" or take actions on their local computers, treating the AI as if it had system-level control.

These misconceptions matter because they can lead to real-world problems in software development. When programmers overestimate AI capabilities, they may waste time trying to get tools to perform impossible tasks or fail to properly validate AI-generated code. The study found evidence of users asking AI assistants to guarantee their code would run without errors—something no static analysis tool can reliably promise.

Contextually, these findings highlight a growing challenge as AI becomes integrated into programming workflows. With different versions of AI assistants offering varying features, and new capabilities being added regularly, users struggle to form accurate mental models of what each tool can do. This confusion could impact software quality and security if programmers rely too heavily on AI outputs without understanding their limitations.

The study acknowledges several limitations, including that the dataset primarily represented users from IT communities and that researchers couldn't directly measure users' beliefs—only infer them from interaction patterns. Additionally, the rapid evolution of AI tools means that some capabilities that were unavailable during the study period may now exist in newer versions.

Ultimately, the research suggests that AI tool developers need to communicate capabilities more clearly to users. As one researcher noted, "If features are reasonably common, like search and code execution, standardized indicators might help users understand what's actually available." With AI programming assistants becoming standard tools for both beginners and experts, ensuring users understand their actual capabilities—not just their perceived ones—will be crucial for productive and secure software development.