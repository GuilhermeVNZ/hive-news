Autonomous drones are increasingly used for tasks like delivery, surveillance, and rescue, but safely landing in unpredictable, cluttered areas remains a major challenge. Current vision-based systems often struggle with changing conditions and lack the ability to explain their decisions, which is crucial for safety certification and real-world trust. A new approach from researchers at Macquarie University addresses this by blending artificial intelligence with symbolic reasoning, creating a system that not only identifies safe landing zones more accurately but also provides human-readable justifications for its choices.

The key finding is that this neuro-symbolic framework, called EURO SYM AND, significantly improves landing safety and interpretability. It uses two pipelines: an offline process where large language models (LLMs) help synthesize and verify safety rules from diverse scenarios, and an online pipeline where a compact vision model builds a scene graph that is evaluated against these rules. This allows the system to reason about obstacles, terrain flatness, and mission priorities, producing ranked landing options with clear explanations.

Methodologically, the team decouples perception from logic to enhance efficiency and verifiability. The perception stage employs a quantized SegFormer-B0 model for semantic segmentation, followed by lightweight post-processing to extract regions and their attributes—like surface type and slope—into a probabilistic semantic scene graph. The symbolic layer, implemented with Scallop, then applies pre-validated rules to assess hazards and score landing zones based on mission-specific needs, such as emergency landings prioritizing proximity to a center point or rescue operations emphasizing access to objectives.

Results from software and hardware simulations show that EURO SYM AND outperforms baseline methods in safety and efficiency. For example, it achieved a 17.4% increase in minimum obstacle distance (MOD) and a 21.9% reduction in touchdown-centroid distance (TCD) compared to a perception-only baseline, meaning it selects landing spots with better buffers and less maneuvering risk. In one case study, it correctly avoided a swimming pool and narrow areas where other methods failed, though it shared limitations in misclassifying rooftops due to upstream vision errors. The system runs on edge devices like the Jetson Orin Nano, with frame processing times around 1,668 milliseconds, making it practical for real-time drone operations.

Contextually, this work matters because it moves toward certifiable AI for autonomous systems, where transparency and reliability are essential. Drones used in emergencies or urban settings can now make decisions that are not only data-driven but also rule-based and auditable, helping operators trust and verify actions without relying on black-box models. This could enhance applications in disaster response, where quick, safe landings near hazards are critical.

Limitations include dependence on the accuracy of the underlying vision model; for instance, misclassifying roofs as landable surfaces led to errors that the reasoning layer could not override. Future work may focus on improving perception fidelity and integrating additional sensors to address such gaps, ensuring that the symbolic reasoning aligns perfectly with real-world visuals.