[VOICEOVER 0:00-0:05] What if AI could learn mathematical functions instead of just weights? [VISUAL DIRECTION] Animated text: "AI Learning Math Functions" with question mark pulsing

[VOICEOVER 0:05-0:20] Traditional neural networks use fixed activation functions, but new research reveals a game-changing alternative. [VISUAL DIRECTION] Show side-by-side comparison: MLP structure vs KAN structure with animated edges

[VOICEOVER 0:20-0:40] Researchers asked: Can we make AI more efficient and interpretable for scientific computing? [VISUAL DIRECTION] Zoom in on research paper title: "A Practitioner's Guide to Kolmogorov-Arnold Networks"

[VOICEOVER 0:40-1:00] They discovered Kolmogorov-Arnold Networks (KANs) replace fixed activations with learnable univariate functions on network edges. [VISUAL DIRECTION] Animated diagram showing edges transforming with B-spline functions

[VOICEOVER 1:00-1:30] KANs use specialized functions like B-splines and Chebyshev polynomials to localize transformations, reducing spectral bias that slows learning. [VISUAL DIRECTION] Show Figure 2 with zoom on coordinate transformation visualization

[VOICEOVER 1:30-1:50] In real applications, KANs achieved superior results in PDE solving, symbolic regression, and anomaly detection with fewer parameters. [VISUAL DIRECTION] Fast cuts showing PDE solving results, regression curves outperforming MLPs

[VOICEOVER 1:50-2:00] This breakthrough means AI can now learn mathematical expressions directly, accelerating discoveries in climate modeling and materials science. [VISUAL DIRECTION] Final text overlay: "KANs: Math-Learning AI for Science"