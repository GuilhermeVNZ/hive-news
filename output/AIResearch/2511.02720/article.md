As artificial intelligence powers more critical decisions in areas like medical imaging and autonomous driving, understanding why AI makes specific choices has become essential for building trust and ensuring safety. Convolutional neural networks (CNNs), which are widely used in computer vision, often operate as 'black boxes,' making their reasoning opaque to non-experts. The LLEXICORP system addresses this by automatically generating clear, human-readable explanations of AI decisions, bridging the gap between technical models and general users.

The key finding from the research is that LLEXICORP successfully translates the internal workings of CNNs into intuitive text, identifying and describing the visual concepts that influence AI predictions. For example, when analyzing an image of an American chameleon, the system pinpointed concepts like 'lizards' (contributing 40.62% to the prediction), 'distinct edges' (21.35%), and 'elongated stems' (21.29%), explaining how each relates to the final classification in simple terms.

Methodologically, LLEXICORP combines concept relevance propagation (CRP) with a multimodal large language model (LLM). CRP identifies the most important channels in the CNN—such as those detecting specific shapes or textures—and generates heatmaps to show where these concepts appear in the image. The LLM then assigns descriptive names to these concepts and creates explanations tailored to different audiences, from experts to non-technical stakeholders. This modular approach ensures that explanations are faithful to the AI's reasoning without requiring manual intervention.

Results from a qualitative evaluation with human participants demonstrate the system's effectiveness. In the study, 78% of evaluators agreed that LLEXICORP's descriptions accurately captured common features in representative images, and 83% found that the final summaries contained only information already present in the concept descriptions, avoiding misleading additions. Notably, for the top-ranked concepts, agreement rates exceeded 90% on aspects like reasonableness and usefulness, indicating that the most salient features are reliably explained. However, localization accuracy for lower-ranked concepts was lower, with only 61% agreement on whether concepts were correctly identified in the image, highlighting an area for improvement.

In practical terms, this advancement matters because it makes AI systems more accessible and accountable. For instance, in healthcare, doctors could use such explanations to verify AI diagnoses based on medical scans, while in autonomous vehicles, passengers might understand why a car identifies an obstacle. By providing transparent reasoning, LLEXICORP helps users trust AI decisions, potentially accelerating adoption in high-stakes environments.

Limitations of the system include reduced performance for less important concepts, where explanations may not significantly enhance interpretability. Additionally, the final summaries, while helpful for consolidation, only provided new insights in 33% of cases, suggesting they primarily repackage existing information rather than offering deeper analysis. Future work could focus on improving localization accuracy and enriching summary content to address these gaps.