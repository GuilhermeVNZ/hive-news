Businesses generate vast amounts of data from their daily operations, but understanding what drives success versus failure has long been a challenge. A new study introduces an AI-driven approach that pinpoints the key differences between efficient and problematic workflows, helping organizations optimize processes without guesswork. This research, detailed in a paper by Ali Wil Aalst from RWTH University, leverages event logs—records of activities like order processing—to automatically discover models that highlight why some executions lead to desirable outcomes while others result in inefficiencies, delays, or resource waste. For non-technical readers, this means companies can now identify and fix hidden bottlenecks more effectively, potentially saving time and money.

The key finding is that the researchers developed a method to separate business process executions into desirable and undesirable groups based on their outcomes. For example, in an order-handling scenario, desirable executions might follow a logical sequence like picking items, assembling the package, and printing a shipping label, leading to on-time delivery. Undesirable ones, however, could involve missteps such as printing labels before picking items, causing delays and errors. By analyzing these distinctions, the AI identifies specific patterns that differentiate successful from flawed processes, as illustrated in the paper's Figure 1, which shows trace variants and their frequencies in datasets like BPIC2012 and hospital billing logs.

Methodologically, the approach starts by encoding event logs into a feature space using declarative constraints—rules that define expected behaviors, such as 'if activity A occurs, activity B must follow.' The researchers then applied ensemble tree-based models, including random forests and gradient boosting, to extract decision rules that capture complex interactions between features. These rules were clustered using hierarchical clustering based on Jaccard distance to group similar patterns, and the most important rules per cluster were selected to filter the event logs. This allowed the team to derive separate process models for each cluster, focusing only on the most discriminative aspects. The entire framework, depicted in Figure 4 of the paper, combines constraint encoding, machine learning, and process discovery techniques like the Inductive Miner and Split Miner to ensure interpretability and accuracy.

Results analysis, based on real-life event logs such as BPIC2012 and hospital billing data, shows that the method effectively isolates critical patterns. For instance, in the BPIC2012 log, the AI identified rules like 'NotSuccession(l, p) violated'—meaning that if printing a label (l) is not followed by picking items (p), it often leads to undesirable outcomes. The models derived from these rules achieved higher discriminative metrics, such as alignment-based accuracy (a-acc) and F1-scores, compared to baseline models that did not account for outcome differences. As shown in Figure 6, the approach consistently improved the ability to distinguish desirable from undesirable traces across multiple experiments, with specific clusters yielding models that better represent efficient behaviors while avoiding inefficient ones.

In practical terms, this research matters because it enables businesses to move beyond one-size-fits-all process models. By focusing on outcome-specific patterns, organizations can tailor improvements to address root causes of inefficiencies, such as in logistics or healthcare billing, where small changes could reduce waste and enhance compliance. For regular readers, this means more reliable services and potentially lower costs, as companies adopt data-driven strategies to streamline operations.

However, the study acknowledges limitations, including potential multicollinearity among features that could destabilize the interpretation of individual rules. The paper notes that in some cases, similar rules had opposite coefficients in the regression model, indicating a need for caution and further refinement. Additionally, the method relies on the availability of labeled event logs, which may not always be straightforward to obtain, and future work could explore direct derivation of subprocess models to enhance robustness.