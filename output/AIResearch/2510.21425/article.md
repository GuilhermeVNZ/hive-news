Large language models (LLMs) like GPT and Llama can generate human-like text but often act as 'black boxes,' making it hard to understand their reasoning. This lack of transparency limits their use in high-risk areas like healthcare, where clear explanations are crucial. Researchers from the University of Hull have developed a roadmap to merge these AI systems with symbolic artificial intelligence, aiming to make them more interpretable and trustworthy.

The key finding is that integrating symbolic AI—which uses structured knowledge from sources like logic rules and knowledge graphs—can enhance LLMs' explainability without sacrificing performance. This approach, known as neurosymbolic AI, combines the learning power of neural networks with the reasoning clarity of symbolic systems. For instance, it helps models provide step-by-step justifications for decisions, reducing errors like hallucinations where AI invents false information.

Methodologically, the team conducted a systematic literature review, analyzing 177 studies from 2018 to 2025. They proposed a novel taxonomy that organizes integration into four dimensions: stages (such as pre-training or inference), coupling strategies (like loose or tight connections between components), architectural paradigms (e.g., pipelines where LLMs output symbolic forms), and perspectives (application-level versus algorithm-level focus). This framework guides how to embed symbolic elements into LLMs, such as injecting knowledge graphs during training or using logic solvers to verify outputs post-inference.

Results show that symbolic integration improves performance on benchmarks like GSM8K for math problems and FOLIO for logical reasoning, with some methods achieving state-of-the-art accuracy. For example, one approach increased performance by 26% over standard chain-of-thought techniques in deductive tasks. The analysis highlights that methods applied at the inference stage, such as retrieval-augmented generation, are common but often loosely coupled, whereas tighter integration at algorithmic levels remains underexplored.

In context, this work matters because it addresses real-world needs for reliable AI in sectors like medicine and law, where decisions must be transparent. By making AI responses interpretable, it builds user trust and could lead to safer deployments in critical applications. The roadmap encourages further research to refine these integrations, potentially enabling AI that not only answers questions but also explains how it arrived at those answers.

Limitations include challenges in scalability, as tightly coupled systems may require significant computational resources, and gaps in handling complex, multi-step reasoning across diverse domains. The study notes that current benchmarks often lack coverage for all reasoning modes, such as abductive inference, and calls for more robust evaluation metrics to ensure progress in this emerging field.