[VOICEOVER 0:00-0:05] What if AI could show its work like a math student? Right now, LLMs are black boxes—we can't see their reasoning.
[VISUAL DIRECTION] [Animated text: "AI BLACK BOXES" with question marks floating around neural network diagram]
[VOICEOVER 0:05-0:20] This creates huge problems in healthcare and high-risk fields where we need to trust AI decisions.
[VISUAL DIRECTION] [Show medical imagery fading to red warning symbols over AI interface]
[VOICEOVER 0:20-0:40] Researchers asked: Can we merge neural networks with symbolic AI to create transparent systems?
[VISUAL DIRECTION] [Split screen: left shows neural network nodes, right shows logical decision trees—arrows connect them]
[VOICEOVER 0:40-1:00] They discovered neurosymbolic integration provides step-by-step justifications, reducing hallucinations by 30% on reasoning tasks.
[VISUAL DIRECTION] [Show GSM8K math problem being solved with visible reasoning steps appearing beside each calculation]
[VOICEOVER 1:00-1:30] The method injects knowledge graphs during training and uses symbolic solvers to verify outputs, creating reliable explanations.
[VISUAL DIRECTION] [Animated flow: LLM output → symbolic verification → green checkmark with "EXPLAINABLE" text]
[VOICEOVER 1:30-1:50] This means AI can now provide transparent reasoning for medical diagnoses and critical decisions.
[VISUAL DIRECTION] [Show doctor reviewing AI diagnosis with visible reasoning trail—zoom on confidence metrics]
[VOICEOVER 1:50-2:00] The future of AI isn't just smart—it's accountable and trustworthy.
[VISUAL DIRECTION] [Final screen: "EXPLAINABLE AI = TRUSTWORTHY DECISIONS" with University Hull logo]