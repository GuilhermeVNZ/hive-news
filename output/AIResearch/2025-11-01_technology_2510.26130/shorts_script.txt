[VOICEOVER 0:00-0:05] What if your AI coding assistant is secretly failing on real projects while acing test benchmarks?
[VISUAL DIRECTION] Animated text: "84-89% vs 25-34%" with dramatic red arrow pointing downward

[VOICEOVER 0:05-0:15] New research from Nature/Science reveals a shocking performance gap in AI coding tools that affects every developer relying on assistants like GitHub Copilot
[VISUAL DIRECTION] Show coding interface with AI suggestions, then transition to error messages popping up

[VOICEOVER 0:15-0:35] Researchers created RealClassEval, a benchmark using actual open-source code, to test seven state-of-the-art LLMs including GPT-4.1 and Codestral
[VISUAL DIRECTION] Show benchmark creation process with code snippets flowing from open-source repositories into testing framework

[VOICEOVER 0:35-0:55] They discovered AI models that achieve 84-89% on synthetic tests plummet to just 25-34% when generating real classes for complete projects
[VISUAL DIRECTION] Zoom on performance comparison chart showing dramatic drop, highlight the 53-62 percentage point gap

[VOICEOVER 0:55-1:15] The study proves this isn't about memorization—models performed identically on seen and unseen data—but about handling complex code dependencies
[VISUAL DIRECTION] Show code dependency diagram with red error indicators at connection points

[VOICEOVER 1:15-1:35] Even detailed documentation only improved correctness by 1-3%, and retrieval-augmented generation introduced new dependency conflicts
[VISUAL DIRECTION] Display error analysis breakdown showing AttributeError, TypeError, and AssertionError as dominant failure types

[VOICEOVER 1:35-1:50] This research exposes critical limitations in current AI-assisted development and urges realistic expectations for production applications
[VISUAL DIRECTION] Show developer reviewing AI-generated code with caution symbols

[VOICEOVER 1:50-2:00] Your AI coding assistant might ace the tests, but it's failing where it matters most—in real projects
[VISUAL DIRECTION] Final screen with text: "AI Coding Reality Check: 53-62% Performance Gap"