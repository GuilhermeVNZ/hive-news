As AI-generated images become more realistic, distinguishing real from fake is increasingly difficult. Researchers have developed a system that not only detects these images with high accuracy but also explains its reasoning, making it suitable for use on low-power devices like smartphones and edge computers. This advancement addresses growing concerns about misinformation and synthetic media in everyday digital interactions.

The key finding is a method that combines a lightweight neural network called Faster-Than-Lies with a vision-language model to classify and localize artifacts in low-resolution 32x32 pixel images. The system achieves 96.5% accuracy in identifying AI-generated content, even when images are altered with noise, blur, or other perturbations designed to evade detection. It provides human-understandable explanations by highlighting specific anomalies, such as unrealistic lighting or anatomical errors, and generates text descriptions for each finding.

Methodologically, the team trained the Faster-Than-Lies classifier on an augmented version of the CiFAKE dataset, which includes 1.2 million real and fake images from sources like Stable Diffusion and GigaGAN. They introduced various perturbations—including Gaussian noise, motion blur, and adversarial attacks—to enhance robustness. For explainability, they used an autoencoder to create reconstruction error maps that pinpoint artifact locations, paired with the Qwen2-VL-7B model to translate these findings into plain language. This approach ensures the system runs efficiently, with inference times of 175 milliseconds on an 8-core CPU and a model size of 98 MB, meeting deployment constraints for resource-limited environments.

Results show that the model maintains high performance under adversarial conditions, with accuracy improving from 94% to 96.5% after retraining on perturbed data. Artifacts are categorized into eight semantic groups, such as geometric anomalies, texture issues, and lighting problems, allowing for detailed analysis. For example, in one test image, the system identified over-sharpening, aliasing on edges, and color coherence breaks as indicators of fakery. The integration with the vision-language model enables it to generate explanations like "Unrealistic specular highlights" or "Misaligned bilateral symmetry," making the output accessible to non-experts.

In context, this technology has broad real-world implications. It could enhance social media moderation by quickly flagging synthetic content, support forensic analysis in low-resolution security footage, and aid industrial inspection by detecting defects in machinery imagery. Applications in medical imaging, such as analyzing X-rays on mobile devices, are also feasible, providing reliable diagnostics in resource-scarce settings. The system's deployability on edge devices means it can operate without constant internet connectivity, preserving privacy and reducing latency.

Limitations include dependency on the vision-language model, which may not cover all AI sources or perturbations, such as certain shadow inconsistencies or material property mismatches. The current implementation is optimized for 32x32 images and might not perform as well on smaller resolutions. Additionally, localization accuracy could be improved with deeper analysis techniques, and the trade-off between speed and accuracy remains a constraint, potentially affecting real-time applications in highly dynamic environments.