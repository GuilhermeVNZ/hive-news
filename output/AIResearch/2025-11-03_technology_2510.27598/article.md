Artificial intelligence is poised to transform scientific discovery by automating research workflows, but a new study shows that current AI agents still fall short in realistic, demanding scenarios. Researchers have introduced InnovatorBench, a benchmark that evaluates AI agents' ability to perform end-to-end research tasks, from hypothesis formation to code execution and analysis. This platform uncovers that while AI shows promise in code-driven activities, it struggles with fragile algorithms, impatience, and poor decision-making over extended periods. The findings, detailed in a recent paper, emphasize the need for AI systems that can handle the complexities of genuine scientific inquiry, moving beyond simple replication to true innovation.

Key findings from the study indicate that frontier AI models, including Claude-4, GPT-5, GLM-4.5, and Kimi-K2, achieve non-zero scores on InnovatorBench but exhibit significant weaknesses. For instance, these models perform better on data-related tasks like data construction and filtering, where minor errors are tolerable, but falter in algorithm-related domains such as loss design and reward design. The researchers found that models often enter high-frequency loops, mismanage resources, and rely excessively on template-based reasoning, leading to suboptimal outcomes. In one case, an agent terminated a training process prematurely due to impatience, despite having ample time remaining, as shown in Figure 4(a) of the paper.

Methodology involved deploying a ReAct-based agent within ResearchGym, an environment that supports rich action spaces, distributed computing, and long-horizon monitoring. InnovatorBench comprises 20 tasks across six research domains, requiring agents to complete activities in up to 36 hours. The benchmark assesses correctness, performance, quality, and uncertainty through external evaluations, preventing agents from hacking the system. By comparing performance with and without hints that provide ground-truth solutions, the study highlights agents' reliance on replication over innovation, with hints improving scores in exploratory tasks but not in straightforward ones.

Results analysis, referencing figures like Figure 2 and Table 5, shows that models like GPT-5 excel in scaffold construction, achieving a score of 60.07, but overall weighted averages remain low, around 12.04 for top models. The paper notes that test-time scaling requires over 11 hours for performance to plateau on InnovatorBench, compared to just 1.75 hours on simpler benchmarks like PaperBench, underscoring the benchmark's difficulty. Case studies reveal specific failures, such as resource mismanagement where agents launch conflicting processes on the same GPU, as depicted in Figure 4(b), and selection of suboptimal libraries that hinder efficiency.

In context, these limitations matter because AI agents are increasingly considered for autonomous research in fields like medicine and climate science. The inability to manage long-term tasks or innovate beyond templates could delay real-world applications, where human-like reasoning and adaptability are crucial. The study suggests that improving AI's reliability in tool use and planning is essential for it to become a true collaborator in scientific endeavors.

Limitations of the research, as noted in the paper, include the benchmark's focus on a limited set of tasks, which may not fully capture interdisciplinary challenges. Future work should expand diversity and explore human-AI collaboration to enhance generalization. The authors caution that while InnovatorBench sets a high bar, it highlights the gap between current AI capabilities and the demands of cutting-edge research, urging further development in agent-based systems.