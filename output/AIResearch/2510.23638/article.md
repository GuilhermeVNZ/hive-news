A new approach to artificial intelligence hardware could make future AI systems dramatically more energy-efficient by directly implementing neural network computations in physical devices. Researchers have developed a method that bridges the gap between mathematical theory and electronic physics, creating AI systems that operate more like biological brains than conventional computers.

The key finding demonstrates that tunnel diodes—electronic components with unique current-voltage characteristics—can directly implement the mathematical functions needed for Kolmogorov-Arnold Networks (KANs), a type of neural network. Unlike traditional AI systems that require separate digital circuits for activation functions, this approach uses the intrinsic physical properties of devices to perform computations. The researchers showed that different device configurations—specifically armchair-oriented and zigzag-oriented tunnel diodes with thicknesses of 15 Å and 21 Å—produce distinct nonlinear responses that can serve as the building blocks for neural network operations.

The methodology combines first-principles device simulations with neural network design. Using density functional theory and nonequilibrium Green's function simulations, the team modeled NbSi2/HfSi2 heterostructure tunnel diodes to extract their current-voltage characteristics. These physical responses were then mapped directly to the basis functions needed for KANs, creating what they call "KANalogue"—a fully analog implementation where each network node corresponds to a physical device. The approach eliminates the need for digital activation function circuits that typically break the vision of fully analog computing pipelines.

Results from testing on standard benchmarks show promising performance with significantly reduced complexity. On MNIST handwritten digit recognition, the KANalogue achieved 97.71% accuracy with just 26,570 parameters, comparable to traditional multilayer perceptrons requiring many more parameters. The system maintained 88.89% accuracy on Fashion-MNIST and reached 50.27% on the more challenging CIFAR-10 dataset. The researchers found that combining multiple tunnel diode functions—particularly armchair 21 Å and zigzag 21 Å configurations—provided the best performance, demonstrating that device physics diversity translates directly to richer computational capabilities.

The real-world implications are substantial for energy-constrained applications. By eliminating digital conversion steps and leveraging physical computation, this approach could enable AI systems that consume orders of magnitude less power—critical for edge computing, mobile devices, and embedded AI applications. The inherent robustness of distributed analog computation also provides natural tolerance to device variations and noise, with testing showing that performance degradation becomes less pronounced as network size increases.

However, several limitations remain. Current evaluations are based on simulated device curves rather than fabricated hardware, and performance on complex datasets like CIFAR-10 indicates scalability challenges. The fixed device responses may not provide sufficient diversity for more demanding tasks, and practical deployment will require addressing circuit-level issues like thermal drift, noise, and device mismatch. The researchers note that future work needs to focus on fabricating actual device arrays and developing strategies for programmable basis functions to support continual learning.