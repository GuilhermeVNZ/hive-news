Artificial intelligence has proven powerful in scientific applications, as evidenced by recent Nobel Prizes in chemistry and physics that recognized AI contributions. However, current AI, particularly deep learning, lacks semantics—the ability to translate its findings into understandable scientific knowledge. This limitation makes AI's discoveries unsatisfactory and hinders its potential for uncovering new facts about the world. In a paper by Artur d’Avila Garcez and Simon Odense, researchers argue that neurosymbolic AI offers a solution by providing a formal framework that links logic with neural networks, addressing this semantic gap and improving AI's reliability and interpretability.

The key finding is that neurosymbolic AI integrates symbolic logic, which is inherently comprehensible and generalizable, with subsymbolic neural networks that excel at pattern matching from data. This combination allows AI systems to perform both high-level reasoning and statistical inference, overcoming the opacity and poor generalization of pure deep learning. The researchers demonstrate that by encoding logical knowledge into neural networks, AI can better align its outputs with interpretable structures, enhancing its utility in scientific contexts.

The methodology involves two main procedures: encoding and extraction. Encoding injects prior logical knowledge into neural networks before training, using approaches like strong encoding (directly setting network weights to implement logic) or soft encoding (incorporating logic through loss functions that guide learning). Extraction, conversely, derives interpretable rules from trained networks, forming a cycle where knowledge is added, learned, and then extracted to refine understanding. This neurosymbolic cycle, depicted in Figure 1 of the paper, promotes modularity and user interaction, ensuring that the knowledge used comports with the dataset and provides non-redundant information.

Results from the paper show that this framework unifies various encoding techniques under a semantic definition, where neural network states map to logical interpretations. For instance, in propositional logic, neurons can represent atoms like 'A' and 'B', with network states corresponding to truth assignments. The aggregation of these states defines the network's beliefs, aligning with logical models. The researchers formalize this with fidelity measures to quantify how closely networks approximate logical bases, using distances in probabilistic or fuzzy logics to ensure consistency.

In real-world terms, this approach matters because it addresses critical limitations in AI, such as lack of reliability, energy efficiency, and fairness. By making AI's reasoning transparent and grounded in logic, it could lead to more trustworthy systems in fields like medicine or autonomous systems, where understanding why an AI makes a decision is as important as the decision itself. For example, in image classification, adding logical constraints about object hierarchies can improve generalization beyond training data, reducing errors in unpredictable scenarios.

Limitations noted in the paper include the absence of general conditions to ensure that encoded knowledge always benefits learning, as the utility depends on how well the logic aligns with the data. Additionally, identifying appropriate semantic mappings remains challenging, analogous to problems in philosophy of mind, such as the 'Wordstar problem' where arbitrary encodings can make semantics meaningless without proper constraints. Future work is needed to develop principles that guarantee encodings preserve relevant information and enhance generalization effectively.