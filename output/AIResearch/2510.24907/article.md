Transformers have revolutionized how computers understand images, but their inner workings remained a mystery—until now. Researchers have developed a method to peer inside these complex AI systems, revealing how they reconstruct three-dimensional scenes from multiple viewpoints. This breakthrough in understanding could make AI systems more reliable for critical applications like autonomous vehicles and medical imaging, where knowing how decisions are made matters as much as the decisions themselves.

The key finding shows that transformers don't process 3D scenes all at once, but rather refine their understanding through an iterative process. As the AI analyzes different views of the same scene, it gradually aligns and improves its geometric understanding layer by layer. The researchers discovered that self-attention layers—a core component of transformers—are responsible for 94% of the error reduction in aligning different viewpoints, while cross-attention layers actually increase alignment error by 11%.

To uncover these insights, the team developed specialized probes that analyze the AI's internal representations at each processing step. They trained these probes to predict 3D point positions from the transformer's intermediate states, allowing them to track how the AI's understanding evolves throughout its processing layers. The method focuses on analyzing skip connections—the pathways that carry information between layers—which proved more stable and interpretable than analyzing direct layer outputs.

The researchers applied their approach to DUSt3R, a transformer model designed for 3D reconstruction from multiple images. Their analysis revealed that the model processes scenes in distinct stages: early layers establish basic geometric relationships, while later layers refine these into precise 3D coordinates. In simple cases with significant overlap between views, the AI resolves rotation components quickly, while challenging scenarios with opposing viewpoints require multiple iterations to achieve correct alignment.

One surprising discovery was how the AI handles correspondence—matching points between different views. Early in processing, the system identifies semantic correspondences (matching similar-looking objects), which gradually refine into geometric correspondences (matching the same physical points). This refinement process improves correspondence accuracy from 40% at the input stage to 60% after just six processing blocks.

The practical implications are significant for applications requiring reliable 3D understanding. Medical imaging systems could benefit from knowing exactly how AI reconstructs anatomical structures, while autonomous vehicles could have more transparent scene understanding. The method also provides a foundation for improving transformer architectures, as engineers can now see which components contribute most to accurate 3D reconstruction.

However, the approach has limitations. The probes require careful design to avoid solving the 3D reconstruction task themselves, and the analysis focuses on specific transformer architectures. The researchers also note that their visualization method works best with scale-sensitive parameterizations, as the scale-invariant nature of many transformers can complicate interpretation. Future work will need to extend these techniques to more complex scenarios and different types of transformers.

This research represents a crucial step toward making AI systems more transparent and trustworthy. By revealing how transformers build their understanding of 3D space, scientists can now work on improving these systems with clearer insight into their internal mechanisms, potentially leading to more reliable AI for critical real-world applications.