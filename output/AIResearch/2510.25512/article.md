Artificial intelligence systems can outperform humans in many tasks, but understanding how they reach their conclusions has remained a fundamental challenge. This 'black box' problem limits AI's use in critical applications like healthcare, where doctors need to trust automated diagnoses. A new method called FaCT (Faithful Concept Traces) now provides transparent explanations for neural network decisions, showing exactly which visual concepts the AI uses to classify images.

The researchers discovered that neural networks can be redesigned to inherently explain their reasoning through concept-based decomposition. Unlike previous approaches that approximated AI decision-making after the fact, FaCT builds explanations directly into the network architecture. The system identifies and tracks specific visual concepts—like 'wheels,' 'yellow color,' or 'animal eyes'—throughout the decision process, showing how much each concept contributes to the final classification.

The method combines two key components: B-cos networks that maintain alignment between inputs and decisions, and sparse autoencoders that extract interpretable concepts from intermediate layers. This architecture ensures that every classification can be fully decomposed into concept contributions. For example, when identifying a school bus, the system might show that 'yellow color' contributes 4.3% to the decision while 'wheel shape' contributes 3.4%, with all concept contributions summing to 100% of the final classification score.

The results demonstrate that FaCT maintains competitive performance on ImageNet classification while providing faithful explanations. In user studies, participants found FaCT's concept visualizations significantly more interpretable than previous methods, with particular improvement for early-layer concepts like background patterns and simple shapes. The system also revealed how neural networks get confused—showing that misclassifications between similar categories like basketball and volleyball often stem from shared concepts like 'jerseys' and 'balls' that activate for both classes.

This transparency matters because it builds trust in AI systems for real-world applications. Doctors could verify which medical features an AI uses for diagnosis, while autonomous vehicle developers could understand why a system classifies objects certain ways. The method also introduces a new metric called C2-score that automatically evaluates how consistently concepts are recognized across different images, avoiding reliance on limited human annotations.

However, the approach doesn't guarantee that all extracted concepts will be easily interpretable to humans. Some concepts remain abstract patterns that don't align with human intuition. The current method also requires training separate concept extraction modules for each layer of interest, though it adds minimal computational overhead during inference.

The work represents a significant step toward AI systems that can explain their reasoning in human-understandable terms, moving beyond simply showing which pixels mattered to revealing the actual concepts driving decisions.