Large language models (LLMs) can now mimic a fundamental human learning process, offering scientists a new way to study how the brain forms associations. Researchers from the Max Planck Institute for Software Systems have shown that these AI systems exhibit associative learning—linking co-occurring items—in patterns strikingly similar to those observed in humans. This discovery positions LLMs as scalable model organisms for cognitive neuroscience, enabling experiments that are difficult or impossible to conduct in biological systems.

The key finding is that LLMs display non-monotonic plasticity during learning. When repeatedly exposed to pairs of tokens, the models' internal representations change in a U-shaped curve: moderately similar pairs differentiate, while highly similar or dissimilar pairs remain stable or integrate. This pattern, known as the Non-Monotonic Plasticity Hypothesis, was observed during a consolidation phase when accuracy stabilized at over 90%, mirroring human hippocampal dynamics.

Methodologically, the team adapted a cognitive neuroscience paradigm to an in-context learning setting. They presented token pairs repeatedly to six open-source LLMs, including Llama and Mistral variants, and measured changes in cosine similarity between hidden representations. Using an optimized search algorithm, they systematically controlled pairwise similarity before learning, sampling across a spectrum from 0.1 to 0.95 similarity.

Analysis revealed three distinct learning phases across models. During encoding, accuracy increased steeply as associations formed. In consolidation, performance stabilized while representations underwent structured reorganization—showing the U-shaped change pattern. Some models exhibited a forgetting phase where accuracy declined despite continued exposure. The researchers found that vocabulary interference—how much a paired item competes with the model's broader knowledge—modulates these changes. Higher interference consistently amplified differentiation, particularly for mid-similarity pairs.

The real-world implications are significant for both AI development and neuroscience. These findings suggest LLMs can serve as testbeds for generating hypotheses about neural representation changes, offering control and scalability unattainable in human studies. However, limitations remain—LLMs differ mechanistically from biological brains, and the vocabulary interference measure approximates rather than fully captures complex memory dynamics. The study focused on final layer representations, leaving earlier layer dynamics and developmental trajectories for future work.