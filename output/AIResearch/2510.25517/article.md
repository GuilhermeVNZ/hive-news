Computer programs that learn logical rules often create internal concepts that remain nameless—hindering human understanding and reuse. Researchers have now demonstrated that large language models (LLMs) can automatically assign meaningful names to these invented predicates, making complex logical theories more interpretable.

The key finding is that LLMs can successfully rename placeholder predicates in logic programs with semantically appropriate labels. In experiments across multiple domains—from family relationships to mathematical operations—models like ChatGPT-4o and ChatGPT-o3mini correctly identified names for most predicates, such as renaming 'h0' as 'parent' in family relationship rules and 'P' as 'coauthors' in academic collaboration rules.

Methodology involved a systematic pipeline where researchers presented LLMs with logic rules containing unnamed predicates (like 'inv1', 'h0', 'P') and asked them to suggest meaningful names. The approach used zero-shot prompting, where models received only the logical rules and instructions without training examples. For challenging cases, researchers employed a judgment step where multiple LLMs evaluated and ranked name suggestions.

Results analysis showed varying performance across models. ChatGPT-o3mini achieved the highest accuracy, correctly naming 11 out of 16 mathematical predicates including 'is_even', 'sum', and 'greater_than'. In family relationship studies, models successfully identified 'grandparent', 'common_ancestor', and 'cousins' from placeholder names. However, performance dropped with complex real-world datasets like mutagenesis, where domain-specific knowledge proved challenging.

This matters because unnamed predicates in logic programming create significant barriers to understanding and reusing automated reasoning systems. When AI systems invent intermediate concepts during learning, those concepts typically remain as meaningless symbols like 'inv1' or 'h0'. The ability to automatically assign human-understandable names makes these systems more transparent and accessible to programmers, researchers, and potential users across scientific domains.

Limitations include the approach's dependence on the semantic content available in the rules themselves. For highly specialized domains with minimal contextual clues, LLMs struggled to infer appropriate names. The paper also notes that some models occasionally failed to follow output format instructions or suggested syntactically invalid predicate names, requiring additional processing steps.