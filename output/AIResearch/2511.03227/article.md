Imagine crafting a movie script where you can visually rearrange scenes, experiment with different storylines, and generate accompanying videos and audio—all through a simple drag-and-drop interface. This is now possible thanks to a new AI system that transforms storytelling from a linear process into an interactive, visual experience. For anyone who creates content—from filmmakers and game developers to educators and marketers—this approach makes complex multimedia production accessible without requiring technical expertise.

The key innovation is representing stories as node-based graphs, where each node corresponds to a scene or event that can be iteratively created, expanded, and edited using AI. Unlike traditional systems that generate content in a single pass, this framework supports branching narratives, parallel storylines, and selective editing while maintaining structural coherence. As shown in Figure 1, users can visualize their entire story structure and make changes at any level, from individual scene details to overall tone.

The system works through a task-selection agent that interprets natural language input and routes requests to specialized components. A Reasoner decomposes story concepts into nodes and relationships, a Diagrammer formats them into structured JSON (as demonstrated in Appendix A.1), and Generators produce corresponding multimedia content. Text is generated by GPT-4.1, images by GPT-Image-1, audio by GPT-4o's text-to-speech, and videos by OpenAI's Sora. This modular architecture allows the system to handle diverse media types while keeping the underlying story graph consistent.

Quantitative evaluation showed strong performance in generating both linear and branching narratives. For linear stories, the system produced correct graphs in 8 out of 10 trials (80% success rate), while for branching narratives it achieved perfect results (10 out of 10 trials, 100% success rate). More importantly, qualitative observations revealed practical advantages: users could manually edit specific nodes (like changing object descriptions) or use AI for structural revisions (such as adjusting tone). The system also supported global edits where all nodes could be rewritten simultaneously while preserving the story structure, as illustrated in Figures 13 and 14.

This technology matters because it lowers barriers to multimedia creation. Instead of working with separate tools for text, images, and video, creators can now develop integrated stories through a unified interface. The node-based visualization makes complex narrative structures understandable, while the AI assistance handles time-consuming generation tasks. This could transform fields like education, where teachers could create interactive lessons, or independent media production, where small teams could produce professional-quality content without extensive resources.

The system does have limitations. It relies heavily on text-based models to maintain consistency across media generations, which can sometimes produce incoherent results when stories become complex. Handling larger graphs also presents challenges, as the current implementation may struggle with maintaining narrative coherence across many nodes. Future work will explore hierarchical approaches and conduct user studies to improve usability and understand the system's impact on creative workflows.