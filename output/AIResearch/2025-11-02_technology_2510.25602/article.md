Artificial intelligence systems are becoming more powerful, but they consume vast amounts of energy and computational resources. A new study reveals that the way these systems handle numbers internally—a process called quantization—can be optimized to make AI faster and more efficient without sacrificing accuracy. This research challenges the industry's current preference for floating-point number formats, showing that integer formats often perform better in fine-grained scenarios, potentially reducing hardware costs by up to 37%.

Researchers conducted a comprehensive comparison of low-bit quantization formats, focusing on how AI models like large language models (LLMs) represent numbers during computation. They found that while floating-point formats excel in coarse-grained quantization, integer formats outperform them in fine-grained settings. Specifically, the study demonstrated that MXINT8, an 8-bit integer format, consistently achieved higher accuracy and efficiency than its floating-point counterpart, MXFP8, in tasks involving AI model training and inference.

The methodology involved a systematic evaluation using theoretical frameworks and real-data experiments. The team developed a quantization signal-to-noise ratio (QSNR) metric to compare formats directly, analyzing how well each preserved data fidelity. They tested formats on various AI models, including Llama and Qwen architectures, with block sizes of 32 for MX formats and 16 for NV formats. Techniques like Hadamard rotation were applied to mitigate outliers, improving integer format performance in cases like NVINT4 surpassing NVFP4 after rotation.

Results from tensor-wise analysis showed that MXINT8 had an average QSNR of 40.35 dB, significantly higher than MXFP8's 31.50 dB, indicating better data preservation. In direct-cast inference tests, integer formats like MXINT8 and NVINT4 often matched or exceeded floating-point formats in accuracy metrics such as KL divergence and perplexity on datasets like WikiText2. For training, MXINT8 supported nearly lossless low-bit operations, with loss curves overlapping those of high-precision BF16 and outperforming MXFP8 in common-sense reasoning tasks.

This research matters because it addresses the growing computational demands of AI, which strain energy resources and hardware capabilities. By advocating for integer formats in fine-grained quantization, the findings could lead to more efficient AI accelerators, reducing area and energy costs. For instance, hardware analysis indicated that mixed-format configurations like MXINT8+NVINT4 could cut energy use by 34% compared to floating-point baselines, making AI deployment more sustainable and accessible.

Limitations of the study include its focus on specific formats and models, leaving open questions about generalizability to other AI architectures. The paper notes that performance trade-offs depend on factors like crest factor and granularity, and further research is needed to explore these dynamics in diverse applications. The code and datasets are publicly available for reproducibility, encouraging continued investigation into optimal quantization strategies.