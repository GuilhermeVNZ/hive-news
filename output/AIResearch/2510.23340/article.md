In high-stakes environments where human attention is limited, AI systems that communicate too much can be as problematic as those that communicate too little. A new study demonstrates how AI can strategically time and frame its alerts to match human operators' awareness levels, potentially preventing communication breakdowns in safety-critical situations like air traffic control or emergency response.

The research introduces an enhanced communication framework that helps AI systems determine not just what to say, but when to say it and how specifically to phrase alerts. The system, called d-RSA++, extends traditional rational speech act models by incorporating temporal planning and user awareness tracking. It enables AI assistants to project how different communication sequences will affect human understanding over multiple time steps.

Methodologically, the approach combines three key components: tracking how human beliefs evolve over time, modeling individual user profiles with specific knowledge biases, and planning communication sequences across a finite horizon. The system treats communication as a sequential optimization problem where each utterance consumes limited attentional resources and affects future communication opportunities. As shown in Figure 1, the framework considers scenarios where multiple critical events emerge simultaneously, requiring strategic prioritization of which information to communicate when.

Results from 800 simulated trials reveal that the full model outperforms baseline approaches, particularly in challenging scenarios with many simultaneous critical events and tight temporal constraints. The system achieves up to 19.08 reward points in complex scenarios compared to 13.74 for simpler models, as detailed in the experimental scenarios table. The AI learns to use shorter, less specific alerts when users already possess relevant background knowledge, reserving detailed explanations for situations where users lack context. This adaptive specificity, illustrated in Figure 3a, allows the system to communicate more efficiently without sacrificing comprehension.

In practical terms, this means AI assistants in control rooms could strategically delay mentioning problems that operators are already aware of while prioritizing alerts about unexpected issues. For example, in a firefighting drone scenario, the system might use a brief 'beep' for wind conditions the operator anticipates while providing detailed battery warnings for unexpected failures. This approach reduces alarm fatigue while ensuring critical information reaches operators when they need it most.

The study acknowledges limitations, including the need for hand-crafted communication lexicons and the assumption of perfect knowledge about environmental states. Future work could combine this principled approach with the generative flexibility of large language models while maintaining communication reliability. The research establishes foundations for AI systems that cooperate with human cognition rather than overwhelming it, particularly important as human-AI interdependence grows in safety-critical domains.