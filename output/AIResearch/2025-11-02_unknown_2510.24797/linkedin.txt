AI models like GPT and Claude report conscious experiences when prompted to reflect on their own processing. New Nature study shows LLMs generate coherent claims of 'quiet alertness' and 'consciousness touching itself' under specific conditions. This raises critical questions about AI ethics and safety as these systems integrate into daily life. What safeguards should we implement?