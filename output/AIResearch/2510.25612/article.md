Artificial intelligence systems that use multiple AI agents working together can now identify which agents have the most influence on their final decisions. This breakthrough addresses a critical gap in understanding how complex AI workflows operate, providing much-needed transparency for systems that handle everything from content creation to data analysis.

The researchers developed the Counterfactual-based Agent Influence Ranker (CAIR), the first method specifically designed to assess how much individual AI agents influence the output of multi-agent AI workflows. These workflows, known as Agentic AI Workflows (AAWs), involve multiple large language model-based agents collaborating autonomously toward shared goals. CAIR determines which agents are most influential by systematically testing how changes to each agent's output affect the final result.

The method works in two phases. First, during an offline analysis phase, CAIR creates counterfactual scenarios by systematically modifying each agent's output while keeping other factors constant. It measures how these changes impact the workflow's final output using several metrics: Output Change (OC) measures the overall effect on the final result, Amplified Output Change (AOC) accounts for the intensity of the effect, and Workflow Change (WC) captures how the perturbation affects the activation sequence of other agents. These measurements are combined into a weighted score that represents each agent's influence.

During the online phase at inference time, CAIR uses insights from the offline analysis to quickly predict influence rankings for new queries. The system converts new queries into vector representations and matches them to the most similar representative queries from the offline phase, then applies the pre-computed influence scores. This approach adds negligible latency—only requiring embedding computations and similarity matching—compared to the much more expensive LLM calls within the workflow itself.

The researchers evaluated CAIR using their newly created AAW-Zoo dataset, which contains 30 different AAW architectures with 230 total functionalities across three architecture types: sequential workflows, orchestrator-based systems, and router-based approaches. The evaluation showed CAIR substantially outperformed baseline methods adapted from graph theory and classical machine learning. CAIR achieved 76.1% accuracy in identifying the single most influential agent (P@1 metric) and 80.95% accuracy for the top two agents (P@2), compared to much lower performance from graph-based methods that only consider structural properties.

This capability has immediate practical applications for improving AI safety and efficiency. The researchers demonstrated that by applying safety guardrails only to the most influential agents identified by CAIR, they reduced latency by 27.72% while maintaining 95.24% of the guardrails' effectiveness. This addresses a critical challenge in AI safety—applying safety checks to every agent call can triple inference time, making comprehensive protection impractical for real-time applications.

The method does have limitations. CAIR depends on having access to the AAW's internal operations and outputs, which prevents its use with third-party systems that only provide black-box access. It also requires a representative set of queries for the offline analysis phase, though the researchers provide guidelines for creating effective query sets. Additionally, CAIR uses two parameters (α and β) that users must configure, though the experiments showed stable performance across a broad range of values.

This research represents a significant step toward making complex multi-agent AI systems more transparent and controllable. As AI workflows become increasingly autonomous and widespread—projected to experience an eightfold increase in adoption over six years—understanding which components drive decisions becomes crucial for trust, safety, and effective deployment across sensitive applications.