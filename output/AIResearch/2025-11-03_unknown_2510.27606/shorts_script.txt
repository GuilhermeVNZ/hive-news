[VOICEOVER 0:00-0:05] What if AI could teach itself to understand 3D space using only ordinary photos - no expensive sensors or human labels required?
[VISUAL DIRECTION] [Animated text: "AI TEACHING ITSELF 3D VISION" with question mark spinning]

[VOICEOVER 0:05-0:20] Today's AI has a critical weakness: poor spatial understanding limits robots, self-driving cars, and navigation systems. But researchers just found a breakthrough solution.
[VISUAL DIRECTION] [Show robot struggling with navigation, then transition to Spatial457 benchmark chart]

[VOICEOVER 0:20-0:40] The challenge: teaching AI spatial reasoning without costly 3D data collection. How can machines learn depth, position, and relationships from simple 2D images?
[VISUAL DIRECTION] [Show side-by-side: expensive 3D sensor setup vs. ordinary camera]

[VOICEOVER 0:40-1:00] Enter Spatial-SSRL - a self-supervised method where AI generates its own training signals. The system learns by reordering shuffled image patches, identifying flipped regions, and predicting depth order.
[VISUAL DIRECTION] [Animated sequence showing patch reordering, region flipping, depth prediction]

[VOICEOVER 1:00-1:30] Using only public datasets like COCO and DIODE, the AI teaches itself through reinforcement learning. The 3B parameter model gained seven points on Spatial457 benchmark - from 53.39% to 56.53% on 3DSRBench.
[VISUAL DIRECTION] [Zoom on performance charts from paper, highlight 7-point improvement]

[VOICEOVER 1:30-1:50] This means more accessible, scalable spatial intelligence for autonomous vehicles, robotics, and embodied AI - without sacrificing general capabilities. The AI actually improved on non-spatial tasks too.
[VISUAL DIRECTION] [Show applications: self-driving car, robot arm, drone navigation]

[VOICEOVER 1:50-2:00] AI is learning to see our world in 3D - and teaching itself how to do it.
[VISUAL DIRECTION] [Final text overlay: "SELF-TAUGHT SPATIAL INTELLIGENCE" with rotating 3D cube animation]