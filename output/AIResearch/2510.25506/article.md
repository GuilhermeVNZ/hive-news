When scientists publish research, other researchers should be able to repeat their experiments and get similar results. This fundamental principle of science—called reproducibility—is failing dramatically in artificial intelligence research involving large language models like ChatGPT. A comprehensive analysis of 86 recent AI studies reveals that only 5 were even suitable for reproduction attempts, and none of those produced identical results when retested.

The researchers focused on studies presented at two major software engineering conferences (ICSE and ASE) that used OpenAI's commercial language models. They developed a systematic framework to attempt reproduction of these studies using the exact same methods and data described in the original papers. Out of 65 studies that used OpenAI services, only 5 contained enough detail and functional code to even attempt reproduction.

The reproduction attempts revealed stark inconsistencies. In one study about translating programming languages, the original paper reported a 79.9% success rate for Python-to-Go translations, but the reproduction attempts showed success rates ranging from 45.6% to 55.3%. Another study on code summarization showed completely different performance patterns when retested, with some metrics reversing their relationships entirely.

The researchers used a statistical method called Bayesian bootstrapping to analyze their results, running each experiment multiple times to account for the inherent variability in language model outputs. They found that even the studies that seemed partially reproducible showed significant differences from the originally reported results.

Several key factors prevented reproduction attempts. Over half of the studies (35 out of 65) didn't provide accessible data, while 15 others had code that simply wouldn't run. Many studies failed to specify critical details like which exact model version they used—only saying "ChatGPT" or "LLM" without specifying the model or version. Only 29 studies reported the temperature setting (which controls randomness in AI outputs), and just 14 mentioned other important configuration parameters.

The problem extends to the badges meant to signal research quality. The study examined 19 papers that had earned ACM artifact evaluation badges, which are supposed to indicate that other researchers can reproduce the work. Yet 11 of these "reusable" artifacts couldn't actually be reused, and 4 were completely non-functional. In three cases, the resources referenced in the papers were no longer available.

This reproducibility crisis matters because it undermines the scientific foundation of AI research. When results can't be verified, it becomes difficult to build cumulative knowledge or trust the findings. The problem is particularly acute with commercial AI models that change frequently and lack transparency about updates. As the authors note, without structural improvements, "we cannot build a reliable body of knowledge and, consequently, cannot be trusted."

The study's limitations include focusing only on OpenAI models and specific conferences, but the findings highlight a broader challenge for AI research. The authors recommend that researchers document their experiments more thoroughly, share complete versioned artifacts, and that conferences strengthen their evaluation criteria. As AI continues to evolve rapidly, ensuring reproducible research becomes increasingly critical for scientific progress.