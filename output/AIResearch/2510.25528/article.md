Artificial intelligence systems have traditionally excelled at tasks with clear right and wrong answers, like math problems and programming challenges. But real-world reasoning often involves ambiguous questions where verification is difficult—from creative writing to complex analysis. A new approach called General Zero Reinforcement Learning now enables AI models to develop sophisticated reasoning skills across diverse domains, even when there's no straightforward way to check if their answers are correct.

The researchers developed a unified framework that combines verifiable tasks (like mathematics with clear answers) with non-verifiable tasks (like creative writing and general reasoning) during training. This approach allows reasoning behaviors learned from structured problems to transfer to more open-ended scenarios. The method addresses a critical limitation in current AI systems: while they can solve well-defined STEM problems, they struggle with tasks where responses are difficult to verify objectively.

To implement this approach, the team used reinforcement learning algorithms on base language models without any supervised fine-tuning phase. They trained Qwen3-8B-Base and Qwen3-14B-Base models using a combination of mathematical problems with verifiable answers and general reasoning tasks where responses were evaluated by a separate reward model. The system was designed to generate responses with both thinking processes (within <thinking> tags) and final answers (within <answer> tags), mimicking human reasoning patterns.

A key innovation addresses what researchers call the "reward hacking" problem—where AI models learn to maximize rewards through superficial means rather than genuine reasoning. The team introduced a smooth length penalty that prevents models from producing excessively verbose responses without substantive content. This penalty encourages coordinated growth between thinking content and answer content, ensuring the model develops meaningful reasoning processes rather than just generating longer text.

Experimental results demonstrate significant improvements across multiple benchmarks. On mathematical reasoning tasks like MATH-500 and AIME problems, the General Zero-RL models achieved performance gains of up to 16.9% over baseline models. More importantly, on general reasoning tasks including MMLU-Pro and GPQA-Diamond—which test broad knowledge and analytical skills—the models showed consistent improvements, with the Qwen3-14B model reaching 56.1% on MMLU-Pro and 58.0% on GPQA-Diamond. The models also performed competitively on creative tasks like Arena-Hard and WritingBench, generating coherent and meaningful content.

The research reveals that training exclusively on verifiable tasks fails to transfer reasoning capabilities to broader domains. In ablation studies, models trained only on mathematical problems showed a 13.3% performance drop on general-domain benchmarks compared to models trained with the multi-task approach. This underscores the importance of incorporating diverse task types during training.

For everyday applications, this advancement means AI systems could become more useful in scenarios requiring nuanced judgment and creative problem-solving—from helping with complex decision-making to generating insightful analysis of ambiguous situations. The ability to reason effectively without clear verification mechanisms could make AI assistants more valuable in professional settings where answers aren't always black and white.

The approach does have limitations. The current work didn't explore programming-related tasks, which require specialized verification components like code execution sandboxes. Additionally, the models weren't compared against instruction-tuned variants that use chain-of-thought reasoning, leaving open questions about how the method compares to other reasoning-enhancement techniques. Future work could explore integrating code-related data and comparing performance against more diverse baselines.

This research represents a step toward AI systems that can reason effectively across the full spectrum of human knowledge domains, not just those with easily verifiable answers. By enabling models to develop transferable reasoning skills, the approach moves us closer to artificial intelligence that can navigate the complexity and ambiguity of real-world problems.