Artificial intelligence systems that dominate academic tests are failing where it matters most: making money in live financial markets. A new study reveals that large language models (LLMs) excelling on standardized benchmarks show virtually no correlation with actual trading performance, challenging assumptions about what constitutes true AI intelligence.

The key finding demonstrates that high scores on general AI benchmarks don't translate to financial success. Researchers tested 18 different LLM families—including GPT-5, Claude-Opus-4.1, and Grok-4—in live trading environments over 50 days. The results show a near-zero correlation (Spearman correlation of 0.054) between benchmark performance and stock market returns. In prediction markets, the relationship was actually negative (-0.38), meaning models with higher benchmark scores performed worse.

Researchers developed LiveTradeBench, a testing platform that streams real market data and eliminates dependence on historical backtesting. The system evaluates models across two distinct environments: traditional U.S. stock markets and Polymarket prediction markets. At each trading step, models observe current prices, news, and portfolio positions, then output allocation decisions across multiple assets. This approach captures the uncertainty and feedback delays inherent in real financial markets, unlike static academic benchmarks.

The data reveals stark performance differences. While GPT-4.1 achieved the highest cumulative return in stock markets (over 6%), it performed poorly in prediction markets (return below -30%). Models displayed distinct trading styles—some adopted conservative strategies with smaller drawdowns, while others pursued aggressive gains accepting higher volatility. Analysis of decision-making patterns showed models relying heavily on price information (98.4% of reasoning references) and market history (82.6%), with news playing a smaller role (22.5%).

This research matters because it exposes a fundamental limitation in how we evaluate AI systems. Financial markets represent a real-world test where decisions have immediate consequences, unlike controlled academic environments. The findings suggest that current AI benchmarks may be measuring the wrong capabilities for practical applications. As AI systems increasingly handle real-world tasks from healthcare to autonomous systems, understanding this performance gap becomes crucial.

The study acknowledges several limitations. The environment doesn't include transaction costs or market frictions that would affect real trading profitability. The action space is constrained by current LLM capabilities, limiting the complexity of possible decisions. Additionally, the observation space provides only truncated news content, preventing models from accessing full contextual information.

What remains unknown is whether these limitations fundamentally constrain AI performance or whether future systems can overcome them. The research opens critical questions about how to develop AI that can genuinely adapt to dynamic, uncertain environments rather than simply excelling at predefined tasks.