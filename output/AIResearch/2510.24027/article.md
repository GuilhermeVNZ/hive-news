Cities and researchers can now deploy fewer sensors while maintaining accurate predictions for traffic flow and air quality, thanks to a new AI method that intelligently selects which monitoring locations provide the most valuable data. This breakthrough addresses the practical challenge of limited budgets for sensor deployment, where choosing the right locations becomes critical for forecasting everything from congestion patterns to pollution levels.

The researchers developed a system called Variable-Parameter Iterative Pruning (VIP) that automatically identifies which sensors to deploy when only a fraction of possible monitoring locations can be equipped. Traditional forecasting methods assume either all locations have sensors or that the sensor placement is predetermined. This new approach tackles the previously unstudied problem of optimal sensor selection, achieving up to 30% improvement in forecasting accuracy compared to existing methods while using only 10% of the sensors.

The method works through an iterative process that progressively prunes less informative sensors and model parameters. It starts with a full forecasting model trained on data from all possible locations, then systematically removes the least valuable sensors while maintaining prediction accuracy. The system uses three key components: masked pruning that identifies redundant sensors and model parameters, dynamic extrapolation that infers missing data from selected sensors, and prioritized replay that prevents the model from forgetting important patterns during the optimization process.

Experimental results across five real-world datasets demonstrate the method's effectiveness. On traffic forecasting datasets including PEMS-BAY and METR-LA, the approach achieved Mean Absolute Error (MAE) improvements of 19.87 and 31.33 respectively when using only 10% of sensors. For air quality monitoring (AQI dataset), it achieved an MAE of 16.23, significantly outperforming all baseline methods. The system also showed substantial efficiency gains, reducing model size from over 20 GB to just over 1 GB when scaling to 100,000 variables while maintaining inference times under 200 milliseconds.

This technology has immediate practical applications for city planning and environmental monitoring. Municipalities can deploy expensive permanent sensors at strategically chosen locations while using cheaper temporary sensors for initial data collection, potentially saving thousands of dollars per intersection. The method automatically identifies locations that provide the most informative data, such as major traffic junctions or areas with high variability in pollution levels, ensuring maximum forecasting accuracy with minimal infrastructure investment.

The researchers note that while their method significantly advances sensor selection for forecasting, it assumes complete data is available during the training phase. Future work could explore scenarios where even training data is incomplete, and the approach currently focuses on spatial selection rather than optimizing the timing of sensor deployments. The method's performance also depends on having representative training data that captures the full range of conditions the system will encounter during deployment.