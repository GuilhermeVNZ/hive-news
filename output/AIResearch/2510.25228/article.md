What happens to an artist's creative legacy when they're gone? A new collaboration between artificial intelligence and sound artist Evala offers a radical solution: an AI system that can continue generating new artworks in the artist's style long after their death. This breakthrough, demonstrated in the 'Studies for' installation at Tokyo's NTT InterCommunication Center, represents a fundamental shift in how we think about preserving and extending artistic creation.

The researchers developed a system that uses a lightweight sound generation model called SpecMaskGIT to create an evolving eight-channel sound environment. Unlike traditional archives that simply preserve existing works, this AI-powered approach generates new sounds that maintain the artist's distinctive style while introducing novel elements. During the three-month exhibition, the system continuously produced sound across eight speakers positioned around the audience, creating an immersive experience that changed throughout the exhibition period.

The technical approach centered on training the AI model exclusively on Evala's past sound artworks—approximately 200 hours of material. This artist-specific dataset ensured the model could internalize and reproduce the unique characteristics of Evala's style. The team optimized the system for real-time performance, reducing the model size from 172 million to 89.27 million parameters while maintaining high-quality output. They achieved this by decreasing transformer blocks from 24 to 12 and replacing the standard HiFi-GAN vocoder with the faster Vocos system, enabling continuous generation without perceptible delays.

The results showed that the AI could generate coherent sounds that preserved Evala's artistic identity while introducing unexpected elements. Initially, the model produced outputs that resembled collages of previous works, but after modifying the conditioning structure to incorporate both audio and text prompts, the system began generating sounds that felt genuinely new while remaining faithful to the artist's style. The installation attracted 20,000 visitors who experienced the continuously evolving sound environment in a bright white space enclosed by fabric, creating an intimate yet abstract atmosphere.

This approach addresses a fundamental challenge in sound art preservation. As noted in the paper, sound artworks—particularly those involving spatialization and performance—are inherently ephemeral and difficult to capture through traditional archival formats. The 'new archive' concept offers a speculative solution where the artwork continues to generate new elements rather than simply preserving existing ones. This has practical implications for artists concerned about their legacy and for institutions seeking new ways to maintain living artistic traditions.

The system does have limitations. The model's ability to generate truly novel content depends on the quality and diversity of the training dataset, and the approach requires significant technical optimization to achieve real-time performance. Additionally, the paper notes that while the system can produce surprising outputs, the degree of novelty remains constrained by the original artistic material. The researchers acknowledge that further work is needed to understand how this approach might scale to different artistic styles and technical requirements.

What makes this research particularly significant is its demonstration of how lightweight AI models can support iterative creative workflows while preserving artistic identity. By enabling quick trial-and-error cycles and maintaining the artist's distinctive characteristics, the system provides a practical framework for human-AI co-creation that could extend beyond sound art to other creative domains. The successful three-month exhibition without interruptions shows that such systems can operate reliably in real-world settings, opening new possibilities for how we think about artistic legacy and technological preservation.