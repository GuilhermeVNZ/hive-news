[VOICEOVER 0:00-0:05] What if AI could show doctors exactly how it spots pneumonia on your X-ray?
[VISUAL DIRECTION] [Animated text: "AI EXPLAINS ITS DIAGNOSES" with pulsing question mark]

[VOICEOVER 0:05-0:20] Medical AI is transforming diagnosis, but its inner workings have been a black box - until now.
[VISUAL DIRECTION] [Show X-ray images fading to black screen with "BLACK BOX" text]

[VOICEOVER 0:20-0:40] Researchers asked: Can we make AI's decision-making transparent while maintaining accuracy?
[VISUAL DIRECTION] [Animated transition showing question marks turning into magnifying glass over AI brain]

[VOICEOVER 0:40-1:00] They developed MedSAE, a method that identifies individual neurons in AI models that respond specifically to single medical concepts.
[VISUAL DIRECTION] [Show neural network diagram with specific neurons lighting up for "pneumonia" and "atelectasis"]

[VOICEOVER 1:00-1:30] Using sparse autoencoders, MedSAE disentangles complex AI representations, allowing us to see exactly what features the AI uses for diagnosis.
[VISUAL DIRECTION] [Animated breakdown showing AI layers separating into distinct concept detectors]

[VOICEOVER 1:30-1:50] The results: 82% accuracy in generating human-readable descriptions like 'severe air trapping' and 'pulmonary congestion' - making AI's reasoning clinically coherent.
[VISUAL DIRECTION] [Show text overlays with key stats: "82% accuracy" and clinical terms highlighted]

[VOICEOVER 1:50-2:00] This isn't just better AI - it's trustworthy AI that doctors and patients can actually understand.
[VISUAL DIRECTION] [Final shot of doctor and AI working together with "TRUSTWORTHY MEDICAL AI" text overlay]