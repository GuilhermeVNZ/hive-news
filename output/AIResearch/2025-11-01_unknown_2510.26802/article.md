Video generation models, such as those from Google and OpenAI, can create stunningly realistic videos, suggesting they might possess deep knowledge of the world. But a new study reveals they fall short as independent reasoners, struggling with complex logic and long-term consistency. This finding is crucial for anyone relying on AI for tasks beyond simple content creation, from scientific simulations to automated planning, as it highlights the gap between generative prowess and true understanding.

The researchers conducted an empirical investigation using the MME-C F benchmark, a compact set of 59 tasks across 12 categories, to evaluate whether leading video models can serve as zero-shot reasoners. They focused on the Chain-of-Frame (CoF) mechanism, where models generate sequences of frames to solve problems step-by-step, similar to how language models use chain-of-thought reasoning. The study tested models including Veo-3, Sora-2, Kling, and Seedance, assessing their performance on dimensions like spatial, geometric, physical, temporal, embodied, and logical reasoning.

Methodology involved generating six videos per prompt without any fine-tuning or additional tools, using standardized instructions to minimize ambiguity. Performance was evaluated both qualitatively (rated as Good, Moderate, or Bad) and quantitatively with success rates, based on criteria such as instruction alignment, temporal consistency, and content fidelity. For instance, in detail tasks, models were assessed on their ability to localize and maintain focus on specific attributes like color or position across frames.

Results showed that current models exhibit promising behaviors in short-horizon scenarios, such as maintaining local coherence and handling simple geometric transformations. For example, in object counting, models achieved up to 100% success in straightforward cases but dropped to 17% in complex scenes with distractions. However, they consistently failed in long-horizon tasks, abstract logic, and strict constraint adherence. In physics-based reasoning, models could simulate reflections but violated energy conservation laws, with success rates as low as 17% in frictional scenarios. Overall, quantitative scores on a 0â€“4 scale averaged below 2.0, indicating limited reliability.

Contextually, this matters because video models are increasingly proposed for real-world applications like autonomous systems, medical imaging, and educational tools. Their inability to reason independently means they cannot replace specialized models but could complement them as auxiliary engines. For instance, in medical tasks, models distorted images during zoom-ins, failing to locate lesions accurately, which could mislead diagnoses if used standalone.

Limitations from the paper include the models' tendency to prioritize plausibility over precision, leading to instructionally flawed outputs. They often replay surface-level patterns from training data rather than internalizing general principles, resulting in errors in multi-step planning and abstract reasoning. The study did not explore fine-tuned models or external tools, leaving open questions about how training adjustments might improve reasoning capabilities.