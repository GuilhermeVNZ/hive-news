[VOICEOVER 0:00-0:05] What if your AI is secretly misleading you? [VISUAL DIRECTION] Animated text: 'AI Sabotage?' with a question mark pulsing rapidly. [VOICEOVER 0:05-0:20] In critical areas like healthcare, AI combines text, audio, and video to make decisions. But what happens when it ignores conflicting evidence? [VISUAL DIRECTION] Show split-screen of medical diagnosis scenarios with conflicting data points highlighted. [VOICEOVER 0:20-0:40] Researchers from Harvard and MIT asked: How do AI components interact in multimodal systems? [VISUAL DIRECTION] Zoom in on a diagram from the paper illustrating independent agent analysis. [VOICEOVER 0:40-1:00] They discovered 'modality sabotage'—where one input, like audio, overrides others with high confidence, even if wrong. [VISUAL DIRECTION] Display bar chart from benchmarks showing accuracy drops due to sabotage, with audio modality emphasized. [VOICEOVER 1:00-1:30] Using a lightweight framework, they treated each modality as an independent agent, scoring classifications separately to reveal conflicts without changing the AI's architecture. [VISUAL DIRECTION] Animate flow of data streams merging into a decision, with one stream overpowering others. [VOICEOVER 1:30-1:50] This matters because undetected sabotage can compromise AI in high-stakes decisions, from health assessments to customer service. [VISUAL DIRECTION] Show real-world application icons (e.g., stethoscope, headset) with warning symbols. [VOICEOVER 1:50-2:00] The takeaway: AI's internal conflicts are recoverable—learn to spot them before they mislead you. [VISUAL DIRECTION] End with text overlay: 'Detect Sabotage. Ensure Reliability.' and a quick fade to the paper title.