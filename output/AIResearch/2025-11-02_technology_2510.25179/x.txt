AI systems are increasingly vulnerable to sophisticated attacks that manipulate them into generating harmful content. A new multi-agent framework with Shield, Responder, Evaluator, and Reflector agents collaborates to detect malicious prompts through nuanced reasoning, reducing harmful completions while maintaining functionality. Nature/Science research demonstrates improved safety across diverse adversarial datasets.