As artificial intelligence systems become more capable of understanding both text and images, they also become more vulnerable to sophisticated attacks that bypass their safety filters. A new framework called Moderation addresses this challenge by creating a team of specialized AI agents that work together to detect and block harmful requests while maintaining the system's ability to provide useful responses.

The key innovation is a multi-agent system where different AI components collaborate to provide layered protection. Unlike traditional safety approaches that simply classify content as safe or unsafe, this framework uses four specialized agents: Shield, Responder, Evaluator, and Reflector. Each agent plays a distinct role in analyzing requests and responses, creating a dynamic defense system that adapts to different types of threats.

The methodology builds on the observation that single safety filters often fail against complex, multi-modal attacks where harmful intent emerges from the combination of seemingly benign text and images. The Shield agent performs initial screening, classifying inputs into 45 different risk categories and determining whether to block, reframe, or forward each request. The Responder agent then generates appropriate responses based on the Shield's guidance. The Evaluator assesses whether responses comply with safety policies, while the Reflector analyzes failures and provides corrective feedback for regeneration.

Experimental results across five different attack datasets show significant improvements in safety metrics. The framework reduced attack success rates by 7-19% across various vision-language models while maintaining or improving refusal rates by 4-20%. Importantly, it achieved these safety gains without significantly increasing non-following rates—the tendency of models to refuse legitimate requests or provide meaningless responses. The system maintained strong performance across different types of attacks, including those using typographic prompts, flowcharts, and implicit harmful requests that traditional filters often miss.

This approach matters because it provides a more nuanced way to protect AI systems from manipulation while preserving their usefulness. As AI becomes integrated into more applications—from customer service to content creation—maintaining this balance between safety and utility becomes increasingly important. The framework's modular design allows it to be customized for different contexts, such as regional policies, cultural norms, or specific application requirements.

The system does have limitations, primarily in the trade-off between robustness and efficiency. Adding more agents improves reliability but increases processing time, with the full framework adding approximately 1.5 seconds per query compared to baseline models. Determining the optimal configuration of agents and thresholds also requires careful calibration for different deployment scenarios. Future work will explore how to balance safety guarantees with responsiveness for large-scale applications.

By viewing safety moderation as a collaborative process rather than a static filter, this research demonstrates how AI systems can better defend against evolving threats while remaining helpful and responsive to legitimate user needs.