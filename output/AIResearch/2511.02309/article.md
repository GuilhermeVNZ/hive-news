Artificial intelligence systems often struggle with complex reasoning, but a new approach could make them smarter without increasing computational costs. Researchers have discovered that running AI models in a step-by-step, iterative sequence significantly outperforms the common method of running multiple independent processes in parallel when both use the same computing resources. This finding, based on tests with five state-of-the-art open-source models, challenges a long-standing assumption in AI development and could lead to more efficient and accurate AI applications in fields like education, research, and problem-solving.

The key finding is that sequential reasoning, where the AI refines its answers over multiple steps, achieves higher accuracy than parallel reasoning across 95.6% of tested scenarios. For example, on the AIME-2025 math benchmark, sequential methods improved accuracy by up to 46.7 percentage points compared to parallel approaches. This advantage holds across different model sizes, from 20 billion to 235 billion parameters, indicating it is a fundamental property of how AI models process information rather than a quirk of specific architectures.

Methodologically, the researchers compared two paradigms under strict matched compute conditions. In sequential reasoning, the AI starts with an initial attempt and iteratively refines it over steps like self-correction and verification, building on previous outputs. In parallel reasoning, the AI generates multiple independent attempts simultaneously and aggregates them, typically through voting. To ensure fairness, the team controlled token usage precisely—for instance, using 24,576 tokens for both 3-chain parallel and 6-step sequential setups. They evaluated this on benchmarks such as AIME-2024/2025 for math problems and GPQA-Diamond for graduate-level science questions, which require multi-step logical inference.

Results analysis reveals that sequential reasoning excels due to mechanisms like error correction, where the AI fixes mistakes in later steps, and insight accumulation, where it builds on prior knowledge. For instance, in creative tasks like joke generation, sequential methods showed greater lexical diversity, meaning they produced richer and more varied content. The researchers also introduced inverse-entropy weighted voting, a training-free method that assigns weights based on the AI's confidence (using token-level log probabilities), which improved accuracy in 97% of cases for sequential setups. Figure 2 in the paper illustrates how iterative refinement allows the AI to reference and build on earlier attempts, leading to more reliable outcomes.

In real-world context, this shift could make AI systems more effective in applications requiring deep reasoning, such as tutoring systems, scientific research, or customer support, without needing additional training data or fine-tuning. It emphasizes that how AI allocates compute during inference—focusing on depth over breadth—can enhance performance, potentially reducing costs and improving accessibility for resource-limited settings.

Limitations from the paper note that the study focused on reasoning tasks with transformer-based models and token budgets up to 16K, so its generalizability to other domains like commonsense reasoning or multimodal problems requires further validation. Additionally, sequential methods incur higher latency due to serial execution, making them less suitable for real-time applications where speed is critical. Future work should explore hybrid approaches and extend these findings to areas like vision-language tasks to broaden their impact.