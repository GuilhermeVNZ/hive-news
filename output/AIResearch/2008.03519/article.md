Imagine learning to draw an entirely new alphabet after seeing just a few examples, then applying that knowledge to sketch unfamiliar symbols. This everyday human ability—rapid, flexible learning—has long eluded artificial intelligence systems. Now, research reveals how people spontaneously develop abstract, program-like routines when learning to draw, offering a blueprint for building AI that generalizes more like humans.

In a series of experiments, researchers found that people who briefly practiced copying geometric shapes developed reusable drawing strategies that transferred to new tasks. These strategies weren't just motor habits; they reflected abstract rules about shape structure, similar to how a computer program might generate multiple drawings from the same code. For example, subjects trained on vertically grouped shapes (like lines with circles stacked above them) later showed a strong bias toward vertical transitions when drawing, while those trained on horizontal shapes preferred horizontal movements. This suggests that learning wasn't just about memorizing strokes, but about inferring underlying patterns that could be reapplied.

To understand this process, the team built a computational model called DreamCoder, which learns by combining two key principles: abstraction and compositionality. Abstraction means focusing on higher-order structure—like the arrangement of lines and circles—while ignoring specific details. Compositionality allows the model to build complex routines by combining simpler ones, much like assembling Lego blocks. The model starts with basic primitives (like drawing a line or circle) and, through a wake-sleep cycle of exploring possible programs and compressing successful ones into a library, discovers reusable subroutines. When trained on the same drawing tasks as humans, it learned routines such as 'skewer' (drawing a vertical line with shapes attached) or 'barbell' (horizontal lines with circles at ends), mirroring strategies observed in people.

The results showed that both humans and the model generalized their learning. In tests with rotated stimuli, humans adapted their drawing strategies to match the new orientation, maintaining the abstract structure rather than just repeating motor patterns. For instance, subjects who initially learned vertical transitions switched to horizontal ones when shapes were rotated 90 degrees. The model similarly produced drawings that captured the essence of new shapes, with trained versions making far fewer errors (e.g., 1.9 mistakes on average for one group) compared to an untrained baseline. Analysis of human drawings revealed that over 70% of strokes could be categorized into program-like strategies, and the model's output closely matched human performance, with similarity scores indicating it captured key aspects of behavior.

This work matters because it bridges a gap in AI: how to learn quickly from limited data, like humans do. Today's AI often requires massive datasets to generalize, but this model, inspired by human cognition, shows that structured, abstract representations can lead to flexible problem-solving. For everyday applications, this could lead to AI assistants that learn new tasks from a few examples—such as interpreting sketches in design software or adapting to user preferences in educational tools. It also offers insights into cognitive disorders; by modeling how drawing routines break down, researchers might develop better diagnostics for conditions like dementia or stroke.

However, the study has limitations. The drawing tasks were simplified to focus on learning principles, so the model doesn't capture the full complexity of human drawing, which involves factors like artistic skill or cultural experience. Future work will need to address this diversity and test the approach in noisier, real-world scenarios. Despite this, the research underscores that AI doesn't always need big data to learn—sometimes, it just needs the right structure.