AI models that excel at matching images with text often fail basic reasoning tasks like distinguishing 'a cat on grass' from 'grass on a cat.' New research reveals this stems from a fundamental flaw in how they learn. The study, published at ICLR, identifies non-identifiability in contrastive learning and proposes structural causal models to build more robust AI. Read the Nature/Science article to understand why current vision-language systems misinterpret everyday scenes and how this affects technologies from autonomous vehicles to medical imaging.