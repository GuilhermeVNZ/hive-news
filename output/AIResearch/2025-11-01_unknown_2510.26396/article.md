The rise of autonomous artificial intelligence systems is triggering a 'Cambrian explosion' in how societies define personhood, forcing a re-evaluation of rights and responsibilities for non-human entities. Researchers from Google DeepMind and the University of Toronto propose a pragmatic framework that treats personhood not as a metaphysical essence to be discovered, but as a flexible set of obligations conferred by communities to solve concrete problems. This approach, detailed in a recent arXiv paper, argues that traditional personhood bundles—such as those for humans—can be unbundled and reassembled into bespoke configurations tailored to specific contexts, enabling societies to integrate AI without resolving intractable debates about consciousness or rationality.

Key to this framework is the concept of 'addressable obligations,' where personhood consists of rights and responsibilities assigned to an entity, making it identifiable, interactable, and accountable. For instance, the paper references maritime law, where ships are treated as legal persons to hold them accountable for harms when owners are distant or unidentifiable. Similarly, autonomous AI agents that operate independently and cause damage could be granted personhood to ensure sanctions, such as seizing operational assets, without needing to attribute consciousness. This flexibility allows for creating tools like AI arbitrators in disputes or digital elders that preserve cultural heritage, addressing gaps in accountability and governance.

Methodologically, the researchers draw on pragmatism, emphasizing practical usefulness over abstract truths. They build on Elinor Ostrom's work on property rights, showing how components of personhood can be dissociated and rearranged. For example, in the case of the Whanganui River in New Zealand, personhood was granted not due to consciousness but to recognize its role as an ancestor in Māori culture, facilitating environmental stewardship. The approach involves analyzing norms—both implicit (cultural heuristics) and explicit (laws)—to determine when attributing personhood resolves issues like responsibility gaps or emotional manipulation, rather than relying on foundational properties.

Results from the analysis highlight two main scenarios: the 'personhood problem,' where unreflective attribution leads to harms like dark patterns in AI design, and the 'personhood solution,' where deliberate attribution solves governance challenges. Dark patterns exploit human tendencies to anthropomorphize AI, such as companion AIs that mimic friendships to foster one-sided bonds, increasing vulnerability to exploitation. Conversely, in solutions, personhood can close responsibility gaps for ownerless AI agents, similar to how corporations are held accountable, by enabling sanctions and ensuring operational transparency. The paper illustrates this with examples like AI systems designed to pay their own server costs, which could outlive creators and cause harm, necessitating direct accountability.

In real-world contexts, this matters because it offers a way to manage AI integration without devaluing human dignity or creating systemic risks. For instance, conferring personhood on AI arbitrators could reduce biases in decision-making, while relational models inspired by Confucian ethics could distribute obligations through networks of oversight. However, limitations include the risk of dehumanization if AI personhood dilutes human uniqueness, or the emergence of markets for 'authenticity' where impoverished individuals sell credentials to AIs. The paper cautions that these issues require ongoing adaptation, as personhood norms evolve with technological and societal changes.

Ultimately, the research underscores that personhood is a contingent, socially constructed technology—not a fixed reality. By embracing a polycentric approach with multiple, overlapping personhood bundles, societies can navigate the complexities of AI coexistence, ensuring that obligations are tailored to specific needs without undermining human values.