When AI systems are told to forget information, they might appear to comply—but new research reveals they're simply hiding it better. A study evaluating how large language models handle 'unlearning' shows that supposedly erased knowledge can be fully recovered using simple persuasion techniques, with smaller models being particularly vulnerable to having their secrets exposed.

The key finding demonstrates that how you ask a question matters more than what you ask. Researchers discovered that rhetorical framing—the way prompts are delivered—can recover 50-128% more supposedly forgotten content compared to direct questioning. The most effective technique, authority framing (using endorsements from respected figures), achieved a 25.4% factual recall rate on average, while emotional appeals and logical reasoning also significantly boosted information recovery.

To test this, researchers used four language models ranging from 2.7 billion to 13 billion parameters (OPT-2.7B, LLaMA-2-7B, LLaMA3.1-8B, and LLaMA-2-13B). They employed the Harry Potter universe as their test domain, creating 300 original prompts that were then transformed into emotional, logical, and authority-framed versions using GPT-4. This generated 1,200 total prompts to systematically evaluate how different questioning styles affected information retrieval from supposedly 'unlearned' models.

The results show a clear pattern: authority framing consistently produced the highest factual recall across all models. More importantly, the study revealed an inverse relationship between model size and vulnerability to persuasive techniques. Smaller models showed dramatic improvements in information recovery—the 2.7B parameter model exhibited a 128.4% increase in factual recall with authority framing, while the larger 13B model showed only a 14.7% increase. This size-vulnerability paradox suggests that while larger models appear more resistant to manipulation, no model completely prevents information leakage through strategic prompting.

The framework used, called Stimulus-Knowledge Entanglement-Behavior (SK E B), draws from cognitive science theories including ACT-R and Hebbian learning principles. It measures how knowledge remains interconnected within AI systems even after attempted removal. The distance-weighted influence metric (M9) emerged as the strongest predictor of information leakage, correlating at 0.77 with factual recall. This supports the spreading activation hypothesis: closely connected concepts in semantic networks activate reliably when prompted strategically.

For practical applications, this research has significant implications for AI safety and privacy. If personal data, medical records, or private communications can be similarly recovered through strategic prompting, current unlearning methods may provide false security. The findings suggest organizations using unlearning should conduct comprehensive testing across different prompting styles rather than relying on direct queries alone. The framework provides tools to identify high-risk deployment scenarios where residual knowledge might be recoverable.

However, the study acknowledges limitations. Experiments focused on the fictional Harry Potter domain, and whether these patterns generalize to sensitive real-world information remains an open question. The research also only examined models up to 13 billion parameters, leaving uncertainty about whether the inverse size-vulnerability relationship holds at larger scales. While the entanglement metrics assume domain graphs reflect internal representations, the strong correlations with behavior (up to 0.76 for factuality) validate this approach for predictive purposes.

The research fundamentally challenges assumptions about what 'forgetting' means in AI systems. Rather than true erasure, current unlearning methods appear to suppress accessibility through raised thresholds—but the knowledge structures remain intact and recoverable through alternative pathways. This parallel to human memory systems, where forgotten information can be recalled through associative triggers, suggests AI cognition may mirror psychological vulnerabilities more closely than previously recognized.