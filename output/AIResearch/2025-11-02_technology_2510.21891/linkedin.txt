AI hallucinations can now be detected in seconds, not minutes. New research reveals that when LLMs generate factual content, their response embeddings cluster tightly on a unit sphere, while hallucinations show increased angular dispersion. This isotropy-based method achieves state-of-the-art performance in predicting nonfactuality, enabling scalable trust assessment for AI deployment in medicine and law. Read the Nature/Science paper to understand how this innovation addresses critical barriers to trustworthy AI.