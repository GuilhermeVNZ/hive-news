[VOICEOVER 0:00-0:05] What if AI could read financial reports better than human analysts? [VISUAL DIRECTION] Animated text: "87.79% ACCURACY" with glowing effect
[VOICEOVER 0:05-0:20] Financial reports contain critical information about company health, but they're notoriously complex and time-consuming to analyze manually. [VISUAL DIRECTION] Show scrolling SEC Form 10-K document with highlighted jargon
[VOICEOVER 0:20-0:40] Researchers asked: Can large language models accurately detect whether sentences express positive, negative, or neutral stances toward key financial metrics? [VISUAL DIRECTION] Show question mark transforming into AI brain icon
[VOICEOVER 0:40-1:00] They discovered GPT-4.1-Mini achieved the highest accuracy at 87.79%, outperforming Llama, Gemma, and Mistral models. [VISUAL DIRECTION] Bar chart comparing model performance with GPT-4.1-Mini highlighted
[VOICEOVER 1:00-1:30] Using chain-of-thought prompting—where models generate intermediate reasoning steps—improved performance by 4.23% across all models. Context from management discussions further boosted accuracy. [VISUAL DIRECTION] Animated flow diagram showing chain-of-thought process
[VOICEOVER 1:30-1:50] This breakthrough allows financial institutions to automate complex analysis without expensive training data, transforming how corporate health is assessed. [VISUAL DIRECTION] Show institutional building with automated analysis flowing through it
[VOICEOVER 1:50-2:00] AI is now reading between the lines of financial reports with unprecedented accuracy. [VISUAL DIRECTION] Final screen with paper title: "Evaluating Large Language Models for Stance Detection in Financial Reports"