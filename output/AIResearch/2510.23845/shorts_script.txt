[VOICEOVER 0:00-0:05] What if AI could spot a suicide crisis with near-perfect accuracy? [VISUAL DIRECTION] Animated text: '87.9% Accuracy' flashes on a dark background with a pulsing red alert symbol. [VOICEOVER 0:05-0:20] Researchers at Emory University tackled a critical gap: AI safety in vulnerable situations like self-harm and domestic violence. [VISUAL DIRECTION] Show a split screen: left side with a generic chatbot icon, right side with a medical cross. Zoom in on the cross. [VOICEOVER 0:20-0:40] They asked: Can AI ensembles—combining models like GPT-5 and Claude-4—outperform individual systems in crisis detection? [VISUAL DIRECTION] Display a simple flowchart: 'AI Models → Voting → Higher Accuracy'. Use fast cuts between model logos. [VOICEOVER 0:40-1:00] The finding? Yes. By voting together, they achieved an Exact score of 0.8794, beating GPT-5's solo 0.8183. [VISUAL DIRECTION] Show Figure 2 from the paper, zooming on the peak distribution bar for the ensemble. Overlay text: 'Ensemble: 0.8794 vs GPT-5: 0.8183'. [VOICEOVER 1:00-1:30] How? They built BENCH using 4,287 clinician-annotated cases from subreddits, fine-tuning AI to distinguish ongoing from historical crises. [VISUAL DIRECTION] Animate text snippets from mock social posts (e.g., 'I need help now') with green checkmarks for correct AI IDs. Pan across a dataset visualization. [VOICEOVER 1:30-1:50] Implications: This boosts AI in health chatbots and hotlines, but limits remain—like over-predicting or missing urgent cases. [VISUAL DIRECTION] Show a caution icon next to a list: 'Limitations: Over-prediction, Ambiguity'. Hold for 3 seconds. [VOICEOVER 1:50-2:00] The takeaway? AI is getting smarter at saving lives, but human oversight is still essential. [VISUAL DIRECTION] Fade to a hopeful image of a hand reaching out, with text: 'AI + Humans = Safer Support'.