Artificial intelligence is increasingly used in medicine to interpret complex data, but its real-world success often hinges on whether clinicians can understand and trust its recommendations. A new study shows that when AI systems provide clear explanations for their decisions, they significantly enhance diagnostic accuracy and user acceptance in sleep medicine. This finding addresses a critical barrier to AI adoption in high-stakes healthcare settings.

The researchers conducted an application-grounded user study with eight professional sleep medicine practitioners. They evaluated how AI assistance affects the scoring of nocturnal arousal events from polysomnographic data—a key task for diagnosing sleep disorders like obstructive sleep apnea, which affects about 20% of the population. Participants performed scoring under three conditions: unaided, with black-box AI assistance (opaque recommendations), and with white-box AI assistance (transparent recommendations that include explanations). Assistance was provided either at the start of scoring or as a post-hoc quality control step.

The study revealed that transparent AI assistance improved event-level diagnostic performance by approximately 18% compared to black-box assistance. This benefit was most pronounced during the quality control phase, where white-box AI outperformed black-box by 30% on average. When AI was applied as a targeted quality control step, it yielded median performance improvements of around 30% over black-box assistance. While white-box approaches initially required more time for scoring, participants overwhelmingly preferred them, with seven out of eight expressing willingness to adopt such a system with minor or no modifications.

Methodologically, the researchers employed a state-of-the-art deep learning model based on the DeepSleep architecture, adapted for detecting arousal onsets. They used the DeepLIFT method to generate post-hoc feature attributions, providing local and global explanations. The study design involved a within-participant factorial approach, with rigorous statistical analyses including repeated-measures ANOVA and permutation testing to ensure robust findings.

Results analysis showed that human-AI teams significantly outperformed unaided experts when evaluated against the ground truth used to train the AI. Collaboration also reduced variability among scorers. Count-based measures, crucial for clinical indices like the arousal index, indicated that AI assistance helped correct human underestimation biases, bringing team counts closer to ground truth.

The implications for real-world healthcare are substantial. Transparent AI can function as a reliable co-scorer in sleep laboratories, enhancing diagnostic consistency without replacing human expertise. However, benefits come with increased time demands—white-box assistance doubled scoring duration initially, though some users achieved comparable speeds with familiarity. Strategically timed transparent assistance effectively balances accuracy and efficiency, offering a promising pathway toward trustworthy AI integration in medical workflows.

Limitations from the paper include the small sample size of eight participants, which may affect generalizability, and the use of a single explanation method. The model's optimization for recall over precision also influenced user behavior, suggesting a need for further exploration of alternative AI training strategies. Future research should investigate long-term effects and diverse explanation modalities to optimize human-AI collaboration in clinical practice.