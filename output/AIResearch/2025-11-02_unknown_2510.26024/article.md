Large language models like ChatGPT and Gemini are increasingly used globally, but their ability to handle culturally specific information remains problematic. A new study reveals that efforts to make these AI models work uniformly across languages inadvertently erase cultural nuances, forcing models to provide generic, often Western-centric answers even when local knowledge is required.

Researchers discovered that cross-lingual alignment—techniques designed to make AI models perform consistently across different languages—systematically degrades their ability to provide culturally appropriate responses. The study introduces a framework that quantifies both knowledge transfer (how well models share information across languages) and cultural localization (how well they adapt to local contexts). When evaluating six languages including Korean, Greek, and Arabic, all alignment methods improved knowledge sharing but consistently reduced cultural accuracy by up to 3.4%.

The research team analyzed four popular alignment approaches: instruction tuning, middle-layer representation alignment, cross-lingual optimization, and English-steering. They tested these methods using two benchmarks—Global MMLU for universal knowledge and BLEND for culturally adaptive knowledge. The experiments revealed that more aggressive alignment techniques, while better at transferring knowledge between languages, caused the most significant cultural erasure.

Analysis of the models' internal representations showed that cultural knowledge isn't completely lost but becomes harder to access. The researchers found that universal knowledge and cultural knowledge are encoded in different layers of the neural network. Universal knowledge transfer works best in middle layers (around layer 20), while cultural localization operates optimally in deeper layers (around layer 28). This separation allowed the team to develop "Surgical Steering," a method that applies different interventions at specific layers to balance both objectives.

This finding has immediate practical implications for AI deployment in diverse regions. For emergency services, tourism, or local business applications, AI systems need to provide accurate local information—whether it's emergency numbers, cultural norms, or regional regulations. The study demonstrates that current alignment methods often override this local knowledge in favor of uniform responses.

The research acknowledges that some cultural nuances may be permanently lost during alignment processes. Even with targeted interventions, models that underwent strong alignment remained less responsive to cultural steering than unaligned baselines. This suggests fundamental trade-offs in multilingual AI development that cannot be fully eliminated.

The study provides tools for developers to create more culturally aware AI systems while maintaining cross-lingual capabilities. However, it also highlights that complete cultural preservation may not be achievable with current approaches, pointing to inherent limitations in how we build multilingual AI.