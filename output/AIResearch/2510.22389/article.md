Assessing the quality of academic research is a critical task for universities and funding bodies, often relying on time-consuming expert reviews. Now, a new study reveals that even small artificial intelligence models can evaluate journal articles effectively, offering a faster, cheaper alternative for institutions with limited resources. This finding could democratize access to AI tools for research assessment, especially where data privacy or computational power is a concern.

The researchers tested various large language models (LLMs), including open-weight versions like Gemma3, Qwen3, and reasoning models such as DeepSeek R1, on a dataset of 2,780 medical, health, and life science papers. They compared the AI-generated scores to human expert evaluations from the UK's Research Excellence Framework and author-assigned ratings. Key results show that smaller models with as few as 4 billion parameters often perform comparably to larger cloud-based models like ChatGPT 4o-mini and Gemini 2.0 Flash in correlating with human judgements. For instance, the Gemma3 4b model achieved similar correlation levels to its larger counterparts in most fields, though the 1b version was less effective, indicating a potential minimum size threshold for reliable performance.

Methodology involved prompting the AI models with article titles and abstracts, using both zero-shot (no examples) and few-shot (with four examples) approaches. The models were asked to score papers on a scale from 1 to 4 stars, based on originality, significance, and rigour. Averaging scores from five identical queries consistently improved accuracy across models, suggesting a simple strategy to enhance reliability without complex adjustments.

Analysis of the data, referenced in figures such as Figure 1 and Figure 4 from the paper, indicates that smaller models like the 12b variants worked well across all units of assessment, including clinical medicine and biological sciences. However, performance varied by field, with some areas like public health showing lower correlations. The study also found that reasoning models, which process information through step-by-step thinking, did not clearly outperform non-reasoning models, despite being slower and more resource-intensive.

In practical terms, this means organizations can use smaller, locally run AI models to support research evaluations, reducing costs and enhancing data security. For example, a university with limited GPU memory could deploy a 4b model to preliminarily rank grant applications or publication outputs. This approach avoids reliance on external cloud services, addressing confidentiality concerns while maintaining reasonable accuracy.

Limitations of the study include its focus on health and life sciences, which may not generalize to all disciplines, and the use of proxy human scores that could dampen correlation strengths. Additionally, the wide confidence intervals in results mean that differences between models might not be statistically significant in all cases. The paper notes that AI scores tend to avoid extreme low values, indicating potential biases that require further investigation.

Overall, this research highlights that AI's role in research assessment isn't reserved for the largest models, opening doors for broader adoption in academic and institutional settings where efficiency and privacy are paramount.