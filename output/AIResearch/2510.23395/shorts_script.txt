[VOICEOVER 0:00-0:05] What if the most advanced AI models are actually worse than simple keyword searches at understanding environmental communication? [VISUAL DIRECTION] [Animated text: AI FAILS at environmental language] [VOICEOVER 0:05-0:20] Researchers from Vrije Universiteit Amsterdam discovered something surprising about how AI interprets our discussions about climate change and ecological concepts. [VISUAL DIRECTION] [Show research institution logo, transition to data visualization] [VOICEOVER 0:20-0:40] They analyzed over 880,000 sentences from organizations' websites, comparing traditional keyword methods against sophisticated AI models like GPT-4o and Llama 3.3 70B. [VISUAL DIRECTION] [Animated counter showing 880,000+ sentences analyzed] [VOICEOVER 0:40-1:00] The results were striking: simple keyword searches automatically flagged 1,229 instances of environmental language, while AI models missed 88.5% of these references. [VISUAL DIRECTION] [Show comparison chart with 88.5% failure rate highlighted] [VOICEOVER 1:00-1:30] The AI operated in zero-shot settings without specific training, relying on statistical patterns that failed to capture nuanced meanings. For example, it frequently dismissed "Mother Earth" as metaphorical rather than recognizing its environmental significance. [VISUAL DIRECTION] [Show example text "Mother Earth" with AI misinterpretation overlay] [VOICEOVER 1:30-1:50] This matters because organizations increasingly use automated systems to analyze communications about climate urgency. AI's inability to properly interpret these discussions could lead to critical misunderstandings in policy and research. [VISUAL DIRECTION] [Show organizational communications with misinterpretation warnings] [VOICEOVER 1:50-2:00] The takeaway: Current AI cannot be trusted as an objective classifier for environmental language, revealing fundamental gaps in machine understanding of human culture.