UC Berkeley researchers have developed the first complete mathematical theory explaining how word2vec learns language representations. The breakthrough shows the algorithm learns in discrete steps, building orthogonal concepts sequentially. This fundamental understanding could inform development of more interpretable AI systems and provides insights into feature learning across modern language models.