The past decade has witnessed a fundamental shift in how machine learning progresses. Research involving carefully designed, mathematically principled architectures now yields only marginal improvements, while compute-intensive engineering efforts scaling to massive training sets and parameter counts produce remarkable capabilities that existing theory cannot predict. Mathematics and statistics, once the primary guides of machine learning research, now struggle to provide immediate insight into the latest breakthroughs.

This empirical acceleration has prompted speculation about mathematics' diminished role in AI research. The field is becoming increasingly interdisciplinary, drawing from biology's experience with complex systems and social sciences as AI integrates deeper into society. However, mathematics remains as relevant as ever—its role is simply evolving from providing theoretical guarantees to offering post-hoc explanations of empirical phenomena observed during training.

Pure mathematical domains like topology, algebra, and geometry are now joining traditionally applied fields like probability theory and linear algebra in machine learning research. These pure fields have developed over the last century to handle high levels of abstraction and complexity, promising to address modern deep learning's biggest challenges. Their capabilities help mathematicians make discoveries about spaces and processes that initially seem beyond human intuition.

One key area where mathematics provides insight is in understanding neural network symmetries. The concept of equivariance—where applying a transformation before a function yields the same result as applying it after—has long been important in computer vision. Convolutional neural networks exemplify this, being approximately equivariant to image translation, which explains why they don't need to recognize objects at every possible location.

Researchers are now systematically using group theory to build more expressive architectures that respect various symmetries. Well-studied examples include rotation and reflection equivariance in vision, translation equivariance for molecular structures, and permutation equivariance for graph neural networks. Encoding these exotic symmetries has proven useful across domains from physics to materials science.

The shift toward scale has broadened the scope of mathematics applicable to machine learning. While this situation might disappoint mathematicians who hoped for closer ties to advanced physics, empirical success actually opens new paths for mathematics to support research progress. Performant models may serve as gateways to analyzing previously inaccessible complex systems.

As the world becomes increasingly mathematized through machine learning models, this represents an interesting time to be a mathematician. The challenge lies in adapting mathematical toolkits to a landscape where empirical breakthroughs often precede theoretical understanding. By embracing this shift, mathematics can continue playing a crucial, albeit evolving, role in shaping AI's future.