A provocative new analysis is challenging the fundamental terminology used in one of AI safety's most critical debates. The argument centers on whether 'continuous takeoff' accurately describes the competing visions of AI progress held by leading researchers Paul Christiano and Eliezer Yudkowsky.

The debate over AI progress models has profound implications for how we approach AI safety and alignment. Christiano's model, often labeled 'continuous takeoff,' suggests AI capabilities will improve through steady, predictable steps. Yudkowsky's competing vision, typically called 'discontinuous takeoff,' allows for sudden, unpredictable breakthroughs that could dramatically accelerate AI capabilities.

According to the analysis, these mathematical terms obscure the actual disagreement. Both researchers agree progress could be mathematically continuous or discontinuous - the real divide concerns whether future AI development will follow historical patterns or break from established trends entirely.

The author proposes replacing the terminology with 'historical progress' versus 'ahistorical progress.' This reframing better captures whether AI capabilities will evolve in ways we can anticipate based on past development or whether we're entering uncharted territory where historical patterns no longer apply.

This terminology shift matters because it affects how policymakers and researchers approach AI safety timelines. If progress follows historical patterns, we might have more warning before reaching dangerous capability thresholds. If progress becomes ahistorical, we could face sudden, unpredictable capability jumps.

The analysis notes that Christiano originally introduced the 'continuous takeoff' terminology in his 2018 'Takeoff Speeds' post, focusing specifically on pre-AGI improvements. However, the mathematical continuity of progress curves doesn't necessarily determine whether that progress will be predictable or follow established patterns.

Both models can be represented as Poisson processes with different parameters - Christiano's featuring frequent, small capability improvements and Yudkowsky's involving rare, significant capability jumps. Yet mathematical continuity alone fails to capture the core disagreement about predictability and historical precedent.

The terminology debate highlights how precise language becomes crucial when discussing existential risks from advanced AI systems. As AI capabilities accelerate, getting the framing right could mean the difference between adequate preparation and catastrophic miscalculation.