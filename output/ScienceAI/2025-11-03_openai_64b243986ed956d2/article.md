In a landmark deal that reshapes the artificial intelligence landscape, Amazon Web Services and OpenAI have announced a multi-year strategic partnership valued at $38 billion. The agreement, effective immediately, provides OpenAI with unprecedented access to AWS's cloud infrastructure to run and scale its core AI workloads, including training and inference for models like ChatGPT.

The partnership represents one of the largest cloud computing agreements in AI history, spanning seven years with provisions for continued expansion. OpenAI will gain access to hundreds of thousands of state-of-the-art NVIDIA GPUs, including GB200 and GB300 models, with capacity to scale to tens of millions of CPUs for agentic workloads. This infrastructure deployment is scheduled for completion by the end of 2026, with further expansion planned through 2027 and beyond.

AWS brings proven expertise in managing massive-scale AI infrastructure, having operated clusters exceeding 500,000 chips. The architectural design features Amazon EC2 UltraServers with clustered NVIDIA GPUs interconnected on the same network, enabling low-latency performance critical for AI processing efficiency. This optimized infrastructure will support everything from ChatGPT inference serving to training next-generation frontier models.

OpenAI CEO Sam Altman emphasized the strategic importance of the partnership, stating: 'Scaling frontier AI requires massive, reliable compute. Our partnership with AWS strengthens the broad compute ecosystem that will power this next era and bring advanced AI to everyone.' The deal comes as AI technology advancement creates unprecedented demand for computing power, with frontier model providers increasingly relying on cloud infrastructure for performance, scale, and security.

AWS CEO Matt Garman highlighted the significance for both companies: 'As OpenAI continues to push the boundaries of what's possible, AWS's best-in-class infrastructure will serve as a backbone for their AI ambitions. The breadth and immediate availability of optimized compute demonstrates why AWS is uniquely positioned to support OpenAI's vast AI workloads.'

The partnership builds on existing collaboration between the companies. Earlier this year, OpenAI's open-weight foundation models became available on Amazon Bedrock, making them accessible to millions of AWS customers. Thousands of organizations including Bystreet, Comscore, Peloton, Thomson Reuters, Triomics, and Verana Health are already using OpenAI models on AWS for agentic workflows, coding, scientific analysis, and mathematical problem-solving.

This strategic alliance positions AWS as a critical infrastructure provider in the rapidly evolving AI ecosystem, while giving OpenAI the scalable compute resources needed to advance its research and deployment capabilities. The partnership signals continued consolidation in the AI infrastructure market as leading model developers partner with cloud providers to meet growing computational demands.