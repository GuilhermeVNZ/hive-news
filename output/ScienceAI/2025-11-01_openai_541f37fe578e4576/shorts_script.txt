Take 1: Hey, OpenAI just dropped two new AI models for keeping content safe. (Visual: Text overlay 'New AI Safety Models')
Take 2: They're called gpt-oss-safeguard-120b and -20b. (Visual: Model names on screen)
Take 3: These models use reasoning to label content based on your policies. (Visual: Diagram showing policy input)
Take 4: They're open-weight, so developers can customize them. (Visual: Code snippet with Apache license)
Take 5: Safety tests show they handle risks well in chat settings. (Visual: Graph with safety metrics)
Take 6: Not for chattingâ€”best for content moderation tasks. (Visual: Icon of shield with checkmark)
Take 7: Supports multiple languages and effort levels. (Visual: Globe icon with low/medium/high labels)
Take 8: This could change how we automate online safety. (Visual: Futuristic network graphic)
Take 9: Want to try them? Check OpenAI's report. (Visual: Link to website)
Take 10: Stay tuned for more AI updates! (Visual: Wave goodbye, subscribe prompt)