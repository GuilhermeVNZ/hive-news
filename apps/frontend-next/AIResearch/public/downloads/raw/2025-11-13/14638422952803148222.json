{
  "id": "14638422952803148222",
  "title": "Why 95% of AI Pilots Fail — and What the Other 5% Do Differently",
  "url": "https://www.salesforce.com/news/stories/why-ai-pilots-fail/",
  "published_date": "2025-11-12T17:00:00Z",
  "author": null,
  "summary": null,
  "original_title": "Why 95% of AI Pilots Fail — and What the Other 5% Do Differently",
  "source_type": "rss",
  "content_html": "Over the past year, Ive had conversations with hundreds of CIOs, from regional banks to Fortune 100 enterprises. They all tell me the same thing: They are tired of demos. Yet another demo, another pilot, without getting business value. They see the potential. Theyve invested in AI. But they cant get to broad deployment and ROI. Instead they get stuck in pilot purgatory. The issue is widespread: According to an MIT study, 95 of enterprise generative AI pilots fail to deliver demonstrable ROI. So what are the other 5 doing differently? What does it take to get from pilot to deployment at scale? I run engineering, customer success, professional services, and our sales teams in India and ASEAN. I hear what customers ask for during the sales process; my teams help them successfully implement and expand agents across their organizations; and I make sure customer insights go straight back into improving our products. The role gives me a unique vantage point on the full agent deployment cycle, and from that position, Ive learned a few things about why AI pilots stall and how they can really take off. If your sales team lives in Salesforce, your agents need to work there. If your engineering team lives in Slack, thats where agents belong. Fundamentally, becoming an Agentic Enterprise isnt about layering on AI tools, its about reimagining your work processes from the ground up. The shift to agentic AI isnt just technological its also cultural, operational, and organizational. You cant simply build an agent, ship it, and expect it to work at scale. These are the operational gaps where most enterprises get stuck: Many people treat AI as a simple automation tool an add-on to existing processes. They build an AI tool and ship it, yet they cant assess if its delivering value because they didnt define specific business outcomes at the start. Agents need performance metrics, testing frameworks, continuous monitoring, lifecycle management. You have to track their output, train them as requirements change, and define escalation paths when they encounter edge cases. If you cant manage it and measure it, you cant scale it. Agents must be part of the systems people are using to get work done. When employees have to stop what theyre doing, switch tools, and re-enter information, adoption drops fast. A deeper problem with isolating agents from the flow of work is context: Large Language Models (LLMs) alone are not enough. Agents need detailed context from enterprise systems to return useful answers and take actions. When agents dont have that context, people fall into what we call the prompt doom loop rewriting prompts to supply context that the agent should already know. Its a frustrating employee experience, resulting in low adoption and stalled pilots. AI thats disconnected from enterprise systems never earns sustained use. When we started Agentforce on help.salesforce.com, we tracked every message. That was doable at a few thousand conversations. Once it hits millions of conversations, human review becomes impossible. But an auditor still asks: How do you know your agents are compliant? Show me the audit trail. Most pilots work in controlled environments, but at scale, legal shuts them down because theres no framework for permissions, audit trails, or compliance. You need role-based permissions, approval workflows, and audit capabilities just like youd have for employees. If you cant prove governance, you cant get to production. You need infrastructure to manage agents at scale. Most enterprises build pilots that work in isolation, then realize they cant scale without rebuilding everything. They have no way to test agent behavior before wider deployment, no monitoring systems to catch problems in production, and no frameworks for updating agents as business logic changes. The platform debt compounds until the pilot cant move forward. A year ago, we deployed Agentforce across every part of our business, working through all of these potential blockers on our way to becoming our own Customer Zero. The results speak for themselves: In support, Agentforce now handles the bulk of daily customer conversations (over 2 million to date). You can see this working in real-time on help.salesforce.com we even publish exactly how many more conversations the help agent now handles compared to its human teammates. This unlocked capacity allows us to shift employees from offering reactive support to providing proactive service helping customers get their own Agentforce agents up and running in just weeks, even days. In engineering, AI agents handle routine tasks, code maintenance, proactive threat monitoring, and rapid incident response. This has led to 30 cycle time improvement, with agents detecting 91 of incidents within eight minutes and auto-remediating 87 of them in under 20 minutes. Now engineers have the time to focus on more strategic development: new features, even greater reliability, and improved quality. In sales, our website and events were generating 250,000 leads per week, but we could only follow up on the top 25 of highly qualified leads. It was just too expensive to do more. Now our SDR agent handles personalized outreach and qualification, generating 60 million of annualized pipeline during initial rollout. Were reaching customers we couldnt before. From those deployments, and from working with more than 12,000 Agentforce customers over the past year, here are a few key lessons: A truly integrated agent understands your business process: when a customer contacts support, it knows their purchase history, open cases, contract terms, and escalation paths. Agents need context from your CRM, service platform, data warehouse, and collaboration tools and they need to understand the relationships between these systems and the business logic that governs data flow, with specific role-based permissions that control access. That context cant be assembled at prompt time it has to be native in the place where work is happening. You need to define what each agent can do: what data it can access, what actions it can take, what requires human approval. You need audit trails showing what every agent did, when, and why. You need testing frameworks that validate agent behavior before deployment and monitoring systems to track performance after deployment. Think of it as agent lifecycle management: clear goals and metrics, performance management, continuous training, and escalation paths when agents encounter edge cases. Use One Model or Many, but Commit to Singular Governance For most customers, the underlying AI model is just infrastructure. All they really want to know is that their SDR agent is closing more leads; their service agent is resolving issues faster. Yet some typically with advanced technical teams or specialized requirements want choice: OpenAI for certain use cases, Anthropic for others, Googles models for specific tasks, or specialized models optimized for cost and performance. But every agent, regardless of which model powers it, has to operate within your enterprises governance framework. Think of it like API discovery versus API security: knowing whats available doesnt mean every agent should have unrestricted access. When APIs came onto the scene, we learned that you need a central way to govern and monitor every API across the enterprise. Agents are no different you need a command center to register, audit, and manage them, regardless of their source. Salesforce now supports open standards like MCP (Model Context Protocol) so agents can discover tools and capabilities across systems, but MCP alone isnt enough. You still need the trust layer that enforces permissions, audit requirements, and compliance across everything. Think of it like API discovery versus API security: knowing whats available doesnt mean every agent should have unrestricted access. The companies that win this next phase wont be the ones experimenting with AI theyll be the ones running on it. When CIOs ask me where to begin, I tell them: Pick one low-risk use case. Build one agent. Ship it. Then expand by connecting to more data, adding skills, and plugging into collaborative platforms Starting with lower-risk internal use cases employee tools, knowledge management, channel summaries is a good way to build organizational confidence and get quick wins. Once you prove it works, move to customer-facing deployments. Invite their feedback. Iterate. Thats how you escape pilot purgatory. Thats how you turn AI from potential into performance as an Agentic Enterprise.",
  "content_text": "Over the past year, Ive had conversations with hundreds of CIOs, from regional banks to Fortune 100 enterprises. They all tell me the same thing: They are tired of demos. Yet another demo, another pilot, without getting business value. They see the potential. Theyve invested in AI. But they cant get to broad deployment and ROI. Instead they get stuck in pilot purgatory. The issue is widespread: According to an MIT study, 95 of enterprise generative AI pilots fail to deliver demonstrable ROI. So what are the other 5 doing differently? What does it take to get from pilot to deployment at scale? I run engineering, customer success, professional services, and our sales teams in India and ASEAN. I hear what customers ask for during the sales process; my teams help them successfully implement and expand agents across their organizations; and I make sure customer insights go straight back into improving our products. The role gives me a unique vantage point on the full agent deployment cycle, and from that position, Ive learned a few things about why AI pilots stall and how they can really take off. If your sales team lives in Salesforce, your agents need to work there. If your engineering team lives in Slack, thats where agents belong. Fundamentally, becoming an Agentic Enterprise isnt about layering on AI tools, its about reimagining your work processes from the ground up. The shift to agentic AI isnt just technological its also cultural, operational, and organizational. You cant simply build an agent, ship it, and expect it to work at scale. These are the operational gaps where most enterprises get stuck: Many people treat AI as a simple automation tool an add-on to existing processes. They build an AI tool and ship it, yet they cant assess if its delivering value because they didnt define specific business outcomes at the start. Agents need performance metrics, testing frameworks, continuous monitoring, lifecycle management. You have to track their output, train them as requirements change, and define escalation paths when they encounter edge cases. If you cant manage it and measure it, you cant scale it. Agents must be part of the systems people are using to get work done. When employees have to stop what theyre doing, switch tools, and re-enter information, adoption drops fast. A deeper problem with isolating agents from the flow of work is context: Large Language Models (LLMs) alone are not enough. Agents need detailed context from enterprise systems to return useful answers and take actions. When agents dont have that context, people fall into what we call the prompt doom loop rewriting prompts to supply context that the agent should already know. Its a frustrating employee experience, resulting in low adoption and stalled pilots. AI thats disconnected from enterprise systems never earns sustained use. When we started Agentforce on help.salesforce.com, we tracked every message. That was doable at a few thousand conversations. Once it hits millions of conversations, human review becomes impossible. But an auditor still asks: How do you know your agents are compliant? Show me the audit trail. Most pilots work in controlled environments, but at scale, legal shuts them down because theres no framework for permissions, audit trails, or compliance. You need role-based permissions, approval workflows, and audit capabilities just like youd have for employees. If you cant prove governance, you cant get to production. You need infrastructure to manage agents at scale. Most enterprises build pilots that work in isolation, then realize they cant scale without rebuilding everything. They have no way to test agent behavior before wider deployment, no monitoring systems to catch problems in production, and no frameworks for updating agents as business logic changes. The platform debt compounds until the pilot cant move forward. A year ago, we deployed Agentforce across every part of our business, working through all of these potential blockers on our way to becoming our own Customer Zero. The results speak for themselves: In support, Agentforce now handles the bulk of daily customer conversations (over 2 million to date). You can see this working in real-time on help.salesforce.com we even publish exactly how many more conversations the help agent now handles compared to its human teammates. This unlocked capacity allows us to shift employees from offering reactive support to providing proactive service helping customers get their own Agentforce agents up and running in just weeks, even days. In engineering, AI agents handle routine tasks, code maintenance, proactive threat monitoring, and rapid incident response. This has led to 30 cycle time improvement, with agents detecting 91 of incidents within eight minutes and auto-remediating 87 of them in under 20 minutes. Now engineers have the time to focus on more strategic development: new features, even greater reliability, and improved quality. In sales, our website and events were generating 250,000 leads per week, but we could only follow up on the top 25 of highly qualified leads. It was just too expensive to do more. Now our SDR agent handles personalized outreach and qualification, generating 60 million of annualized pipeline during initial rollout. Were reaching customers we couldnt before. From those deployments, and from working with more than 12,000 Agentforce customers over the past year, here are a few key lessons: A truly integrated agent understands your business process: when a customer contacts support, it knows their purchase history, open cases, contract terms, and escalation paths. Agents need context from your CRM, service platform, data warehouse, and collaboration tools and they need to understand the relationships between these systems and the business logic that governs data flow, with specific role-based permissions that control access. That context cant be assembled at prompt time it has to be native in the place where work is happening. You need to define what each agent can do: what data it can access, what actions it can take, what requires human approval. You need audit trails showing what every agent did, when, and why. You need testing frameworks that validate agent behavior before deployment and monitoring systems to track performance after deployment. Think of it as agent lifecycle management: clear goals and metrics, performance management, continuous training, and escalation paths when agents encounter edge cases. Use One Model or Many, but Commit to Singular Governance For most customers, the underlying AI model is just infrastructure. All they really want to know is that their SDR agent is closing more leads; their service agent is resolving issues faster. Yet some typically with advanced technical teams or specialized requirements want choice: OpenAI for certain use cases, Anthropic for others, Googles models for specific tasks, or specialized models optimized for cost and performance. But every agent, regardless of which model powers it, has to operate within your enterprises governance framework. Think of it like API discovery versus API security: knowing whats available doesnt mean every agent should have unrestricted access. When APIs came onto the scene, we learned that you need a central way to govern and monitor every API across the enterprise. Agents are no different you need a command center to register, audit, and manage them, regardless of their source. Salesforce now supports open standards like MCP (Model Context Protocol) so agents can discover tools and capabilities across systems, but MCP alone isnt enough. You still need the trust layer that enforces permissions, audit requirements, and compliance across everything. Think of it like API discovery versus API security: knowing whats available doesnt mean every agent should have unrestricted access. The companies that win this next phase wont be the ones experimenting with AI theyll be the ones running on it. When CIOs ask me where to begin, I tell them: Pick one low-risk use case. Build one agent. Ship it. Then expand by connecting to more data, adding skills, and plugging into collaborative platforms Starting with lower-risk internal use cases employee tools, knowledge management, channel summaries is a good way to build organizational confidence and get quick wins. Once you prove it works, move to customer-facing deployments. Invite their feedback. Iterate. Thats how you escape pilot purgatory. Thats how you turn AI from potential into performance as an Agentic Enterprise."
}