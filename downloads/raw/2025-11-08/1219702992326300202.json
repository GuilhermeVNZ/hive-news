{
  "id": "1219702992326300202",
  "title": "Notion’s rebuild for agentic AI: How GPT‑5 helped unlock autonomous workflows",
  "url": "https://openai.com/index/notion",
  "published_date": "2025-11-07T10:00:00Z",
  "author": null,
  "summary": null,
  "original_title": "Notion’s rebuild for agentic AI: How GPT‑5 helped unlock autonomous workflows",
  "source_type": "rss",
  "content_html": "By rebuilding their agent system with GPT5, Notion created an AI workspace that can reason, act, and adapt across workflows. In late 2022, within weeks of getting access to GPT4, Notion had already shipped a writing assistant, rolled out workspace-wide QA features, and integrated OpenAI models deeply across its search, content, and planning tools. But as models advancedand users began asking agents to complete entire workflowsNotions team saw limits in their system architecture. The old pattern of prompting models to do isolated tasks was limiting the ceiling of what was capable on their platform. Agents needed to make decisions, orchestrate tools, and reason through ambiguity, and that shift required more than prompt engineering. Instead of patching their existing stack, Notion rebuilt it. They replaced task-specific prompt chains with a central reasoning model that coordinates modular sub-agents. These agents can search across Notion, Slack, or the web; add to or edit databases; and synthesize responses using whatever tools the task requires. With their launch of Notion 3.0, AI isnt just embedded in workflows; it can now run them. Users assign a broad taskfor example, compiling stakeholder feedbackand their agent plans, executes, and reports back. The shift toward agents that choose how to work meant designing for model autonomy from the start. To validate the architectural shift, Notion evaluated GPT5 against other state-of-the-art models using actual user tasks. Evaluations were grounded in feedback Notion had already marked as high priority, including questions that surfaced in Research Mode, long-form tasks that required multi-step reasoning, and ambiguous or outdated content where model judgment mattered. The team used a combination of LLM-as-judge scoring, structured test fixtures, and human-labeled feedback. These evaluations helped Notion identify where GPT5 added valuefor example, in reasoning, ambiguity, researchand where environment-specific tuning would improve results. We didnt cherry-pick tasks. These were high-signal workflows from our product, says Sachs. Thats where model differences actually show up. Some tasks need fast responses; others dont. By experimenting with the different reasoning levels of GPT5, Notion was able to customize the intelligence of their agents and find the perfect balance between response quality and latency depending on the requirements of the task. Notion designed its agents to run for seconds or minutes depending on the job. Short latency is prioritized for direct lookups. Long-running agentsup to 20 minutesare used for background workflows like summarizing content or updating databases. What matters most to the team is how much time the user gets back, and not how fast the model responds. That philosophy drives how orchestration and expectations are set across the UI. Every Notion team uses Notion AI. That daily use generates structured feedback and direct annotation from humans when something goes wrong. If a user thumbs down a result, it enters a pipeline for trace-level debugging. But internal use alone wasnt enough. The team also worked with design partnerstechnical customers with early access to agent featuresto uncover edge cases and spot blind spots. This outside-in testing helped shape product readiness, tune orchestration behaviors, and validate where GPT5 really moved the needle. OpenAI also uses Notion to coordinate projects and knowledge, with Notion AI embedded in daily workflows to speed up reviews and close the loop on feedback. This mutual usage creates a unique dynamic; both teams build with each others products, providing constant feedback and visibility into how the work performs in practice. Notions rebuild wasnt just about launching Notion 3.0. It was about designing a system that could support new model capabilities and adapt as those models get smarter. Their approach offers a clear roadmap for other teams deploying agentic AI in production: Were already seeing returns from the rebuild, says Sachs. If the next model unlocks something new, well do what it takes to support it.",
  "content_text": "By rebuilding their agent system with GPT5, Notion created an AI workspace that can reason, act, and adapt across workflows. In late 2022, within weeks of getting access to GPT4, Notion had already shipped a writing assistant, rolled out workspace-wide QA features, and integrated OpenAI models deeply across its search, content, and planning tools. But as models advancedand users began asking agents to complete entire workflowsNotions team saw limits in their system architecture. The old pattern of prompting models to do isolated tasks was limiting the ceiling of what was capable on their platform. Agents needed to make decisions, orchestrate tools, and reason through ambiguity, and that shift required more than prompt engineering. Instead of patching their existing stack, Notion rebuilt it. They replaced task-specific prompt chains with a central reasoning model that coordinates modular sub-agents. These agents can search across Notion, Slack, or the web; add to or edit databases; and synthesize responses using whatever tools the task requires. With their launch of Notion 3.0, AI isnt just embedded in workflows; it can now run them. Users assign a broad taskfor example, compiling stakeholder feedbackand their agent plans, executes, and reports back. The shift toward agents that choose how to work meant designing for model autonomy from the start. To validate the architectural shift, Notion evaluated GPT5 against other state-of-the-art models using actual user tasks. Evaluations were grounded in feedback Notion had already marked as high priority, including questions that surfaced in Research Mode, long-form tasks that required multi-step reasoning, and ambiguous or outdated content where model judgment mattered. The team used a combination of LLM-as-judge scoring, structured test fixtures, and human-labeled feedback. These evaluations helped Notion identify where GPT5 added valuefor example, in reasoning, ambiguity, researchand where environment-specific tuning would improve results. We didnt cherry-pick tasks. These were high-signal workflows from our product, says Sachs. Thats where model differences actually show up. Some tasks need fast responses; others dont. By experimenting with the different reasoning levels of GPT5, Notion was able to customize the intelligence of their agents and find the perfect balance between response quality and latency depending on the requirements of the task. Notion designed its agents to run for seconds or minutes depending on the job. Short latency is prioritized for direct lookups. Long-running agentsup to 20 minutesare used for background workflows like summarizing content or updating databases. What matters most to the team is how much time the user gets back, and not how fast the model responds. That philosophy drives how orchestration and expectations are set across the UI. Every Notion team uses Notion AI. That daily use generates structured feedback and direct annotation from humans when something goes wrong. If a user thumbs down a result, it enters a pipeline for trace-level debugging. But internal use alone wasnt enough. The team also worked with design partnerstechnical customers with early access to agent featuresto uncover edge cases and spot blind spots. This outside-in testing helped shape product readiness, tune orchestration behaviors, and validate where GPT5 really moved the needle. OpenAI also uses Notion to coordinate projects and knowledge, with Notion AI embedded in daily workflows to speed up reviews and close the loop on feedback. This mutual usage creates a unique dynamic; both teams build with each others products, providing constant feedback and visibility into how the work performs in practice. Notions rebuild wasnt just about launching Notion 3.0. It was about designing a system that could support new model capabilities and adapt as those models get smarter. Their approach offers a clear roadmap for other teams deploying agentic AI in production: Were already seeing returns from the rebuild, says Sachs. If the next model unlocks something new, well do what it takes to support it."
}