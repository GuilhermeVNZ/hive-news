{
  "id": "14597174366444335119",
  "title": "Build more knowledgeable AI applications with new LLMs and greater control in Pinecone Assistant",
  "url": "https://www.pinecone.io/blog/assistant-new-llms/",
  "published_date": null,
  "author": null,
  "summary": "Get the latest updates via email when they're published: Pinecone Assistant is a fully-managed service built to power grounded chat and agent-based applications. The API takes away your need to worry about the many systems and steps required to build an AI assistant for knowledge-intensive tasks with private data. That includes chunking, embedding, file storage, query planning, vector search, model orchestration, reranking, and more. Pinecone Assistant uses LLMs to provide context and final answ",
  "original_title": "Build more knowledgeable AI applications with new LLMs and greater control in Pinecone Assistant",
  "image_url": "https://www.pinecone.io/api/og/?title=Build%20more%20knowledgeable%20AI%20applications%20with%20new%20LLMs%20and%20greater%20control%20in%20Pinecone%20Assistant&category=Blog",
  "source_type": "html",
  "content_html": "Get the latest updates via email when they're published: Pinecone Assistant is a fully-managed service built to power grounded chat and agent-based applications. The API takes away your need to worry about the many systems and steps required to build an AI assistant for knowledge-intensive tasks with private data. That includes chunking, embedding, file storage, query planning, vector search, model orchestration, reranking, and more. Pinecone Assistant uses LLMs to provide context and final answer generation. Its quality stems first and foremost from the ability to generate highly-relevant context through better search. After a thorough review of models that meet this criteria, as well as strong user demand, today we are excited to add support for new OpenAI (gpt-4.1, o4-mini) and Anthropic (claude-3-7-sonnet) LLMs, and to welcome Gemini (gemini-2.5-pro) to Assistant. Three main factors guide our model support selection: security, availability, and stability. Before making new models available through Pinecone Assistant, our team works to ensure that the providers can offer 1. a private cloud deployment with sufficient token-per-minute rate support 2. stability/consistency with regard to response quality and citation formatting. Changing the LLM you want to use with Pinecone Assistant is done by simply passing the model name with your choice. Check out the Pinecone Assistant reference and guide in our docs for more information and implementation details. In addition to the new selection of models, were now giving you more control by exposing the temperature parameter for your chosen LLM. Temperature affects the sampling process from the probability distribution of new tokens. In short, low temperatures (0.0) yield more consistent, predictable answers and high temperatures (1.0) increase a models explanatory power. Higher temperatures are generally better for creative tasks or when you want to generate multiple completions and select one in post-processing. You can specify temperature directly in the Pinecone Assistant chat API: If the temperature parameter is not passed, Pinecone Assistant falls back to the default value set by the given model provider. As providers increase the pace of innovation, weve updated our infrastructure to be able to quickly adapt and add more models going forward. In the world of knowledgeable AI applications, context is king. Were excited about what the future holds and to see what Pinecone Assistant users build with increased capabilities. Reach out and let us know! Create your first index for free, then pay as you go when you're ready to scale.",
  "content_text": "Get the latest updates via email when they're published: Pinecone Assistant is a fully-managed service built to power grounded chat and agent-based applications. The API takes away your need to worry about the many systems and steps required to build an AI assistant for knowledge-intensive tasks with private data. That includes chunking, embedding, file storage, query planning, vector search, model orchestration, reranking, and more. Pinecone Assistant uses LLMs to provide context and final answer generation. Its quality stems first and foremost from the ability to generate highly-relevant context through better search. After a thorough review of models that meet this criteria, as well as strong user demand, today we are excited to add support for new OpenAI (gpt-4.1, o4-mini) and Anthropic (claude-3-7-sonnet) LLMs, and to welcome Gemini (gemini-2.5-pro) to Assistant. Three main factors guide our model support selection: security, availability, and stability. Before making new models available through Pinecone Assistant, our team works to ensure that the providers can offer 1. a private cloud deployment with sufficient token-per-minute rate support 2. stability/consistency with regard to response quality and citation formatting. Changing the LLM you want to use with Pinecone Assistant is done by simply passing the model name with your choice. Check out the Pinecone Assistant reference and guide in our docs for more information and implementation details. In addition to the new selection of models, were now giving you more control by exposing the temperature parameter for your chosen LLM. Temperature affects the sampling process from the probability distribution of new tokens. In short, low temperatures (0.0) yield more consistent, predictable answers and high temperatures (1.0) increase a models explanatory power. Higher temperatures are generally better for creative tasks or when you want to generate multiple completions and select one in post-processing. You can specify temperature directly in the Pinecone Assistant chat API: If the temperature parameter is not passed, Pinecone Assistant falls back to the default value set by the given model provider. As providers increase the pace of innovation, weve updated our infrastructure to be able to quickly adapt and add more models going forward. In the world of knowledgeable AI applications, context is king. Were excited about what the future holds and to see what Pinecone Assistant users build with increased capabilities. Reach out and let us know! Create your first index for free, then pay as you go when you're ready to scale."
}