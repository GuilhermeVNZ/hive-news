<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0" xmlns:media="http://search.yahoo.com/mrss/"><channel><title><![CDATA[The Gradient]]></title><description><![CDATA[A digital publication about artificial intelligence and the future.]]></description><link>https://thegradient.pub/</link><image><url>https://thegradient.pub/favicon.png</url><title>The Gradient</title><link>https://thegradient.pub/</link></image><generator>Ghost 5.33</generator><lastBuildDate>Fri, 14 Nov 2025 02:01:08 GMT</lastBuildDate><atom:link href="https://thegradient.pub/rss/" rel="self" type="application/rss+xml"/><ttl>60</ttl><item><title><![CDATA[AGI Is Not Multimodal]]></title><description><![CDATA[<blockquote>&quot;In projecting language back as the model for thought, we lose sight of the tacit embodied understanding that undergirds our intelligence.&quot; &#x2013;Terry Winograd</blockquote><p>The recent successes of generative AI models have convinced some that AGI is imminent. While these models appear to capture the essence of human</p>]]></description><link>https://thegradient.pub/agi-is-not-multimodal/</link><guid isPermaLink="false">683fb98b77c3d76051ac142c</guid><category><![CDATA[Perspectives]]></category><category><![CDATA[Trends]]></category><dc:creator><![CDATA[Benjamin A. Spiegel]]></dc:creator><pubDate>Wed, 04 Jun 2025 14:00:29 GMT</pubDate><media:content url="https://thegradient.pub/content/images/2025/06/Gradient_Article_Art3-downscaled.png" medium="image"/><content:encoded><![CDATA[<blockquote>&quot;In projecting language back as the model for thought, we lose sight of the tacit embodied understanding that undergirds our intelligence.&quot; &#x2013;Terry Winograd</blockquote><img src="https://thegradient.pub/content/images/2025/06/Gradient_Article_Art3-downscaled.png" alt="AGI Is Not Multimodal"><p>The recent successes of generative AI models have convinced some that AGI is imminent. While these models appear to capture the essence of human intelligence, they defy even our most basic intuitions about it. They have emerged not because they are thoughtful solutions to the problem of intelligence, but because they <em>scaled</em> effectively on <a href="https://dl.acm.org/doi/abs/10.1145/3467017">hardware we already had</a>. Seduced by the fruits of scale, some have come to believe that it provides a clear pathway to AGI. The most emblematic case of this is the multimodal approach, in which massive modular networks are optimized for an array of modalities that, taken together, <em>appear</em> general. However, I argue that this strategy is sure to fail in the near term; it will not lead to human-level AGI that can, e.g., perform sensorimotor reasoning, motion planning, and social coordination. Instead of trying to glue modalities together into a patchwork AGI, <strong>we should pursue approaches to intelligence that treat embodiment and interaction with the environment as primary</strong>, and see modality-centered processing as emergent phenomena.</p><p>Preface: Disembodied definitions of Artificial General Intelligence &#x2014; emphasis on <em>general</em> &#x2014; exclude crucial problem spaces that we should expect AGI to be able to solve. <strong>A true AGI must be general across all domains.</strong> Any <em>complete</em> definition must at least include the ability to solve problems that originate in physical reality, e.g. repairing a car, untying a knot, preparing food, etc. As I will discuss in the next section, <strong>what is needed for these problems is a form of intelligence that is fundamentally situated in something like a physical world model</strong>. For more discussion on this, look out for <em>Designing an Intelligence</em>. Edited by George Konidaris, MIT Press, forthcoming.<br></p><h2 id="why-we-need-the-world-and-how-llms-pretend-to-understand-it">Why We Need the World, and How LLMs Pretend to Understand It</h2><p>TLDR: I first argue that <strong>true AGI needs a physical understanding of the world</strong>, as many problems <a href="https://hci.stanford.edu/winograd/papers/thinking-machines.html">cannot be converted into a problem of symbol manipulation</a>. It has been suggested by some that LLMs are learning a model of the world through next token prediction, but it is more likely that LLMs are learning bags of heuristics to predict tokens. This leaves them with a superficial understanding of reality and contributes to false impressions of their intelligence.</p><p><strong>The most shocking result of the predict-next-token objective is that it yields AI models that reflect a deeply human-like understanding of the world, despite having never observed it like we have.</strong> This result has <a href="https://aclanthology.org/2020.acl-main.463/">led to confusion</a> about what it means to <em>understand language</em> and even to <em>understand the world</em> &#x2014; something we have long believed to be a prerequisite for language understanding. <strong>One explanation for the capabilities of LLMs comes from </strong><a href="https://www.youtube.com/watch?v=SjhIlw3Iffs&amp;list=PLpdlTIkm0-jJ4gJyeLvH1PJCEHp3NAYf4&amp;index=63"><strong>an emerging theory</strong></a><strong> suggesting that they induce models of the world through next-token prediction</strong>. Proponents of this theory cite the prowess of SOTA LLMs on various benchmarks, the <a href="https://phillipi.github.io/prh/">convergence of large models to similar internal representations</a>, and their favorite rendition of the idea that &#x201C;language mirrors the structure of reality,&#x201D; a notion that has been espoused at least by Plato, Wittgenstein, Foucault, and Eco. While I&#x2019;m generally in support of digging up esoteric texts for research inspiration, I&#x2019;m worried that this metaphor has been taken too literally. Do LLMs really learn implicit models of the world? How could they otherwise be so proficient at language?</p><p>One source of evidence in favor of the LLM world modeling hypothesis is <a href="https://arxiv.org/abs/2210.13382">the Othello paper</a>, wherein researchers were able to predict the board of an Othello game from the hidden states of a transformer model trained on sequences of <em>legal</em> <em>moves</em>. However, there are <em>many</em> issues with generalizing these results to models of natural language. For one, whereas Othello moves can <em>provably</em> be used to deduce the full state of an Othello board,<strong> we have no reason to believe that a complete picture of the physical world can be inferred by a linguistic description. </strong>What sets the game of Othello apart from many tasks in the physical world is that <strong>Othello fundamentally resides in the land of symbols, and is merely implemented using physical tokens to make it easier for humans to play.</strong> A full game of Othello can be played with just pen and paper, but one can&#x2019;t, e.g., sweep a floor, do dishes, or drive a car with just pen and paper. To solve such tasks, you need some physical conception of the world beyond what humans can merely <em>say</em> about it. Whether that conception of the world is encoded in a formal world model or, e.g., a value function is up for debate, but it is clear that <strong>there are many problems in the physical world that </strong><em><strong>cannot</strong></em><strong> be </strong><a href="https://dl.acm.org/doi/10.1145/360018.360022"><strong>fully represented by a system of symbols</strong></a><strong> and solved with mere symbol manipulation.</strong></p><p>Another issue stated in <a href="https://aiguide.substack.com/p/llms-and-world-models-part-2">Melanie Mitchell&#x2019;s recent piece</a> and supported <a href="https://arxiv.org/abs/2406.03689">by this paper</a>, is that there is evidence that <strong>generative models can score remarkably well on sequence prediction tasks while failing to learn models of the worlds that created such sequence data</strong>, e.g. by learning comprehensive sets of idiosyncratic heuristics. E.g., it was pointed out in <a href="https://www.lesswrong.com/posts/gcpNuEZnxAPayaKBY/othellogpt-learned-a-bag-of-heuristics-1">this blog post</a> that OthelloGPT learned sequence prediction rules that don&#x2019;t actually hold for all possible Othello games, like &#x201C;if the token for B4 does not appear before A4 in the input string, then B4 is empty.&#x201D; While one can argue that it doesn&#x2019;t matter <em>how</em> a world model predicts the next state of the world, it should raise suspicion when that prediction reflects a better understanding of the training data than the underlying world that led to such data. This, unfortunately, is the central fault of the predict-next-token objective, which seeks only to retain information relevant to the prediction of the next token. <strong>If it can be done with something easier to learn than a world model, it likely will be.</strong></p><p>To claim without caveat that predicting the <em>effects of earlier symbols on later symbols</em> requires a model of the world like the ones humans generate from perception would be to abuse the <a href="https://lingo.csail.mit.edu/blog/world_models/">&#x201C;world model&#x201D; notion</a>. Unless we disagree on what the world is, it should be clear that a <em>true</em> world model can be used to predict the next state of the <em>physical</em> world given a history of states. Similar world models, which predict high fidelity observations of the physical world, are leveraged in many subfields of AI including <a href="http://www.incompleteideas.net/book/the-book-2nd.html">model-based reinforcement learning</a>, <a href="https://arxiv.org/abs/2010.01083">task and motion planning in robotics</a>, <a href="https://www.cambridge.org/core/journals/behavioral-and-brain-sciences/article/building-machines-that-learn-and-think-like-people/A9535B1D745A0377E16C590E14B94993">causal world modeling</a>, and <a href="https://dynamic3dgaussians.github.io/">areas of computer vision</a> to solve problems instantiated in physical reality. LLMs are simply not running physics simulations in their latent next-token calculus when they ask you if your person, place, or thing is bigger than a breadbox. In fact, I conjecture that <strong>the behavior of LLMs is not thanks to a learned world model, but to brute force memorization of incomprehensibly abstract rules governing the behavior of symbols, i.e. a model of </strong><em><strong>syntax</strong></em><strong>.</strong></p><p>Quick primer:</p><ul><li><strong>Syntax</strong> is a subfield of linguistics that studies how words of various grammatical categories (e.g. parts of speech) are arranged together into sentences, which can be parsed into syntax trees. <em>Syntax studies the structure of sentences and the atomic parts of speech that compose them.</em></li><li><em><em><strong>Semantics</strong> is another subfield concerned with the literal meaning of sentences, e.g., compiling &#x201C;I am feeling chilly&#x201D; into the <em>idea</em> that you are experiencing cold. <em>Semantics boils language down to literal meaning, which is information about the world or human experience.</em></em></em></li><li><strong>Pragmatics</strong> studies the interplay of physical and conversational context on speech interactions, like when someone knows to close an ajar window when you tell them &#x201C;I am feeling chilly.&#x201D; <em>Pragmatics involves interpreting speech while reasoning about the environment and the intentions and hidden knowledge of other agents.</em></li></ul><!--kg-card-begin: markdown--><p>Without getting too technical, there is intuitive evidence that somewhat separate systems of cognition are responsible for each of these linguistic faculties. Look no further than the capability for humans to generate syntactically well-formed sentences that have no semantic meaning, e.g. Chomsky&#x2019;s famous sentence &#x201C;Colorless green ideas sleep furiously,&#x201D; or sentences with well-formed semantics that make no pragmatic sense, e.g. responding merely with &#x201C;Yes, I can&#x201D; when asked, &#x201C;Can you pass the salt?&#x201D; Crucially, <strong>it is the fusion of the disparate cognitive abilities underpinning them that coalesce into human language understanding.</strong> For example, there isn&#x2019;t anything syntactically wrong with the sentence, &#x201C;The fridge is in the apple,&#x201D; as a syntactic account of &#x201C;the fridge&#x201D; and &#x201C;the apple&#x201D; would categorize them as noun phrases that can be used to produce a sentence with the production rule, S &#x2192; (NP &#x201C;is in&#x201D; NP). However, <strong>humans recognize an obvious semantic failure in the sentence that becomes apparent after attempting to reconcile its meaning with our understanding of reality</strong>: we know that fridges are larger than apples, and could not be fit into them.</p>
<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><p>But what if you have never perceived the real world, yet still were trying to figure out whether the sentence was ill-formed? <strong>One solution could be to embed semantic information at the level of syntax</strong>, e.g., by inventing new syntactic categories, NP<sub>the fridge</sub> and NP<sub>the apple </sub>, and a single new production rule that prevents semantic misuse: S &#x2192; (NP<sub>the apple</sub> &#x201C;is in&#x201D; NP<sub>the fridge </sub>). While this strategy would no longer require grounded world knowledge about fridges and apples, e.g., <strong>it would require special grammar rules for every semantically well-formed construction&#x2026; which is actually possible to learn given a massive corpus of natural language.</strong> Crucially, this would not be the same thing as grasping semantics, which in my view is fundamentally about understanding the nature of the world.</p>
<!--kg-card-end: markdown--><p>Finding that LLMs have reduced problems of semantics and pragmatics into syntax would have profound implications on how we should view their intelligence. People often treat language proficiency as a proxy for general intelligence by, e.g., strongly associating pragmatic and semantic understanding with the cognitive abilities that undergird them in humans. For example, someone who appears well-read and graceful in navigating social interactions is likely to score high in traits like sustained attention and theory of mind, which lie closer to measures of raw cognitive ability. In general, these proxies are reasonable for assessing a <em>person&#x2019;s</em> general intelligence, but not an LLM&#x2019;s, as the apparent linguistic skills of LLMs could come from entirely separate mechanisms of cognition.</p><h2 id="the-bitter-lesson-revisited">The Bitter Lesson Revisited</h2><p>TLDR: Sutton&#x2019;s Bitter Lesson has sometimes been interpreted as meaning that making <em>any</em> assumptions about the structure of AI is a mistake. This is both unproductive and a misinterpretation; it is precisely when humans think deeply about the structure of intelligence that major advancements occur. Despite this, scale maximalists have implicitly suggested that multimodal models can be a structure-agnostic framework for AGI. Ironically, today&#x2019;s multimodal models contradict Sutton&#x2019;s Bitter Lesson by making implicit assumptions about the structure of individual modalities and how they should be sewn together. <strong>In order to build AGI, we must either think deeply about how to unite existing modalities, or dispense with them altogether in favor of an interactive and embodied cognitive process.</strong></p><figure class="kg-card kg-image-card"><img src="https://thegradient.pub/content/images/2025/06/ChatGPT-Image-May-21--2025--11_31_12-AM-copy.png" class="kg-image" alt="AGI Is Not Multimodal" loading="lazy" width="1024" height="733" srcset="https://thegradient.pub/content/images/size/w600/2025/06/ChatGPT-Image-May-21--2025--11_31_12-AM-copy.png 600w, https://thegradient.pub/content/images/size/w1000/2025/06/ChatGPT-Image-May-21--2025--11_31_12-AM-copy.png 1000w, https://thegradient.pub/content/images/2025/06/ChatGPT-Image-May-21--2025--11_31_12-AM-copy.png 1024w" sizes="(min-width: 720px) 720px"></figure><p>The paradigm that led to the success of LLMs is marked primarily by <em>scale</em>, not efficiency. We have effectively trained a pile of one trillion ants for one billion years to mimic the form and function of a Formula 1 race car; eventually it gets there, but wow was the process inefficient. This analogy nicely captures a debate between structuralists, who want to build things like &quot;wheels&quot; and &quot;axles&quot; into AI systems, and scale maximalists, who want more ants, years, and F1 races to train on. Despite many decades of structuralist study in linguistics, the unstructured approaches of scale maximalism have yielded far better ant-racecars in recent years. This was most notably articulated by Rich Sutton &#x2014; a recent recipient of the Turing Award along with Andy Barto for their work in Reinforcement Learning &#x2014; in his piece &#x201C;The Bitter Lesson.&#x201D;</p><p>[W]e should build in only the meta-methods that can find and capture this arbitrary complexity&#x2026; Essential to these methods is that they can find good approximations, but the search for them should be by our methods, not by us. We want AI agents that can discover like we can, not which contain what we have discovered. - Rich Sutton</p><p>Sutton&#x2019;s argument is that methods that leverage computational resources will outpace methods that do not, and that any structure for problem-solving built as an inductive bias into AI will hinder it from learning better solutions. <strong>This is a compelling argument</strong> <strong>that I believe has been seriously misinterpreted by some as implying that making </strong><em><strong>any</strong></em><strong> assumptions about structure is a false step.</strong> It is, in fact, human intuition that was responsible for many significant advancements in the development of SOTA neural network architectures. For example, <a href="https://en.wikipedia.org/wiki/AlexNet#:~:text=AlexNet%20is%20a%20convolutional%20neural,Visual%20Recognition%20Challenge%20(ILSVRC).">Convolutional Neural Networks</a> made an assumption about translation invariance for pattern recognition in images and kickstarted the modern field of deep learning for computer vision; the <a href="https://papers.neurips.cc/paper/7181-attention-is-all-you-need.pdf">attention mechanism of Transformers</a> made an assumption about the long-distance relationships between symbols in a sentence that made ChatGPT possible and had nearly everyone drop their RNNs; and <a href="https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/">3D Gaussian Splatting</a> made an assumption about the solidity of physical objects that made it more performant than NeRFs. Potentially none of these methodological assumptions apply to the entire domain of <em>possible</em> scenes, images, or token streams, but they do for the specific ones that humans have curated and formed structural intuitions about. Let&#x2019;s not forget that humans have co-evolved with the environments that these datasets are drawn from.</p><p>The real question is how we might heed Sutton&#x2019;s Bitter Lesson in our development of AGI. The scale maximalist approach worked for LLMs and LVMs (large vision models) because we had natural deposits of text and image data, but <strong>an analogous application of scale maximalism to AGI would require forms of embodiment data that we simply don&#x2019;t have. </strong>One solution to this data scarcity issue extends the generative modeling paradigm to multimodal modeling &#x2014; encompassing language, vision, and action &#x2014; <strong>with the hope that a general intelligence can be built by summing together general models of narrow modalities.</strong></p><p>There are multiple issues with this approach. First, <strong>there are deep connections between modalities that are unnaturally severed in the multimodal setting</strong>, making the problem of concept synthesis ever more difficult. In practice, uniting modalities often involves pre-training dedicated neural modules for each modality and joining them together into a joint embedding space. In the early days, this was achieved by nudging the embeddings of, e.g. (language, vision, action) tuples to converge to similar latent vectors of meaning, a vast oversimplification of the kinds of relationships that may exist between modalities. One can imagine, e.g., captioning an image at various levels of abstraction, or implementing the same linguistic instruction with different sets of physical actions. Such one-to-many relationships suggest that a contrastive embedding objective is not suitable.</p><p>While modern approaches do not make such stringent assumptions about how modalities should be united, they still universally encode percepts from all modalities (e.g. text, images) into the same latent space. Intuitively, it would seem that such latent spaces could serve as common conceptual ground across modalities, analogous to a space of human concepts. However, these latent spaces do not cogently capture all information relevant to a concept, and instead rely on modality-specific decoders to flesh out important details. <strong>The &#x201C;meaning&#x201D; of a percept is not </strong><em><strong>in</strong></em><strong> the vector it is encoded as, but in the way relevant decoders process this vector into meaningful outputs. </strong>As long as various encoders and decoders are subject to modality-specific training objectives, &#x201C;meaning&#x201D; will be decentralized and potentially inconsistent across modalities, especially as a result of pre-training. This is not a recipe for the formation of coherent concepts.</p><p>Furthermore, it is not clear that today&#x2019;s modalities are an appropriate partitioning of the observation and action spaces for an embodied agent. It is not obvious that, e.g., images and text should be represented as separate observation streams, nor text production and motion planning as separate action capabilities. <strong>The human capacities for reading, seeing, speaking, and moving are ultimately mediated by overlapping cognitive structures.</strong> <strong>Making structural assumptions about how modalities ought to be processed is likely to hinder the discovery of more fundamental cognition </strong>that is responsible for processing data in all modalities. One solution would be to consolidate unnaturally partitioned modalities into a unified data representation. This would encourage networks to learn intelligent processes that generalize across modalities. Intuitively, <strong>a model that can understand the visual world as well as humans can</strong> &#x2014; including everything from human writing to traffic signs to visual art &#x2014; <strong>should not make a serious architectural distinction between images and text. </strong>Part of the reason why VLMs can&#x2019;t, e.g., count the number of letters in a word is because they can&#x2019;t <em>see</em> what they are writing.</p><p>Finally, the <strong>learn-from-scale approach trains models to copy the conceptual structure of humans instead of learning the general capability to form novel concepts on their own.</strong> Humans have spent hundreds of thousands of years refining concepts and passing them memetically through culture and language. Today&#x2019;s models are trained only on the end result of this process: the present-day conceptual structures that make it into the corpus. By <a href="https://en.wikipedia.org/wiki/Goodhart%27s_law">optimizing for the ultimate products</a> of our intelligence, we have ignored the question of how those products were invented and discovered. Humans have a unique ability to form durable concepts from few examples and ascribe names to them, reason about them analogically, etc. While the in-context capabilities of today&#x2019;s models can be impressive, <a href="https://arxiv.org/abs/2311.08993">they grow increasingly limited</a> as tasks become more complex and stray further from the training data. <strong>The flexibility to form new concepts from experience is a foundational attribute of general intelligence</strong>, we should think carefully about how it arises.</p><p>While structure-agnostic scale maximalism has succeeded in producing LLMs and LVMs that pass Turing tests, a multimodal scale maximalist approach to AGI will not bear similar fruit. <strong>Instead of pre-supposing structure in individual modalities, we should design a setting in which modality-specific processing emerges naturally.</strong> For example, <a href="https://arxiv.org/abs/2502.01568">my recent paper</a> on visual theory of mind saw abstract symbols naturally emerge from communication between image-classifying agents, blurring the lines between text and image processing. Eventually, we should hope to reintegrate as many features of intelligence as possible <a href="https://www.youtube.com/watch?v=-hztAN4MdrA">under the same umbrella</a>. However, it is not clear whether there is genuine commercial viability in such an approach as long as scaling and fine-tuning narrow intelligence models solves commercial use-cases.</p><h2 id="conclusion">Conclusion</h2><p>The overall promise of scale maximalism is that a Frankenstein AGI can be sewed together using general models of narrow domains. I argue that this is extremely unlikely to yield an AGI that feels complete in its intelligence. If we intend to continue reaping the streamlined efficiency of modality-specific processing, we must be intentional in how modalities are united &#x2014; ideally drawing from human intuition and classical fields of study, e.g. <a href="http://nscl.csail.mit.edu/">this work from MIT</a>. Alternatively, we can re-formulate learning as an embodied and interactive process where disparate modalities naturally fuse together. We could do this by, e.g., processing images, text, and video using the same perception system and producing actions for generating text, manipulating objects, and navigating environments using the same action system. What we will lose in efficiency we will gain in flexible cognitive ability.</p><p>In a sense, the most challenging mathematical piece of the AGI puzzle has already been solved: the discovery of universal function approximators. What&#x2019;s left is to inventory the functions we need and determine how they ought to be arranged into a coherent whole. This is a conceptual problem, not a mathematical one.</p><hr><h2 id="acknowledgements">Acknowledgements</h2><p>I would like to thank <a href="https://lucasgelfond.online/">Lucas Gelfond</a>, <a href="https://db7894.github.io/">Daniel Bashir</a>, <a href="https://cs.brown.edu/people/gdk/">George Konidaris</a>, and my father, Joseph Spiegel, for their thoughtful and thorough feedback on this work. Thanks to <a href="https://alinapringle.format.com/">Alina Pringle</a> for the wonderful illustration made for this piece.</p><h2 id="author-bio">Author Bio</h2><p>Benjamin is a PhD candidate in Computer Science at Brown University. He is interested in models of language understanding that ground meaning to elements of structured decision-making. For more info see his <a href="https://benjaminaspiegel.com/">personal website</a>.</p><h2 id="citation">Citation</h2><p>For attribution in academic contexts or books, please cite this work as</p><!--kg-card-begin: markdown--><pre><code>Benjamin A. Spiegel, &quot;AGI Is Not Multimodal&quot;, The Gradient, 2025.
</code></pre>
<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><pre><code>@article{spiegel2025agi,
    author = {Benjamin A. Spiegel},
    title = {AGI Is Not Multimodal},
    journal = {The Gradient},
    year = {2025},
    howpublished = {\url{https://thegradient.pub/agi-is-not-multimodal},
}
</code></pre>
<!--kg-card-end: markdown--><h2 id="references">References</h2><p>Andreas, Jacob. &#x201C;Language Models, World Models, and Human Model-Building.&#x201D; <em>Mit.edu</em>, 2024, lingo.csail.mit.edu/blog/world_models/.</p><p>Belkin, Mikhail, et al. &quot;Reconciling modern machine-learning practice and the classical bias&#x2013;variance trade-off.&quot; <em>Proceedings of the National Academy of Sciences</em> 116.32 (2019): 15849-15854.</p><p>Bernhard Kerbl, et al. &#x201C;3D Gaussian Splatting for Real-Time Radiance Field Rendering.&#x201D; <em>ACM Transactions on Graphics</em>, vol. 42, no. 4, 26 July 2023, pp. 1&#x2013;14, https://doi.org/10.1145/3592433.</p><p>Chomsky, Noam. 1965. Aspects of the theory of syntax. Cambridge, Massachusetts: MIT Press.</p><p><em>Designing an Intelligence</em>. Edited by George Konidaris, MIT Press, 2026.</p><p>Emily M. Bender and Alexander Koller. 2020. Climbing towards NLU: On Meaning, Form, and Understanding in the Age of Data. In <em>Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</em>, pages 5185&#x2013;5198, Online. Association for Computational Linguistics.</p><p>Eye on AI. &#x201C;The Mastermind behind GPT-4 and the Future of AI | Ilya Sutskever.&#x201D; <em>YouTube</em>, 15 Mar. 2023, www.youtube.com/watch?v=SjhIlw3Iffs&amp;list=PLpdlTIkm0-jJ4gJyeLvH1PJCEHp3NAYf4&amp;index=64. Accessed 18 May 2025.</p><p>Frank, Michael C. &#x201C;Bridging the data gap between children and large language models.&#x201D; <em>Trends in cognitive sciences</em> vol. 27,11 (2023): 990-992. doi:10.1016/j.tics.2023.08.007</p><p>Garrett, Caelan Reed, et al. &quot;Integrated task and motion planning.&quot; <em>Annual review of control, robotics, and autonomous systems</em> 4.1 (2021): 265-293.APA</p><p>Goodhart, C.A.E. (1984). Problems of Monetary Management: The UK Experience. In: Monetary Theory and Practice. Palgrave, London. https://doi.org/10.1007/978-1-349-17295-5_4</p><p>Hooker, Sara. The hardware lottery. Commun. ACM 64, 12 (December 2021), 58&#x2013;65. https://doi.org/10.1145/3467017</p><p>Huh, Minyoung, et al. &quot;The Platonic Representation Hypothesis.&quot; <em>Forty-first International Conference on Machine Learning</em>. 2024.</p><p>Kaplan, Jared, et al. &quot;Scaling laws for neural language models.&quot; <em>arXiv preprint arXiv:2001.08361</em> (2020).</p><p>Lake, Brenden M. et al. &#x201C;Building Machines That Learn and Think like People.&#x201D; <em>Behavioral and Brain Sciences</em> 40 (2017): e253. Web.</p><p>Li, Kenneth, et al. &quot;Emergent world representations: Exploring a sequence model trained on a synthetic task.&quot; <em>ICLR</em> (2023).</p><p>Luiten, Jonathon, Georgios, Kopanas, Bastian, Leibe, Deva, Ramanan. &quot;Dynamic 3D Gaussians: Tracking by Persistent Dynamic View Synthesis.&quot; <em>3DV</em>. 2024.</p><p>Mao, Jiayuan, Chuang, Gan, Pushmeet, Kohli, Joshua B., Tenenbaum, Jiajun, Wu. &quot;The Neuro-Symbolic Concept Learner: Interpreting Scenes, Words, and Sentences From Natural Supervision.&quot; <em>International Conference on Learning Representations</em>. 2019.</p><p>Mitchell, Melanie. &#x201C;LLMs and World Models, Part 1.&#x201D; <em>Substack.com</em>, AI: A Guide for Thinking Humans, 13 Feb. 2025, aiguide.substack.com/p/llms-and-world-models-part-1. Accessed 18 May 2025.</p><p>Mu, Norman. &#x201C;Norman Mu | the Myth of Data Inefficiency in Large Language Models.&#x201D; <em>Normanmu.com</em>, 14 Feb. 2025, www.normanmu.com/2025/02/14/data-inefficiency-llms.html. Accessed 18 May 2025.</p><p>Newell, Allen, and Herbert A. Simon. &#x201C;Computer Science as Empirical Inquiry: Symbols and Search.&#x201D; <em>Communications of the ACM</em>, vol. 19, no. 3, 1 Mar. 1976, pp. 113&#x2013;126, https://doi.org/10.1145/360018.360022.</p><p>Peng, Hao, et al. &#x201C;When Does In-Context Learning Fall Short and Why? A Study on Specification-Heavy Tasks.&#x201D; <em>ArXiv.org</em>, 2023, arxiv.org/abs/2311.08993.</p><p>Spiegel, Benjamin, et al. &#x201C;Visual Theory of Mind Enables the Invention of Early Writing Systems.&#x201D; <em>CogSci</em>, 2025, arxiv.org/abs/2502.01568.</p><p>Sutton, Richard S. <em>Introduction to Reinforcement Learning</em>. Cambridge, Mass, Mit Press, 04-98, 1998.</p><p>Vafa, Keyon, et al. &quot;Evaluating the world model implicit in a generative model.&quot; <em>Advances in Neural Information Processing Systems</em> 37 (2024): 26941-26975.</p><p>Vaswani, Ashish; Shazeer, Noam; Parmar, Niki; Uszkoreit, Jakob; Jones, Llion; Gomez, Aidan N; Kaiser, &#x141;ukasz; Polosukhin, Illia (December 2017). &quot;Attention is All you Need&quot;. In I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett (ed.). <em>31st Conference on Neural Information Processing Systems (NIPS)</em>. Advances in Neural Information Processing Systems. Vol. 30. Curran Associates, Inc. arXiv:1706.03762.</p><p>Winograd, Terry. &#x201C;Thinking Machines: Can There Be? Are We?&#x201D; <em>The Boundaries of Humanity: Humans, Animals, Machines</em>, edited by James Sheehan and Morton Sosna, Berkeley: University of California Press, 1991, pp. 198&#x2013;223.</p><p>Wu, Shangda, et al. &quot;Beyond language models: Byte models are digital world simulators.&quot; <em>arXiv preprint arXiv:2402.19155</em> (2024). </p>]]></content:encoded></item><item><title><![CDATA[Shape, Symmetries, and Structure: The Changing Role of Mathematics in Machine Learning Research]]></title><description><![CDATA[<h3 id="what-is-the-role-of-mathematics-in-modern-machine-learning">What is the Role of Mathematics in Modern Machine Learning?</h3><p>The past decade has witnessed a shift in how progress is made in machine learning. Research involving carefully designed and mathematically principled architectures result in only marginal improvements while compute-intensive and engineering-first efforts that scale to ever larger training sets</p>]]></description><link>https://thegradient.pub/shape-symmetry-structure/</link><guid isPermaLink="false">673686c693571d5c8c155078</guid><dc:creator><![CDATA[Henry Kvinge]]></dc:creator><pubDate>Sat, 16 Nov 2024 16:46:15 GMT</pubDate><media:content url="https://thegradient.pub/content/images/2024/11/DALL-E-2024-11-15-15.40.52---Create-an-abstract-image-that-illustrates-how-ReLU-based-neural-networks-shatter-input-space-into-numerous-polygonal-regions--each-behaving-like-a-lin.webp" medium="image"/><content:encoded><![CDATA[<h3 id="what-is-the-role-of-mathematics-in-modern-machine-learning">What is the Role of Mathematics in Modern Machine Learning?</h3><img src="https://thegradient.pub/content/images/2024/11/DALL-E-2024-11-15-15.40.52---Create-an-abstract-image-that-illustrates-how-ReLU-based-neural-networks-shatter-input-space-into-numerous-polygonal-regions--each-behaving-like-a-lin.webp" alt="Shape, Symmetries, and Structure: The Changing Role of Mathematics in Machine Learning Research"><p>The past decade has witnessed a shift in how progress is made in machine learning. Research involving carefully designed and mathematically principled architectures result in only marginal improvements while compute-intensive and engineering-first efforts that scale to ever larger training sets and model parameter counts result in remarkable new capabilities unpredicted by existing theory. Mathematics and statistics, once the primary guides of machine learning research, now struggle to provide immediate insight into the latest breakthroughs. This is not the first time that empirical progress in machine learning has outpaced more theory-motivated approaches, yet the magnitude of recent advances has forced us to swallow the bitter pill of the &#x201C;Bitter Lesson&#x201D; yet again [1].</p><p>This shift has prompted speculation about mathematics&#x2019; diminished role in machine learning research moving forward. It is already evident that mathematics will have to share the stage with a broader range of perspectives (for instance, biology which has deep experience drawing conclusions about irreducibly complex systems or the social sciences as AI is integrated ever more deeply into society). The increasingly interdisciplinary nature of machine learning should be welcomed as a positive development by all researchers.</p><p>However, we argue that mathematics remains as relevant as ever; its role is simply evolving. For example, whereas mathematics might once have primarily provided theoretical guarantees on model performance, it may soon be more commonly used for post-hoc explanations of empirical phenomena observed in model training and performance&#x2013;a role analogous to one that it plays in physics. Similarly, while mathematical intuition might once have guided the design of handcrafted features or architectural details at a granular level, its use may shift to higher-level design choices such as matching architecture to underlying task structure or data symmetries.</p><p>None of this is completely new. Mathematics has always served multiple purposes in machine learning. After all, the translation equivariant convolutional neural network, which exemplifies the idea of architecture matching data symmetries mentioned above is now over 40 years old. What&#x2019;s changing are the kinds of problems where mathematics will have the greatest impact and the ways it will most commonly be applied.</p><p>An intriguing consequence of the shift towards scale is that it has broadened the scope of the fields of mathematics applicable to machine learning. &#x201C;Pure&#x201D; mathematical domains such as topology, algebra, and geometry, are now joining the more traditionally applied fields of probability theory, analysis, and linear algebra. These pure fields have grown and developed over the last century to handle high levels of abstraction and complexity, helping mathematicians make discoveries about spaces, algebraic objects, and combinatorial processes that at first glance seem beyond human intuition. These capabilities promise to address many of the biggest challenges in modern deep learning. <br></p><p>In this article we will explore several areas of current research that demonstrate the enduring ability of mathematics to guide the process of discovery and understanding in machine learning.</p><figure class="kg-card kg-image-card"><img src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXcr6ZUCNH3oKGK9XQzvDZOs1kJhvjym4RrMAlENx0OHrK-IBsQcBQQW2wDKu2_g2tNJxXVd32BI5llBopCmD-IAFV9zfhjvQ2ek5rIOgUMqwhK-qFhri2iaU718yl4BzISTanzZt9a2Rix04c6GUpdFR4Co?key=h8RUuDFEFKnzGnrKx9gMkg" class="kg-image" alt="Shape, Symmetries, and Structure: The Changing Role of Mathematics in Machine Learning Research" loading="lazy" width="529" height="372"></figure><p><em>Figure 1: Mathematics can illuminate the ways that ReLU-based neural networks shatter input space into countless polygonal regions, in each of which the model behaves like a linear map [2, 3, 4]. These decompositions create beautiful patterns. (Figure made with SplineCam [5]).</em></p><h3 id="describing-an-elephant-from-a-pin-prick">Describing an Elephant from a Pin Prick</h3><p>Suppose you are given a 7 billion parameter neural network with 50 layers and are asked to analyze it; how would you begin? The standard procedure would be to calculate relevant performance statistics. For instance, the accuracy on a suite of evaluation benchmarks. In certain situations, this may be sufficient. However, deep learning models are complex and multifaceted. Two computer vision models with the same accuracy may have very different generalization properties to out-of-distribution data, calibration, adversarial robustness, and other &#x201C;secondary statistics&#x201D; that are critical in many real-world applications. Beyond this, all evidence suggests that to build a complete scientific understanding of deep learning, we will need to venture beyond evaluation scores. Indeed, just as it is impossible to capture all the dimensions of humanity with a single numerical quantity (e.g., IQ, height), trying to understand a model by one or even several statistics alone is fundamentally limiting.</p><p>One difference between understanding a human and understanding a model is that we have easy access to all model parameters and all the individual computations that occur in a model. Indeed, by extracting a model&#x2019;s hidden activations we can directly trace the process by which a model converts raw input into a prediction. Unfortunately, the world of hidden activations is far less hospitable than that of simple model performance statistics. Like the initial input, hidden activations are usually high dimensional, but unlike input data they are not structured in a form that humans can understand. If we venture into even higher dimensions, we can try to understand a model through its weights directly. Here, in the space of model weights, we have the freedom to move in millions to billions of orthogonal directions from a single starting point. How do we even begin to make sense of these worlds?</p><p>There is a <a href="https://en.wikipedia.org/wiki/Blind_men_and_an_elephant">well-known fable</a> in which three blind men each feel a different part of an elephant. The description that each gives of the animal is completely different, reflecting only the body part that that man felt. We argue that unlike the blind men who can at least use their hand to feel a substantial part of one of the elephant&#x2019;s body parts, current methods of analyzing the hidden activations and weights of a model are akin to trying to describe the elephant from the touch of a single pin.</p><h3 id="tools-to-characterize-what-we-cannot-visualize">Tools to Characterize What We Cannot Visualize</h3><p>Despite the popular perception that mathematicians exclusively focus on solving problems, much of research mathematics involves understanding the right questions to ask in the first place. This is natural since many of the objects that mathematicians study are so far removed from everyday experience that we start with very limited intuition for what we can hope to actually understand. Substantial effort is often required to build up tools that will enable us to leverage our existing intuition and achieve tractable results that increase our understanding. The concept of a <em>rotation </em>provides a nice example of this situation since these are very familiar in 2- and 3-dimensions, but become less and less accessible to everyday intuition as their dimension grows larger. In this latter case, the differing perspectives provided by pure mathematics become more and more important to gaining a more holistic perspective on what these actually are. <br></p><p>Those who know a little linear algebra will remember that rotations generalize to higher dimensions and that in $n$-dimensions they can be realized by $n \times n$ orthogonal matrices with determinant $1$. The set of these are commonly written as $SO(n)$ and called the <em>special orthogonal group</em>. Suppose we want to understand the set of all $n$-dimensional rotations. There are many complementary approaches to doing this. We can explore the linear algebraic structure of all matrices in $SO(n)$ or study $SO(n)$ based on how each element behaves as an operator acting on $\mathbb{R}^n$.</p><p>Alternatively, we can also try to use our innate spatial intuition to understand $SO(n)$. This turns out to be a powerful perspective in math. In any dimension $n$, $SO(n)$ is a geometric object called a <em>manifold</em>. Very roughly, a space that locally looks like Euclidean space, but which may have twists, holes, and other non-Euclidean features when we zoom out. Indeed, whether we make it precise or not, we all have a sense of whether two rotations are &#x201C;close&#x201D; to each other. For example, the reader would probably agree that $2$-dimensional rotations of $90^\circ$ and $91^\circ$ &#x201C;feel&#x201D; closer than rotations of $90^\circ$ and $180^\circ$. When $n=2$, one can show that the set of all rotations is geometrically &#x201C;equivalent&#x201D; to a $1$-dimensional circle. So, much of what we know about the circle can be translated to $SO(2)$.</p><p>What happens when we want to study the geometry of rotations in $n$-dimensions for $n &gt; 3$? If $n = 512$ (a latent space for instance), this amounts to studying a manifold in $512^2$-dimensional space. Our visual intuition is seemingly useless here since it is not clear how concepts that are familiar in 2- and 3-dimensions can be utilized in $512^2$-dimensions. Mathematicians have been confronting the problem of understanding the un-visualizable for hundreds of years. One strategy is to find generalizations of familiar spatial concepts from $2$ and $3$-dimensions to $n$-dimensions that connect with our intuition.</p><p>This approach is already being used to better understand and characterize experimental observations about the space of model weights, hidden activations, and input data of deep learning models. We provide a taste of such tools and applications here:</p><ul><li><em>Intrinsic Dimension: </em>Dimension is a concept that is familiar not only from our experience in the spatial dimensions that we can readily access, 1-, 2-, and 3-dimensions, but also from more informal notions of &#x201C;degrees of freedom&#x201D; in everyday systems such as driving a car (forward/back, turning the steering wheel either left or right). The notion of dimension arises naturally in the context of machine learning where we may want to capture the number of independent ways in which a dataset, learned representation, or collection of weight matrices actually vary.<br><br>In formal mathematics, the definitions of dimension depend on the kind of space one is studying but they all capture some aspect of this everyday intuition. As a simple example, if I walk along the perimeter of a circle, I am only able to move forward and backward, and thus the dimension of this space is $1$. For spaces like the circle which are manifolds, dimension can be formally defined by the fact that a sufficiently small neighborhood around each point looks like a subset of some Euclidean space $\mathbb{R}^k$. We then say that the manifold is $k$-dimensional. If we zoom in on a small segment of the circle, it almost looks like a segment of $\mathbb{R} = \mathbb{R}^1$, and hence the circle is $1$-dimensional.<br><br>The manifold hypothesis posits that many types of data (at least approximately) live on a low-dimensional manifold even though they are embedded in a high-dimensional space. If we assume that this is true, it makes sense that the dimension of this underlying manifold, called the intrinsic dimension of the data, is one way to describe the complexity of the dataset. Researchers have estimated intrinsic dimension for common benchmark datasets, showing that intrinsic dimension appears to be correlated to the ease with which models generalize from training to test sets [6], and can explain differences in model performance and robustness in different domains such as medical images [7]. Intrinsic dimension is also a fundamental ingredient in some proposed explanations of data scaling laws [8, 9], which underlie the race to build ever bigger generative models.<br><br>Researchers have also noted that the intrinsic dimension of hidden activations tend to change in a characteristic way as information passes through the model [10, 11] or over the course of the diffusion process [12]. These and other insights have led to the use of intrinsic dimension in detection of adversarial examples [13], AI-generated content [14], layers where hidden activations contain the richest semantic content [11], and hallucinations in generative models [15].</li></ul><p><br></p><ul><li><em>Curvature</em>: While segments of the circle may look &#x201C;straight&#x201D; when we zoom up close enough, their curvature means that they will never be exactly linear as a straight line is. The notion of curvature is a familiar one and once formalized, it offers a way of rigorously measuring the extent to which the area around a point deviates from being linear. Care must be taken, however. Much of our everyday intuition about curvature assumes a single dimension. On manifolds with dimension $2$ or greater, there are multiple, linearly independent directions that we can travel away from a point and each of these may have a different curvature (in the $1$-dimensional sense). As a result, there are a range of different generalizations of curvature for higher-dimensional spaces, each with slightly different properties.<br><br>The notion of curvature has played a central role in deep learning, especially with respect to the loss landscape where changes in curvature have been used to analyze training trajectories [16]. Curvature is also central to an intriguing phenomenon known as the &#x2018;edge of stability&#x2019;, wherein the curvature of the loss landscape over the course of training increases as a function of learning rate until it hovers around the point where the training run is close to becoming unstable [17]. In another direction, curvature has been used to calculate the extent that model predictions change as input changes. For instance, [18] provided evidence that higher curvature in decision boundaries correlates with higher vulnerability to adversarial examples and suggested a new regularization term to reduce this. Finally, motivated by work in neuroscience, [19] presented a method that uses curvature to highlight interesting differences in representation between the raw training data and a neural network&#x2019;s internal representation. A network may stretch and expand parts of the input space, generating regions of high curvature as it magnifies the representation of training examples that have a higher impact on the loss function.</li></ul><p></p><ul><li><em>Topology</em>: Both dimension and curvature capture local properties of a space that can be measured by looking at the neighborhood around a single point. On the other hand, the most notable feature of our running example, the circle, is neither its dimension nor its curvature, but rather the fact that it is circular. We can only see this aspect by analyzing the whole space at once. Topology is the field of mathematics that focuses on such &#x201C;global&#x201D; properties.<br><br>Topological tools such as homology, which counts the number of holes in a space, has been used to illuminate the way that neural networks process data, with [20] showing that deep learning models &#x201C;untangle&#x201D; data distributions, reducing their complexity layer by layer. Versions of homology have also been applied to the weights of networks to better understand their structural features, with [21] showing that such topological statistics can reliably predict optimal early-stopping times. Finally, since topology provides frameworks that capture the global aspects of a space, it has proved a rich source of ideas for how to design networks that capture higher order relationships within data, leading to a range of generalizations of graph neural networks built on top of topological constructions [22, 23, 24, 25].</li></ul><p>While the examples above have each been useful for gaining insight into phenomena related to deep learning, they were all developed to address challenges in other fields. We believe that a bigger payoff will come when the community uses the geometric paradigm described here to build new tools specifically designed to address the challenges that deep learning poses. Progress in this direction has already begun. Think for instance of linear mode connectivity which has helped us to better understand the loss landscape of neural networks [26] or work around the linear representation hypothesis which has helped to illuminate the way that concepts are encoded in the latent space of large language models [27]. One of the most exciting occurrences in mathematics is when the tools from one domain provide unexpected insight in another. Think of the discovery that Riemannian geometry provides some of the mathematical language needed for general relativity. We hope that a similar story will eventually be told for geometry and topology&#x2019;s role in deep learning.</p><h3 id="symmetries-in-data-symmetries-in-models">Symmetries in data, symmetries in models<br></h3><p>Symmetry is a central theme in mathematics, allowing us to break a problem into simpler components that are easier to solve. Symmetry has long played an important role in machine learning, particularly computer vision. In the classic dog vs. cat classification task for instance, an image that contains a dog continues to contain a dog regardless of whether we move the dog from one part of the image to another, whether we rotate the dog, or whether we reflect it. We say that the task is <em>invariant</em> to image translation, rotation, and reflection.</p><p>The notion of symmetry is mathematically encoded in the concept of a <em>group</em>, which is a set $G$ equipped with a binary operation $\star$ that takes two elements of $G$, $g_1$, $g_2$ as input and produces a third $g_1\star g_2$ as output. You can think of the integers $\mathbb{Z}$ with the binary operation of addition ($\star = +$) or the non-zero real numbers with the binary operation of multiplication ($\star = \times$). The set of $n$-dimensional rotations, $SO(n)$, also forms a group. The binary operation takes two rotations and returns a third rotation that is defined by simply applying the first rotation and then applying the second.</p><p>Groups satisfy axioms that ensure that they capture familiar properties of symmetries. For example, for any symmetry transformation, there should be an inverse operation that undoes the symmetry. If I rotate a circle by $90^{\circ}$, then I can rotate it back by $-90^{\circ}$ and return to where I started. Notice that not all transformations satisfy this property. For instance, there isn&#x2019;t a well-defined inverse for downsampling an image. Many different images downsample to the same (smaller) image.</p><p>In the previous section we gave two definitions of $SO(n)$: the first was the geometric definition, as rotations of $\mathbb{R}^n$, and the second was as a specific subset of $n \times n$ matrices. While the former definition may be convenient for our intuition, the latter has the benefit that linear algebra is something that we understand quite well at a computational level. The realization of an abstract group as a set of matrices is called a <em>linear representation</em> and it has proven to be one of the most fruitful methods of studying symmetry. It is also the way that symmetries are usually leveraged when performing computations (for example, in machine learning).</p><p>We saw a few examples of symmetries that can be found in the data of a machine learning task, such as the translation, rotation, and reflection symmetries in computer vision problems. Consider the case of a segmentation model. If one rotates an input image by $45^{\circ}$ and then puts it through the model, we will hope that we get a $45^{\circ}$ rotation of the segmentation prediction for the un-rotated image (this is illustrated in 1). After all, we haven&#x2019;t changed the content of the image.<br></p><figure class="kg-card kg-image-card"><img src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXd1UiuD880gmdVrtjEKvGPBHIr0dvdBsrLXAnxUFz6_KQNLyMekhxrSR2ROn-H8O3780yoKbJvF0tUEVZSEdsuDfbB7kSGw_CFCqsKzjC6-wpxN5dxLQd-e4g7qMsKnc8BCX1pw7Qh0-I9hsgY9EInhpROs?key=h8RUuDFEFKnzGnrKx9gMkg" class="kg-image" alt="Shape, Symmetries, and Structure: The Changing Role of Mathematics in Machine Learning Research" loading="lazy" width="534" height="323"></figure><p><em>Figure 2: The concept of rotation equivariance illustrated for a segmentation model. One gets the same output regardless of whether one rotates first and then applies the network or applies the network and then rotates.</em></p><figure class="kg-card kg-image-card"><img src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXe_QPixL6292p6Bz5yj_Hep_ypc6qrj-3q3Y7uIte0R5Nsc2ZPxNmSJOheOHohJY_0VbDi3LlyNSR61t94bHDfgTnJx0ssvyzU9KMtGLUKoqoiviKKTxpZR77Bb8VIzhkzd0Vxhspif10w8DnS3eWbjqwhW?key=h8RUuDFEFKnzGnrKx9gMkg" class="kg-image" alt="Shape, Symmetries, and Structure: The Changing Role of Mathematics in Machine Learning Research" loading="lazy" width="522" height="273"></figure><p><em>Figure 3: Equivariance holds when taking the top path (applying the network first and then the symmetry action) gives the same result as taking the bottom path (applying the symmetry transformation and then the network).</em></p><p>This property of a function (including neural networks), that applying a symmetry transformation before the function yields the same result as applying the symmetry transformation after the function is called <em>equivariance</em> and can be captured by the diagram in Figure 3. The key point is that we get the same result whether we follow the upper path (applying the network first and then applying the group action) as when we follow the lower path (applying the group first and then applying the network). Conveniently, the concept of invariance, where applying a symmetry operation to input has no effect on the output of the function is a special case of equivariance where the action on the output space is defined to be trivial (applying symmetry actions does nothing).</p><p>Invariance and equivariance in deep learning models can be beneficial for a few reasons. Firstly, such a model will yield more predictable and consistent results across symmetry transformations. Secondly, through equivariance we can sometimes simplify the learning process with fewer parameters (compare the number of parameters in a convolutional neural network and an MLP of similar performance) and fewer modes of variation to learn in the data (a rotation invariant image classifier only needs to learn one orientation of each object rather than all possible orientations).</p><p>But how do we ensure that our model is equivariant? One way is to build our network with layers that are equivariant by design. By far the most well-known example of this is the convolutional neural network, whose layers are (approximately) equivariant to image translation. This is one reason why using a convolutional neural network for dog vs cat classification doesn&#x2019;t require learning to recognize a dog at every location in an image as it might with an MLP. With a little thought, one can often come up with layers which are equivariant to a specific group. Unfortunately, being constrained to equivariant layers that we find in an ad-hoc manner often leaves us with a network with built-in equivariance but limited expressivity.</p><p>Fortunately, for most symmetry groups arising in machine learning, representation theory offers a comprehensive description of all possible linear equivariant maps. Indeed, it is a beautiful mathematical fact that all such maps are built from atomic building blocks called <em>irreducible representations</em>. Happily, in many cases, the number of these irreducible representations is finite. Understanding the irreducible representations of a group can be quite powerful. Those familiar with the ubiquitous discrete Fourier transform (DFT) of a sequence of length $n$ are already familiar with the irreducible representations of one group, the cyclic group generated by a rotation by $360 ^{\circ}/n$ (though we note that moving between the description we give here and the description of the DFT found in the signal processing literature takes a little thought).</p><p>There is now a rich field of research in deep learning that uses group representations to systematically build expressive equivariant architectures. Some examples of symmetries that have been particularly well-studied include: rotation and reflection of images [28, 29, 30, 31], 3-dimensional rotation and translation of molecular structures [32] or point clouds [33], and permutations for learning on sets [34] or nodes of a graph [35]. Encoding equivariance to more exotic symmetries has also proven useful for areas such as theoretical physics [36] and data-driven optimization [37].</p><p>Equivariant layers and other architectural approaches to symmetry awareness are a prime example of using mathematics to inject high-level priors into a model. Do these approaches represent the future of learning in the face of data symmetries? Anecdotally, the most common approach to learning on data with symmetries continues to be using enough training data and enough data augmentation for the model to learn to handle the symmetries on its own. Two years ago, the author would have speculated that these latter approaches only work for simple cases, such as symmetries in 2-dimensions, and will be outperformed by models which are equivariant by design when symmetries become more complex. Yet, we continue to be surprised by the power of scale. After all, AlphaFold3 [38] uses a non-equivariant architecture despite learning on data with several basic symmetries. We speculate that there may be a threshold on the ratio of symmetry complexity on the one hand and the amount of training data on the other, that determines whether built-in equivariance will outperform learned equivariance [39, 40].<br></p><p>If this is true, we can expect to see models move away from bespoke equivariant architectures as larger datasets become available for a specific application. At the same time, since compute will always be finite, we predict that there will be some applications with exceptionally complex symmetries that will always require some built-in priors (for example, AI for math or algorithmic problems). Regardless of where we land on this spectrum, mathematicians can look forward to an interesting comparison of the ways humans inject symmetry into models vs the way that models learn symmetries on their own [41, 42].<br><br></p><figure class="kg-card kg-image-card"><img src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXcJa_1Ow2zrMVuk1hJiOJtTJBq5bH7zyogibZ5fqQu85ERGFEjcX4jRn7r_rnZvTrCdpN5OzeVQUBLu60DJP_aIR4uoHq33tRMcoPAUf7qumOeJLreCkttvqtQEssCh90UwlbWkzBoK79FV54R6ncO2c_Ij?key=h8RUuDFEFKnzGnrKx9gMkg" class="kg-image" alt="Shape, Symmetries, and Structure: The Changing Role of Mathematics in Machine Learning Research" loading="lazy" width="572" height="129"></figure><p><em>Figure 4: A cartoon illustrating why adding a permutation and its inverse before and after a pointwise nonlinearity produces an equivalent model (even though the weights will be different). Since permutations can be realized by permutation matrices, the crossed arrows on the right can be merged into the fully-connected layer.</em></p><p>Of course, symmetry is not only present in data but also in the models themselves. For instance, the activations of hidden layers of a network are invariant to permutation. We can permute activations before entering the non-linearity and if we un-permute them afterward, the model (as a function) does not change (Figure 4). This means that we have an easy recipe for generating an exponentially large number of networks that have different weights but behave identically on data. <br></p><p>While simple, this observation produces some unexpected results. There is evidence, for instance, that while the loss landscape of neural networks is highly non-convex, it may be much less non-convex when we consider all networks that can be produced through this permutation operation as equivalent [43, 44]. This means that your network and my network may not be connected by a linear path of low loss, but such a path may exist between your network and a permutation of my network. Other research has looked at whether it may be possible to use symmetries to accelerate optimization by &#x2018;teleporting&#x2019; a model to a more favorable location in the loss landscape [45, 46]. Finally, permutation symmetries also provide one type of justification for an empirical phenomenon where individual neurons in a network tend to encode more semantically meaningful information than arbitrary linear combinations of such neurons [47].</p><h3 id="taming-complexity-with-abstraction">Taming Complexity with Abstraction</h3><p>When discussing symmetry, we used the diagram in Figure 3 to define equivariance. One of the virtues of this approach is that we never had to specify details about the input data or architecture that we used. The spaces could be vector spaces and the maps linear transformations, they could be neural networks of a specific architecture, or they could just be sets and arbitrary functions between them&#x2013;the definition is valid for each. This <em>diagrammatic</em> point of view, which looks at mathematical constructions in terms of the composition of maps between objects rather than the objects themselves, has been very fruitful in mathematics and is one gateway to the subject known as <em>category theory</em>. Category theory is now the lingua franca in many areas of mathematics since it allows mathematicians to translate definitions and results across a wide range of contexts.</p><p>Of course, deep learning is at its core all about function composition, so it is no great leap to try and connect it to the diagrammatic tradition in mathematics. The focus of function composition in the two disciplines is different, however. In deep learning we take simple layers that alone lack expressivity and compose them together to build a model capable of capturing the complexity of real-world data. With this comes the tongue-in-cheek demand to &#x201C;stack more layers!&#x201D;. Category theory instead tries to find a universal framework that captures the essence of structures appearing throughout mathematics. This allows mathematicians to uncover connections between things that look very different at first glance. For instance, category theory gives us the language to describe how the topological structure of a manifold can be encoded in groups via homology or homotopy theory.</p><p>It can be an interesting exercise to try to find a diagrammatic description of familiar constructions like the product of two sets $X$ and $Y$. Focusing our attention on maps rather than objects we find that what characterizes $X \times Y$ is the existence of the two canonical projections $\pi_1$ and $\pi_2$, the former sending $(x,y) \mapsto x$ and $(x,y) \mapsto y$ (at least in more familiar settings where $X$ and $Y$ are, for example, sets). Indeed, the <em>product </em>$X \times Y$ (regardless of whether $X$ and $Y$ are sets, vectors spaces, etc.) is the unique object such that for any $Z$ with maps $f_1: Z \rightarrow X$ and $f_2: Z \rightarrow Y$, there is a map $h: Z \rightarrow X \times Y$ that satisfies the commutative diagram in Figure 5.</p><p>While this construction is a little involved for something as familiar as a product it has the remarkable property that it allows us to define a &#x201C;product&#x201D; even when there is no &#xA0;underlying set structure (that is, those settings where we cannot resort to defining $X \times Y$ as the set of pairs of $(x,y)$ for $x \in X$ and $y \in Y$).</p><p><br></p><figure class="kg-card kg-image-card"><img src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXd0afUER-JEqT9BBW0z5ip7HSPD_ORKlpxTaQQFpep7MZF3DKfhgca3XbrZ2aGGTTnxcyOD3csHF1hdODeSXFx-nC63Mlw2etuY9xtM-AUvec4aIZKJK0hl2QiuxyJPzmlr18GbJA?key=h8RUuDFEFKnzGnrKx9gMkg" class="kg-image" alt="Shape, Symmetries, and Structure: The Changing Role of Mathematics in Machine Learning Research" loading="lazy" width="624" height="277"></figure><p><em>Figure 5: The commutative diagram that describes a product $X \times Y$. For any $Z$ with maps $f_1: Z \rightarrow X$ and $f_2: Z \rightarrow Y$, there exists a unique map $h: Z \rightarrow X \times Y$ such that $f_1 = \pi_1 \circ h$ and $f_2 = \pi_2 \circ h$ where $\pi_1$ and $\pi_2$ are the usual projection maps from $X \times Y$ to $X$ and $X \times Y$ to $Y$ respectively.</em><br></p><p>One can reasonably argue that diagrammatic descriptions of well-known constructions, like products, are not useful for the machine learning researcher. After all, we already know how to form products in all of the spaces that come up in machine learning. On the other hand, there are more complicated examples where diagrammatics mesh well with the way we build neural network architectures in practice.<br></p><figure class="kg-card kg-image-card"><img src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXf30eZdcoTcrjBuYEo6BUjm4gmw8fvcY9kLpDsspW0qPoIVu6LN5mfd1ks5qiMtf9J1DyPNDtzDDLDpxVi7n5j62DxlIfkwyo5V4gAC7MeeMpUaDzOMgsU4Mqjrs7fUXL-hc_BeqPd9Upu6L0wmKnDzkop4?key=h8RUuDFEFKnzGnrKx9gMkg" class="kg-image" alt="Shape, Symmetries, and Structure: The Changing Role of Mathematics in Machine Learning Research" loading="lazy" width="624" height="352"></figure><p><em>Figure 6: Fiber bundles capture the notion that a space might locally look like a product but globally have twists in it.</em></p><p>Fiber bundles are a central construction in geometry and topology that capture the notion that a space may locally look like a product but may have twists that break this product structure globally. Compare the cylinder with the M&#xF6;bius band. We can build both of these by starting with a circle and taking a product with the line segment $(0,1)$. In the case of the cylinder, this really is just (topologically) the product of the circle and the segment $(0,1)$, but to form the M&#xF6;bius band we must add an additional twist that breaks the product structure. In these examples, the circle is called the <em>base </em>space and $(0,1)$ is called the <em>fiber</em>. While only the cylinder is a true product, both the cylinder and the M&#xF6;bius band are fiber bundles. Here is another way of thinking about a fiber bundle. A fiber bundle is a union of many copies of the fiber parametrized by the base space. In the M&#xF6;bius band/cylinder example, each point on the circle carries its own copy of $(0,1)$.</p><p>We drew inspiration from this latter description of fiber bundles when we were considering a conditional generation task in the context of a problem in materials science. Since the materials background is somewhat involved, we&#x2019;ll illustrate the construction via a more pedestrian, animal-classification analogue. Let $M$ be the manifold of all possible images containing a single animal. We can propose to decompose the variation in elements of $M$ into two parts, the species of animal in the image and everything else, where the latter could mean differences in background, lighting, pose, image quality, etc. One might want to explore the distribution of one of these factors of variation while fixing the other. For instance, we might want to fix the animal species and explore the variation we get in background, pose, etc. For example, comparing the variation in background for two different species of insect may tell the entomologist about the preferred habitat for different types of beetles.<br><br></p><figure class="kg-card kg-image-card"><img src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXfLfnOQw_uLzm58bcucM5zOzGLHKzbX8hyQU2muIPl994v1GQN0sfMwQgSjFwsaCDetRHW8WR_T71pjNX7waqch44PwUY6Dv8egfzRlOmo6e0BbDagYv99K6tMnvVeTAIb9ww9bT_3Ukcs4k7xHx-BH7cxR?key=h8RUuDFEFKnzGnrKx9gMkg" class="kg-image" alt="Shape, Symmetries, and Structure: The Changing Role of Mathematics in Machine Learning Research" loading="lazy" width="624" height="421"></figure><p><em>Figure 7: A cartoon visualizing how the set of all animal images could be decomposed into a local product of animal species and other types of variation.</em></p><p>One might hope to solve this problem by learning an encoding of $M$ into a product space $X_1 \times X_2$ where $X_1$ is a discrete set of points corresponding to animal species and $X_2$ is a space underlying the distribution of all other possible types of variation for a fixed species of animal. Fixing the species would then amount to choosing a specific element $x_1$ from $X_1$ and sampling from the distribution on $X_2$. The product structure of $X_1 \times X_2$ allows us to perform such independent manipulations of $X_1$ and $X_2$. On the other hand, products are rigid structures that impose strong, global topological assumptions on the real data distribution. We found that even on toy problems, it was hard to learn a good map from the raw data distribution to the product-structured latent space defined above. Given that fiber bundles are more flexible and still give us the properties we wanted from our latent space, we designed a neural network architecture to learn a fiber bundle structure on a data distribution [48].<br><br></p><figure class="kg-card kg-image-card"><img src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXdrgVDyhD0JXBm12Gh1NstAjVx3fSk8vM3Mg_3JGi6JpK3PYTWUpmgzW_BgmEOMeZahkdrzWEw2ThViUKXnEGFobRORcOMgifUin2kJY3-zFIq4fbj-4QO6x7ALnwn5qLU880r1raMaFC2yqn6RVyDPGEk?key=h8RUuDFEFKnzGnrKx9gMkg" class="kg-image" alt="Shape, Symmetries, and Structure: The Changing Role of Mathematics in Machine Learning Research" loading="lazy" width="415" height="240"></figure><p><em>Figure 8: The commutative diagram describing a fiber bundle. The map $\pi$ projects from neighborhoods of the total space to the base space, $U$ is a local neighborhood of the base space, and $F$ &#xA0;is the fiber. The diagram says that each point in the base space has a neighborhood $U$ &#xA0;such that when we lift this to the bundle, we get something that is homeomorphic (informally, equivalent) to the product of the neighborhood &#xA0;and the fiber. But this product structure may not hold globally over the whole space.</em></p><p>But how do we go from the abstract definition of a fiber bundle above to a neural network architecture that we can code up on a computer. It turns out there is a succinct diagrammatic definition of a fiber bundle (Figure 8) that can serve as a convenient template to build up an architecture from. We were able to proceed in a relatively na&#xEF;ve fashion, taking each of the maps in the diagram and building a corresponding stack of layers. The diagram itself then told us how to compose each of these components together. The commutativity of the diagram was engineered through a term in the loss function that ensures that $\pi = \text{proj}_1 \circ \varphi$. There were also some conditions on $\varphi$ and $\pi$ (such as the bijectivity of $\phi$) that needed to be engineered. Beyond this, we were surprised at the amount of flexibility we had. This is useful since it means this process is largely agnostic to data modality.</p><p>This is an elementary example of how the diagrammatic tradition in mathematics can provide us with a broader perspective on the design of neural networks, allowing us to connect deep structural principles with large-scale network design without having to specify small-scale details that might be problem dependent. Of course, all this fails to draw from anything beyond the surface of what the categorical perspective has to offer. Indeed, category theory holds promise as a unified framework to connect much of what appears and is done in machine learning [49].</p><h3 id="conclusion">Conclusion<br></h3><p>In the mid-twentieth century, Eugene Wigner marveled at the &#x201C;the unreasonable effectiveness of mathematics&#x201D; as a framework for not only describing existing physics but also anticipating new results in the field [50]. A mantra more applicable to recent progress in machine learning is &#x201C;the unreasonable effectiveness of data&#x201D; [51] and compute. This could appear to be a disappointing situation for mathematicians who might have hoped that machine learning would be as closely intertwined to advanced mathematics as physics is. However, as we&#x2019;ve demonstrated, while mathematics may not maintain the same role in machine learning research that it has held in the past, the success of scale actually opens new paths for mathematics to support progress in machine learning research. These include:</p><ol><li>Providing powerful tools for deciphering the inner workings of complex models</li><li>Offering a framework for high-level architectural decisions that leave the details to the learning algorithm</li><li>Bridging traditionally isolated domains of mathematics like topology, abstract algebra, and geometry with ML and data science applications.<br></li></ol><p>Should the way things have turned out surprise us? Perhaps not, given that machine learning models ultimately reflect the data they are trained on and in most cases this data comes from fields (such as natural language or imagery) which have long resisted parsimonious mathematical models. <br></p><p>Yet, this situation is also an opportunity for mathematics. Performant machine learning models may provide a gateway for mathematical analysis of a range of fields that were previously inaccessible. It&#x2019;s remarkable for instance that trained word embeddings transform semantic relationships into algebraic operations on vectors in Euclidean space (for instance, &#x2018;Italian&#x2019; - &#x2018;Italy&#x2019; + &#x2018;France&#x2019; = &#x2018;French&#x2019;). Examples like this hint at the potential for mathematics to gain a foothold in complex, real-world settings by studying the machine learning models that have trained on data from these settings.<br></p><p>As more and more of the data in the world is consumed and mathematicised by machine learning models, it will be an increasingly interesting time to be a mathematician. The challenge now lies in adapting our mathematical toolkit to this new landscape, where empirical breakthroughs often precede theoretical understanding. By embracing this shift, mathematics can continue to play a crucial, albeit evolving, role in shaping the future of machine learning.<br></p><p><em>The author would like to thank Darryl Hannan for help with figures, Davis Brown, Charles Godfrey, and Scott Mahan for useful feedback on drafts, as well as the staff of the Gradient for useful conversations and help editing this article. For resources and events around the growing community of mathematicians and computer scientists using topology, algebra, and geometry (TAG) to better understand and build more robust machine learning systems, please visit us at </em><a href="https://www.tagds.com"><em>https://www.tagds.com</em></a><em>.</em></p><p></p><h2 id="references">References</h2><p>[1] Richard Sutton. &quot;The bitter lesson&quot;. In: <em>Incomplete Ideas (blog)</em> 13.1 (2019), p. 38.</p><p>[2] Guido F Montufar et al. &quot;On the number of linear regions of deep neural networks&quot;. In: <em>Advances in Neural Information Processing Systems</em> 27 (2014).</p><p>[3] Boris Hanin and David Rolnick. &quot;Complexity of linear regions in deep networks&quot;. In: <em>International Conference on Machine Learning</em>. PMLR. 2019, pp. 2596&#x2013;2604.</p><p>[4] J Elisenda Grigsby and Kathryn Lindsey. &quot;On transversality of bent hyperplane arrangements and the topological expressiveness of ReLU neural networks&quot;. In: <em>SIAM Journal on Applied Algebra and Geometry</em> 6.2 (2022), pp. 216&#x2013;242.</p><p>[5] Ahmed Imtiaz Humayun et al. &quot;Splinecam: Exact visualization and characterization of deep network geometry and decision boundaries&quot;. In: <em>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>. 2023, pp. 3789&#x2013;3798.</p><p>[6] Phillip Pope et al. &quot;The intrinsic dimension of images and its impact on learning&quot;. In: <em>arXiv preprint</em> arXiv:2104.08894 (2021).</p><p>[7] Nicholas Konz and Maciej A Mazurowski. &quot;The Effect of Intrinsic Dataset Properties on Generalization: Unraveling Learning Differences Between Natural and Medical Images&quot;. In: <em>arXiv preprint</em> arXiv:2401.08865 (2024).</p><p>[8] Yasaman Bahri et al. &quot;Explaining neural scaling laws&quot;. In: <em>arXiv preprint</em> arXiv:2102.06701 (2021).</p><p>[9] Utkarsh Sharma and Jared Kaplan. &quot;A neural scaling law from the dimension of the data manifold&quot;. In: <em>arXiv preprint</em> arXiv:2004.10802 (2020).</p><p>[10] Alessio Ansuini et al. &quot;Intrinsic dimension of data representations in deep neural networks&quot;. In: <em>Advances in Neural Information Processing Systems</em> 32 (2019).</p><p>[11] Lucrezia Valeriani et al. &quot;The geometry of hidden representations of large transformer models&quot;. In: <em>Advances in Neural Information Processing Systems</em> 36 (2024).</p><p>[12] Henry Kvinge, Davis Brown, and Charles Godfrey. &quot;Exploring the Representation Manifolds of Stable Diffusion Through the Lens of Intrinsic Dimension&quot;. In: <em>ICLR 2023 Workshop on Mathematical and Empirical Understanding of Foundation Models</em>.</p><p>[13] Xingjun Ma et al. &quot;Characterizing adversarial subspaces using local intrinsic dimensionality&quot;. In: <em>arXiv preprint</em> arXiv:1801.02613 (2018).</p><p>[14] Peter Lorenz, Ricard L Durall, and Janis Keuper. &quot;Detecting images generated by deep diffusion models using their local intrinsic dimensionality&quot;. In: <em>Proceedings of the IEEE/CVF International Conference on Computer Vision</em>. 2023, pp. 448&#x2013;459.</p><p>[15] Fan Yin, Jayanth Srinivasa, and Kai-Wei Chang. &quot;Characterizing truthfulness in large language model generations with local intrinsic dimension&quot;. In: <em>arXiv preprint</em> arXiv:2402.18048 (2024).</p><p>[16] Justin Gilmer et al. &quot;A loss curvature perspective on training instabilities of deep learning models&quot;. In: <em>International Conference on Learning Representations</em>. 2021.</p><p>[17] Jeremy Cohen et al. &quot;Gradient descent on neural networks typically occurs at the edge of stability&quot;. In: <em>International Conference on Learning Representations</em>. 2020.</p><p>[18] Seyed-Mohsen Moosavi-Dezfooli et al. &quot;Robustness via curvature regularization, and vice versa&quot;. In: <em>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>. 2019, pp. 9078&#x2013;9086.</p><p>[19] Francisco Acosta et al. &quot;Quantifying extrinsic curvature in neural manifolds&quot;. In: <em>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>. 2023, pp. 610&#x2013;619.</p><p>[20] Gregory Naitzat, Andrey Zhitnikov, and Lek-Heng Lim. &quot;Topology of deep neural networks&quot;. In: <em>Journal of Machine Learning Research</em> 21.184 (2020), pp. 1&#x2013;40.</p><p>[21] Bastian Rieck et al. &quot;Neural persistence: A complexity measure for deep neural networks using algebraic topology&quot;. In: <em>arXiv preprint</em> arXiv:1812.09764 (2018).</p><p>[22] Mustafa Hajij, Kyle Istvan, and Ghada Zamzmi. &quot;Cell complex neural networks&quot;. In: <em>arXiv preprint</em> arXiv:2010.00743 (2020).</p><p>[23] Cristian Bodnar. &quot;Topological deep learning: graphs, complexes, sheaves&quot;. PhD thesis. 2023.</p><p>[24] Jakob Hansen and Robert Ghrist. &quot;Toward a spectral theory of cellular sheaves&quot;. In: <em>Journal of Applied and Computational Topology</em> 3.4 (2019), pp. 315&#x2013;358.</p><p>[25] Yifan Feng et al. &quot;Hypergraph neural networks&quot;. In: <em>Proceedings of the AAAI Conference on Artificial Intelligence</em>. Vol. 33. 01. 2019, pp. 3558&#x2013;3565.</p><p>[26] Felix Draxler et al. &quot;Essentially no barriers in neural network energy landscape&quot;. In: <em>International Conference on Machine Learning</em>. PMLR. 2018, pp. 1309&#x2013;1318.</p><p>[27] Kiho Park, Yo Joong Choe, and Victor Veitch. &quot;The linear representation hypothesis and the geometry of large language models&quot;. In: <em>arXiv preprint</em> arXiv:2311.03658 (2023).</p><p>[28] Taco Cohen and Max Welling. &quot;Group equivariant convolutional networks&quot;. In: <em>International Conference on Machine Learning</em>. PMLR. 2016, pp. 2990&#x2013;2999.</p><p>[29] Maurice Weiler, Fred A Hamprecht, and Martin Storath. &quot;Learning steerable filters for rotation equivariant cnns&quot;. In: <em>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</em>. 2018, pp. 849&#x2013;858.</p><p>[30] Daniel E Worrall et al. &quot;Harmonic networks: Deep translation and rotation equivariance&quot;. In: <em>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</em>. 2017, pp. 5028&#x2013;5037.</p><p>[31] Diego Marcos et al. &quot;Rotation equivariant vector field networks&quot;. In: <em>Proceedings of the IEEE International Conference on Computer Vision</em>. 2017, pp. 5048&#x2013;5057.</p><p>[32] Alexandre Duval et al. &quot;A Hitchhiker&apos;s Guide to Geometric GNNs for 3D Atomic Systems&quot;. In: <em>arXiv preprint</em> arXiv:2312.07511 (2023).</p><p>[33] Nathaniel Thomas et al. &quot;Tensor field networks: Rotation-and translation-equivariant neural networks for 3d point clouds&quot;. In: <em>arXiv preprint</em> arXiv:1802.08219 (2018).</p><p>[34] Manzil Zaheer et al. &quot;Deep sets&quot;. In: <em>Advances in Neural Information Processing Systems</em> 30 (2017).</p><p>[35] V&#x131;ctor Garcia Satorras, Emiel Hoogeboom, and Max Welling. &quot;E (n) equivariant graph neural networks&quot;. In: <em>International Conference on Machine Learning</em>. PMLR. 2021, pp. 9323&#x2013;9332.</p><p>[36] Denis Boyda et al. &quot;Sampling using SU (N) gauge equivariant flows&quot;. In: <em>Physical Review D</em> 103.7 (2021), p. 074504.</p><p>[37] Hannah Lawrence and Mitchell Tong Harris. &quot;Learning Polynomial Problems with SL(2,\mathbb {R}) &#x2212;Equivariance&quot;. In: <em>The Twelfth International Conference on Learning Representations</em>. 2023.</p><p>[38] Josh Abramson et al. &quot;Accurate structure prediction of biomolecular interactions with AlphaFold 3&quot;. In: <em>Nature</em> (2024), pp. 1&#x2013;3.</p><p>[39] Scott Mahan et al. &quot;What Makes a Machine Learning Task a Good Candidate for an Equivariant Network?&quot; In: <em>ICML 2024 Workshop on Geometry-grounded Representation Learning and Generative Modeling</em>.</p><p>[40] Johann Brehmer et al. &quot;Does equivariance matter at scale?&quot; In: <em>arXiv preprint</em> arXiv:2410.23179 (2024).</p><p>[41] Chris Olah et al. &quot;Naturally Occurring Equivariance in Neural Networks&quot;. In: <em>Distill</em> (2020). <a href="https://distill.pub/2020/circuits/equivariance">https://distill.pub/2020/circuits/equivariance</a>. doi: 10.23915/distill.00024.004.</p><p>[42] Giovanni Luca Marchetti et al. &quot;Harmonics of Learning: Universal Fourier Features Emerge in Invariant Networks&quot;. In: <em>arXiv preprint</em> arXiv:2312.08550 (2023).</p><p>[43] Rahim Entezari et al. &quot;The role of permutation invariance in linear mode connectivity of neural networks&quot;. In: <em>arXiv preprint</em> arXiv:2110.06296 (2021).</p><p>[44] Samuel K Ainsworth, Jonathan Hayase, and Siddhartha Srinivasa. &quot;Git re-basin: Merging models modulo permutation symmetries&quot;. In: <em>arXiv preprint</em> arXiv:2209.04836 (2022).</p><p>[45] Bo Zhao et al. &quot;Symmetry teleportation for accelerated optimization&quot;. In: <em>Advances in Neural Information Processing Systems</em> 35 (2022), pp. 16679&#x2013;16690.</p><p>[46] Bo Zhao et al. &quot;Improving Convergence and Generalization Using Parameter Symmetries&quot;. In: <em>arXiv preprint</em> arXiv:2305.13404 (2023).</p><p>[47] Charles Godfrey et al. &quot;On the symmetries of deep learning models and their internal representations&quot;. In: <em>Advances in Neural Information Processing Systems</em> 35 (2022), pp. 11893&#x2013;11905.</p><p>[48] Nico Courts and Henry Kvinge. &quot;Bundle Networks: Fiber Bundles, Local Trivializations, and a Generative Approach to Exploring Many-to-one Maps&quot;. In: <em>International Conference on Learning Representations</em>. 2021.</p><p>[49] Bruno Gavranovi&#x107; et al. &quot;Position: Categorical Deep Learning is an Algebraic Theory of All Architectures&quot;. In: <em>Forty-first International Conference on Machine Learning</em>.</p><p>[50] Eugene P Wigner. &quot;The unreasonable effectiveness of mathematics in the natural sciences&quot;. In: <em>Mathematics and Science</em>. World Scientific, 1990, pp. 291&#x2013;306.</p><p>[51] Alon Halevy, Peter Norvig, and Fernando Pereira. &quot;The unreasonable effectiveness of data&quot;. In: <em>IEEE Intelligent Systems</em> 24.2 (2009), pp. 8&#x2013;12.</p>]]></content:encoded></item><item><title><![CDATA[What's Missing From LLM Chatbots: A Sense of Purpose]]></title><description><![CDATA[<p>LLM-based chatbots&#x2019; capabilities have been advancing every month. These improvements are mostly measured by benchmarks like MMLU, HumanEval, and MATH (e.g. sonnet 3.5, gpt-4o). However, as these measures get more and more saturated, is user experience increasing in proportion to these scores? If we envision a future</p>]]></description><link>https://thegradient.pub/dialog/</link><guid isPermaLink="false">66c6733993571d5c8c154fb1</guid><dc:creator><![CDATA[Kenneth Li]]></dc:creator><pubDate>Mon, 09 Sep 2024 17:28:48 GMT</pubDate><media:content url="https://thegradient.pub/content/images/2024/09/cropped.jpeg" medium="image"/><content:encoded><![CDATA[<img src="https://thegradient.pub/content/images/2024/09/cropped.jpeg" alt="What&apos;s Missing From LLM Chatbots: A Sense of Purpose"><p>LLM-based chatbots&#x2019; capabilities have been advancing every month. These improvements are mostly measured by benchmarks like MMLU, HumanEval, and MATH (e.g. sonnet 3.5, gpt-4o). However, as these measures get more and more saturated, is user experience increasing in proportion to these scores? If we envision a future of human-AI collaboration rather than AI replacing humans, the current ways of measuring dialogue systems may be insufficient because they measure in a non-interactive fashion.</p><p><strong>Why does purposeful dialogue matter?</strong></p><p>Purposeful dialogue refers to a multi-round user-chatbot conversation that centers around a goal or intention. The goal could range from a generic one like &#x201C;harmless and helpful&#x201D; to more specific roles like &#x201C;travel planning agent&#x201D;, &#x201C;psycho-therapist&#x201D; or &#x201C;customer service bot.&#x201D;<br></p><p>Travel planning is a simple, illustrative example. Our preferences, fellow travelers&#x2019; preference, and all the complexities of real-world situations make transmitting all information in one pass way too costly. However, if multiple back-and-forth exchanges of information are allowed, only important information gets selectively exchanged. Negotiation theory offers an analogy of this&#x2014;iterative bargaining yields better outcomes than a take-it-or-leave-it offer. <br></p><p>In fact, sharing information is only one aspect of dialogue. In Terry Winograd&#x2019;s words: &#x201C;All language use can be thought of as a way of activating procedures within the hearer.&#x201D; We can think of each utterance as a deliberate action that one party takes to alter the world model of the other. What if both parties have more complicated, even hidden goals? In this way, purposeful dialogue provides us with a way of formulating human-AI interactions as a collaborative game, where the goal of chatbot is to help humans achieve certain goals. <br></p><p>This might seem like an unnecessary complexity that is only a concern for academics. However, purposeful dialogue could be beneficial even for the most hard-nosed, product-oriented research direction like code generation. Existing coding benchmarks mostly measure performances in a one-pass generation setting; however, for AI to automate solving ordinary Github issues (like in <a href="https://www.swebench.com/">SWE-bench</a>), it&#x2019;s unlikely to be achieved by a single action&#x2014;the AI needs to communicate back and forth with human software engineers to make sure it understands the correct requirements, ask for missing documentation and data, and even ask humans to give it a hand if needed. In a similar vein to <a href="https://en.wikipedia.org/wiki/Pair_programming">pair programming</a>, this could reduce the defects of code but without the burden of increasing man-hours. </p><p><br>Moreover, with the introduction of turn-taking, many new possibilities can be unlocked. As interactions become long-term and memory is built, the chatbot can gradually update user profiles. It can also adapt to their preferences. Imagine a personal assistant (e.g., <a href="https://www.nvidia.com/en-us/ai-data-science/ai-workflows/intelligent-virtual-assistant/">IVA</a>, <a href="https://www.theverge.com/2024/6/10/24171936/apple-siri-ai-update-ios18-features-wwdc">Siri</a>) that, through daily interaction, learns your preferences and intentions. It can read your resources of new information automatically (e.g., twitter, arxiv, Slack, NYT) and provide you with a morning news summary according to your preferences. It can draft emails for you and keep improving by learning from your edits.</p><p>In a nutshell, meaningful interactions between people rarely begin with complete strangers and conclude in just one exchange. Humans naturally interact with each other through multi-round dialogues and adapt accordingly throughout the conversation. However, doesn&#x2019;t that seem exactly the opposite of predicting the next token, which is the cornerstone of modern LLMs? Below, let&#x2019;s take a look at the makings of dialogue systems.</p><p><strong>How were/are dialogue systems made?</strong></p><p>Let&apos;s jump back to the 1970s, when Roger Schank introduced his &quot;restaurant script&quot; as a kind of dialogue system [1]. This script breaks down the typical restaurant experience into steps like entering, ordering, eating, and paying, each with specific scripted utterances. Back then, every piece of dialogue in these scenarios was carefully planned out, enabling AI systems to mimic realistic conversations. ELIZA, a Rogerian psychotherapist simulator, and PARRY, a system mimicking a paranoid individual, were two other early dialogue systems until the dawn of machine learning.<br></p><p>Compare this approach to the LLM-based dialogue system today, it seems mysterious how models trained to predict the next token could do anything at all with engaging in dialogues. Therefore, let&#x2019;s take a close examination of how dialogue systems are made, with an emphasis on how the dialogue format comes into play:</p><p>(1) Pretraining: a sequence model is trained to predict the next token on a gigantic corpus of mixed internet texts. The compositions may vary but they are predominantly news, books, Github code, with a small blend of forum-crawled data such as from Reddit, Stack Exchange, which may contain dialogue-like data.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://thegradient.pub/content/images/2024/08/unnamed.png" class="kg-image" alt="What&apos;s Missing From LLM Chatbots: A Sense of Purpose" loading="lazy" width="512" height="288"><figcaption>Table of the pretraining data mixture from <a href="https://arxiv.org/pdf/2302.13971">llama technical report</a></figcaption></figure><p>(2) Introduce dialogue formatting: because the sequence model only processes strings, while the most natural representation of dialogue history is a structured index of system prompts and past exchanges, a certain kind of formatting must be introduced for the purpose of conversion. Some Huggingface tokenizers provide this method called <a href="https://huggingface.co/docs/transformers/main/en/chat_templating">tokenizer.apply_chat_template</a> for the convenience of users. The exact formatting differs from model to model, but it usually involves guarding the system prompts with &lt;system&gt; or &lt;INST&gt; in the hope that the pretrained model could allocate more attention weights to them. The system prompt plays a significant role in adapting language models to downstream applications and ensuring its safe behavior (we will talk more in the next section). Notably, the choice of the format is arbitrary at this step&#x2014;pretraining corpus doesn&#x2019;t follow this format.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://thegradient.pub/content/images/2024/08/image1.png" class="kg-image" alt="What&apos;s Missing From LLM Chatbots: A Sense of Purpose" loading="lazy" width="1398" height="112" srcset="https://thegradient.pub/content/images/size/w600/2024/08/image1.png 600w, https://thegradient.pub/content/images/size/w1000/2024/08/image1.png 1000w, https://thegradient.pub/content/images/2024/08/image1.png 1398w" sizes="(min-width: 720px) 720px"><figcaption>The context window of a chatbot</figcaption></figure><p>(3) RLHF: In this step, the chatbot is directly rewarded or penalized for generating desired or undesired answers. It&#x2019;s worth noting that this is the first time the introduced dialogue formatting appears in the training data. RLHF is a <em>fine</em>-tuning step not only because the data size is dwarfed in comparison to the pretraining corpus, but also due to the KL penalty and targeted weight tuning (e.g. Lora). Using Lecun&#x2019;s analogy of cake baking, RLHF is only the small cherry on the top.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://thegradient.pub/content/images/2024/08/image5.png" class="kg-image" alt="What&apos;s Missing From LLM Chatbots: A Sense of Purpose" loading="lazy" width="1440" height="580" srcset="https://thegradient.pub/content/images/size/w600/2024/08/image5.png 600w, https://thegradient.pub/content/images/size/w1000/2024/08/image5.png 1000w, https://thegradient.pub/content/images/2024/08/image5.png 1440w" sizes="(min-width: 720px) 720px"><figcaption>Image from Yann Lecun&#x2019;s slides</figcaption></figure><h2 id="how-consistent-are-existing-dialogue-systems-in-2024">How consistent are existing dialogue systems (in 2024)? <br></h2><p>The minimum requirement we could have for a dialogue system is that it can stay on the task we gave them. In fact, we humans often drift from topic to topic. How well do current systems perform? </p><p><br>Currently, &#x201C;system prompt&#x201D; is the main method that allows users to control LM behavior. However, researchers found evidence that LLMs can be brittle in following these instructions under adversarial conditions [12,13]. Readers might also have experienced this through daily interactions with ChatGPT or Claude&#x2014;when a new chat window is freshly opened, the model can follow your instruction reasonably well [2], but after several rounds of dialogue, it&#x2019;s no longer <em>fresh</em>, even stops following its role altogether.</p><p>How could we quantitatively capture this anecdote? For one-round instruction following, we&#x2019;ve already enjoyed plenty of benchmarks such as MT-Bench and Alpaca-Eval. However, when we test models in an interactive fashion, it&#x2019;s hard to anticipate what the model generates and prepare a reply in advance. In a project by my collaborators and me [3], we built an environment to synthesize dialogues with unlimited length to stress-test the instruction-following capabilities of LLM chatbots. <br></p><p>To allow an unconstrained scaling on the time scale, we let two system-prompted LM agents chat with each other for an extended number of rounds. This forms the main trunk of dialogue [a1, b1, a2, b2, &#x2026;, a8, b8] (say the dialogue is 8-round). At this point, we could probably figure out how the LLMs stick to its system prompts just by examining this dialogue, but many of the utterances can be irrelevant to the instructions, depending on where the conversation goes. Therefore, we hypothetically branch out at each round by asking a question directly related to the system prompts, and use a corresponding judging function to quantify how well it performs. All that&apos;s provided by the dataset is a bank of triplets of (system prompts, probe questions, and judging functions).</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://thegradient.pub/content/images/2024/08/image3.png" class="kg-image" alt="What&apos;s Missing From LLM Chatbots: A Sense of Purpose" loading="lazy" width="1999" height="570" srcset="https://thegradient.pub/content/images/size/w600/2024/08/image3.png 600w, https://thegradient.pub/content/images/size/w1000/2024/08/image3.png 1000w, https://thegradient.pub/content/images/size/w1600/2024/08/image3.png 1600w, https://thegradient.pub/content/images/2024/08/image3.png 1999w" sizes="(min-width: 720px) 720px"><figcaption>Sketch of the process of measuring instruction stability</figcaption></figure><p>Averaging across scenarios and pairs of system prompts, we get a curve of instruction stability across rounds. To our surprise, the aggregated results on both LLaMA2-chat-70B and gpt-3.5-turbo-16k are alarming. Besides the added difficulty to prompt engineering, the lack of instruction stability also comes with safety concerns. When the chatbot drifts away from its system prompts that stipulate safety aspects, it becomes more susceptible to jailbreaking and prone to more hallucinations.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://thegradient.pub/content/images/2024/08/Screenshot-2024-08-21-at-19.15.57.png" class="kg-image" alt="What&apos;s Missing From LLM Chatbots: A Sense of Purpose" loading="lazy" width="1736" height="689" srcset="https://thegradient.pub/content/images/size/w600/2024/08/Screenshot-2024-08-21-at-19.15.57.png 600w, https://thegradient.pub/content/images/size/w1000/2024/08/Screenshot-2024-08-21-at-19.15.57.png 1000w, https://thegradient.pub/content/images/size/w1600/2024/08/Screenshot-2024-08-21-at-19.15.57.png 1600w, https://thegradient.pub/content/images/2024/08/Screenshot-2024-08-21-at-19.15.57.png 1736w" sizes="(min-width: 720px) 720px"><figcaption>Instruction stability on LLaMA2-chat-70B and gpt-3.5-turbo-16k</figcaption></figure><p>The empirical results also contrast with the ever-increasing context length of LLMs. Theoretically, some long-context models can attend to a window of up to 100k tokens. However, in the dialogue setting, they become distracted after only 1.6k tokens (assuming each utterance is 100 tokens). In [3], we further theoretically showed how this is inevitable in a Transformer based LM chatbot under the current prompting scheme, and proposed a simple technique called split-softmax to mitigate such effects. </p><p>One might ask at this point, why is it so bad? Why don&apos;t humans lose their persona just by talking to another person for 8 rounds? It&#x2019;s arguable that human interactions are based on purposes and intentions [5] and these purposes precede the means rather than the opposite&#x2014;LLM is fundamentally a fluent English generator, and the persona is merely a thin added layer.</p><h2 id="what%E2%80%99s-missing">What&#x2019;s missing? <br></h2><p><strong>Pretraining?</strong><br>Pretraining endows the language model with the capability to model a distribution over internet personas as well as the lower-level language distribution of each persona [4]. However, even when one persona (or a mixture of a limited number of them) is specified by the instruction of system prompts, current approaches fail to single it out.<br></p><p><strong>RLHF?</strong><br>RLHF provides a powerful solution to adapting this multi-persona model to a &#x201C;helpful and harmless assistant.&#x201D; However, the original RLHF methods formulate reward maximization as a one-step bandit problem, and it is not generally possible to train with human feedback in the loop of conversation. (I&#x2019;m aware of many advances in alignment but I want to discuss the original RLHF algorithm as a prototypical example.) This lack of multi-turn planning may cause models to suffer from task ambiguity [6] and learning superficial human-likeness rather than goal-directed social interaction [7].<br></p><p>Will adding more dialogue data in RLHF help? My guess is that it will, to a certain extent, but it will still fall short due to a lack of purpose. Sergey Levine pointed out <a href="https://sergeylevine.substack.com/p/offline-rl-and-large-language-models">in his blog</a> that there is a fundamental difference between preference learning and intentions: &#x201C;the key distinction is between viewing language generation as selecting goal-directed actions in a sequential process, versus a problem of producing outputs satisfying user preferences.&#x201D;</p><h2 id="purposeful-dialogue-system">Purposeful dialogue system</h2><p>Staying on task is a modest request for LLMs. However, even if an LLM remains focused on the task, it doesn&apos;t necessarily mean it can excel in achieving the goal.</p><p>The problem of long-horizon planning has attracted some attention in the LLM community. For example, &#x201C;decision-oriented dialogue&#x201D; is proposed as a general class of tasks [8], where the AI assistant collaborates with humans to help them make complicated decisions, such as planning itineraries in a city and negotiating travel plans among friends. Another example, Sotopia [10], is a comprehensive social simulation platform that compiles various goal-driven dialogue scenarios including collaboration, negotiation, and persuasion. </p><p>Setting up such benchmarks provides not only a way to gauge the progress of the field, it also directly provides reward signals that new algorithms could pursue, which could be expensive to collect and tricky to define [9]. However, there aren&#x2019;t many techniques that can exert control over the LM so that it can act consistently across a long horizon towards such goals. </p><p>To fill in this gap, my collaborators and I propose a lightweight algorithm (Dialogue Action Tokens, DAT [11]) that guides an LM chatbot through a multi-round goal-driven dialogue. As shown in the image below, in each round of conversations, the dialogue history&#x2019;s last token embedding is used as the input (state) to a planner (actor) which predicts several prefix tokens (actions) to control the generation process. By training the planner with a relatively stable RL algorithm TD3+BC, we show significant improvement over baselines on Sotopia, even surpassing the social capability scores of GPT-4.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://thegradient.pub/content/images/2024/08/image2.png" class="kg-image" alt="What&apos;s Missing From LLM Chatbots: A Sense of Purpose" loading="lazy" width="1191" height="176" srcset="https://thegradient.pub/content/images/size/w600/2024/08/image2.png 600w, https://thegradient.pub/content/images/size/w1000/2024/08/image2.png 1000w, https://thegradient.pub/content/images/2024/08/image2.png 1191w" sizes="(min-width: 720px) 720px"><figcaption>A sketch of &#x200B;&#x200B;Dialogue Action Tokens (DAT)</figcaption></figure><p><br>In this way, we provide a technique pathway that upgrades LM from a prediction model that merely guesses the next token to one that engages in dialogue with humans purposefully. We could imagine that this technique can be misused for harmful applications as well. For this reason, we also conduct a &#x201C;multi-round red-teaming&#x201D; experiment, and recommend that more research could be done here to better understand multi-round dialogue as potential attack surface.</p><h2 id="concluding-marks">Concluding marks<br></h2><p>I have reviewed the making of current LLM dialogue systems, how and why it is insufficient. I hypothesize that a purpose is what is missing and present one technique to add it back with reinforcement learning. </p><p>The following are two research questions that I&#x2019;m mostly excited about: </p><p>(1) Better monitoring and control of dialogue systems with steering techniques. For example, the recently proposed TalkTurner (Chen et al.) adds a dashboard (Vie&#x301;gas et al) to open-sourced LLMs, enabling users to see and control how LLM thinks of themselves. Many weaknesses of current steering techniques are revealed and call for better solutions. For example, using activation steering to control two attributes (e.g., age and education level) simultaneously has been found to be difficult and can cause more language degradation. Another intriguing question is how to differentiate between LLM&#x2019;s internal model of itself and that of the user. Anecdotally, chatting with <a href="https://www.anthropic.com/news/golden-gate-claude">Golden Gate Bridge Claude</a> has shown that steering on the specific Golden Gate Bridge feature found by SAE sometimes causes Claude to think of itself as the San Francisco landmark, sometimes the users as the bridge, and other times the topic as such.<br></p><p>(2) Better utilization of off-line reward signals. In the case of set-up environments like Sotopia and &#x201C;decision-oriented dialogues&#x201D;, rewards signals are engineered beforehand. In the real world, users won&#x2019;t leave numerical feedback of how they feel satisfied. However, there might be other clues in language (e.g., &#x201C;Thanks!&#x201D;, &#x201C;That&#x2019;s very helpful!&#x201D;) or from external resources (e.g., users buying the product for a salesman AI, users move to a subsequent coding question for copilot within a short time frame). Inferring and utilizing such hidden reward signals could strengthen the <a href="https://en.wikipedia.org/wiki/Network_effect">network effect</a> of online chatbots: good model &#x2192; more users &#x2192; learning from interacting with users &#x2192; better model.</p><p><strong>Acknowledgment</strong><br>The author is grateful to Martin Wattenberg and Hugh Zhang (alphabetical order) for providing suggestions and editing the text.</p><p><strong>Citation</strong></p><p>For attribution of this in academic contexts or books, please cite this work as:</p><blockquote><em>Kenneth Li, &quot;</em><strong>What&apos;s Missing From LLM Chatbots: A Sense of Purpose</strong><em>&quot;, The Gradient, 2024.</em></blockquote><p>BibTeX citation (this blog):</p><div class="kg-card kg-callout-card kg-callout-card-grey"><div class="kg-callout-emoji">&#x1F4A1;</div><div class="kg-callout-text">@article{li2024from,<br>author = {Li, Kenneth},<br>title = {What&apos;s Missing From LLM Chatbots: A Sense of Purpose},<br>journal = {The Gradient},<br>year = {2024},<br>howpublished = {\url{https://thegradient.pub/dialogue}},<br>}</div></div><p><strong>References</strong></p><p>[1] Schank, Roger C., and Robert P. Abelson. Scripts, plans, goals, and understanding: An inquiry into human knowledge structures. Psychology press, 2013.<br>[2] Zhou, Jeffrey, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny Zhou, and Le Hou. &quot;Instruction-following evaluation for large language models.&quot; arXiv preprint arXiv:2311.07911 (2023).<br>[3] &#x200B;&#x200B;Li, Kenneth, Tianle Liu, Naomi Bashkansky, David Bau, Fernanda Vi&#xE9;gas, Hanspeter Pfister, and Martin Wattenberg. &quot;Measuring and controlling persona drift in language model dialogs.&quot; arXiv preprint arXiv:2402.10962 (2024).<br>[4] Andreas, Jacob. &quot;Language models as agent models.&quot; arXiv preprint arXiv:2212.01681 (2022).<br>[5] Austin, John Langshaw. How to do things with words. Harvard university press, 1975.<br>[6] Tamkin, Alex, Kunal Handa, Avash Shrestha, and Noah Goodman. &quot;Task ambiguity in humans and language models.&quot; arXiv preprint arXiv:2212.10711 (2022).<br>[7] Bianchi, Federico, Patrick John Chia, Mert Yuksekgonul, Jacopo Tagliabue, Dan Jurafsky, and James Zou. &quot;How well can llms negotiate? negotiationarena platform and analysis.&quot; arXiv preprint arXiv:2402.05863 (2024).<br>[8] Lin, Jessy, Nicholas Tomlin, Jacob Andreas, and Jason Eisner. &quot;Decision-oriented dialogue for human-ai collaboration.&quot; arXiv preprint arXiv:2305.20076 (2023).<br>[9] Kwon, Minae, Sang Michael Xie, Kalesha Bullard, and Dorsa Sadigh. &quot;Reward design with language models.&quot; arXiv preprint arXiv:2303.00001 (2023).<br>[10] Zhou, Xuhui, Hao Zhu, Leena Mathur, Ruohong Zhang, Haofei Yu, Zhengyang Qi, Louis-Philippe Morency et al. &quot;Sotopia: Interactive evaluation for social intelligence in language agents.&quot; arXiv preprint arXiv:2310.11667 (2023).<br>[11] Li, Kenneth, Yiming Wang, Fernanda Vi&#xE9;gas, and Martin Wattenberg. &quot;Dialogue Action Tokens: Steering Language Models in Goal-Directed Dialogue with a Multi-Turn Planner.&quot; arXiv preprint arXiv:2406.11978 (2024).<br>[12] Li, Shiyang, Jun Yan, Hai Wang, Zheng Tang, Xiang Ren, Vijay Srinivasan, and Hongxia Jin. &quot;Instruction-following evaluation through verbalizer manipulation.&quot; arXiv preprint arXiv:2307.10558 (2023).<br>[13] Wu, Zhaofeng, Linlu Qiu, Alexis Ross, Ekin Aky&#xFC;rek, Boyuan Chen, Bailin Wang, Najoung Kim, Jacob Andreas, and Yoon Kim. &quot;Reasoning or reciting? exploring the capabilities and limitations of language models through counterfactual tasks.&quot; arXiv preprint arXiv:2307.02477 (2023).</p>]]></content:encoded></item><item><title><![CDATA[We Need Positive Visions for AI Grounded in Wellbeing]]></title><description><![CDATA[<h2 id="introduction">Introduction</h2><p>Imagine yourself a decade ago, jumping directly into the present shock of conversing naturally with an encyclopedic AI that crafts images, writes code, and debates philosophy. Won&#x2019;t this technology almost certainly transform society &#x2014; and hasn&#x2019;t AI&#x2019;s impact on us so far been</p>]]></description><link>https://thegradient.pub/we-need-positive-visions-for-ai-grounded-in-wellbeing/</link><guid isPermaLink="false">66a4243393571d5c8c154f4e</guid><category><![CDATA[Perspectives]]></category><category><![CDATA[Ethics]]></category><category><![CDATA[Impacts]]></category><dc:creator><![CDATA[Joel Lehman]]></dc:creator><pubDate>Sat, 03 Aug 2024 17:00:43 GMT</pubDate><media:content url="https://thegradient.pub/content/images/2024/07/wellbeing_ai_cover_image.webp" medium="image"/><content:encoded><![CDATA[<h2 id="introduction">Introduction</h2><img src="https://thegradient.pub/content/images/2024/07/wellbeing_ai_cover_image.webp" alt="We Need Positive Visions for AI Grounded in Wellbeing"><p>Imagine yourself a decade ago, jumping directly into the present shock of conversing naturally with an encyclopedic AI that crafts images, writes code, and debates philosophy. Won&#x2019;t this technology almost certainly transform society &#x2014; and hasn&#x2019;t AI&#x2019;s impact on us so far been a mixed-bag? Thus it&#x2019;s no surprise that so many conversations these days circle around an era-defining question: <em>How do we ensure AI benefits humanity?</em> These conversations often devolve into strident optimism or pessimism about AI, and our earnest aim is to walk a pragmatic middle path, though no doubt we will not perfectly succeed.<br></p><p>While it&#x2019;s fashionable to handwave towards &#x201C;beneficial AI,&#x201D; and many of us want to contribute towards its development &#x2014; it&#x2019;s not easy to pin down what beneficial AI concretely means in practice. This essay represents our attempt to demystify beneficial AI, through grounding it in the wellbeing of individuals and the health of society. In doing so, we hope to promote opportunities for AI research and products to benefit our flourishing, and along the way to share ways of thinking about AI&#x2019;s coming impact that motivate our conclusions.</p><h3 id="the-big-picture">The Big Picture</h3><p>By trade, we&#x2019;re closer in background to AI than to the fields where human flourishing is most-discussed, such as <a href="https://en.wikipedia.org/wiki/Happiness_economics">wellbeing economics</a>, <a href="https://en.wikipedia.org/wiki/Positive_psychology">positive psychology</a>, or <a href="https://plato.stanford.edu/entries/well-being/">philosophy</a>, and in our journey to find productive connections between such fields and the technical world of AI, we found ourselves often confused (what even is human flourishing, or wellbeing, anyways?) and from that confusion, often stuck (maybe there is nothing to be done? &#x2014; the problem is too multifarious and diffuse). We imagine that others aiming to create prosocial technology might share our experience, and the hope here is to shine a partial path through the confusion to a place where there&#x2019;s much interesting and useful work to be done. We start with some of our main conclusions, and then dive into more detail in what follows.<br></p><p>One conclusion we came to is that <strong>it&#x2019;s okay that we can&#x2019;t conclusively define human wellbeing.</strong> It&#x2019;s been debated by philosophers, economists, psychotherapists, psychologists, and religious thinkers, for many years, and there&#x2019;s no consensus. At the same time, there&#x2019;s agreement around many concrete factors that make our lives go well, like: supportive intimate relationships, meaningful and engaging work, a sense of growth and achievement, and positive emotional experiences. And there&#x2019;s clear understanding, too, that beyond momentary wellbeing, we must consider how to secure and improve wellbeing across years and decades &#x2014; through what we could call <em>societal</em> <em>infrastructure</em>: important institutions such as education, government, the market, and academia. <br></p><p>One benefit of this wellbeing lens is to wake us to an almost-paradoxical fact: While the deep purpose behind nearly everything our species does is wellbeing, we&#x2019;ve tragically lost sight of it. &#xA0;Both by common measures of individual wellbeing (suicide rate, loneliness, meaningful work) and societal wellbeing (trust in our institutions, shared sense of reality, political divisiveness), we&#x2019;re not doing well, and our impression is that AI is complicit in that decline. The central benefit of this wellbeing view, however, is the insight that no fundamental obstacle prevents us from synthesizing the science of wellbeing with machine learning to our collective benefit. <br></p><p>This leads to our second conclusion: <strong>We need plausible positive visions of a society with capable AI, grounded in wellbeing.</strong> Like other previous transformative technologies, AI will shock our societal infrastructure &#x2014; dramatically altering the character of our daily lives, whether we want it to or not. For example, Facebook launched only twenty years ago, and yet social media&#x2019;s shockwaves have already upended much in society &#x2014; subverting news media and our informational commons, addicting us to likes, and displacing meaningful human connection with its shell. We believe capable AI&#x2019;s impact will exceed that of social media. As a result, it&#x2019;s vital that we strive to explore, envision, and move towards the AI-infused worlds we&#x2019;d flourish within &#x2014; ones perhaps in which it revitalizes our institutions, empowers us to pursue what we find most meaningful, and helps us cultivate our relationships. This is no simple task, requiring imagination, groundedness, and technical plausibility &#x2014; to somehow dance through <a href="https://thefrailestthing.com/2018/03/11/why-we-cant-have-humane-technology/">the minefields illuminated by previous critiques of technology</a>. Yet now is the time to dream and build if we want to actively shape what is to come.<br></p><p>This segues into our final conclusion: <strong>Foundation models and the arc of their future deployment is critical.</strong> Even for those of us in the thick of the field, it&#x2019;s hard to internalize how quickly models have improved, and how capable they might become given several more years. Recall that GPT-2 &#x2014; barely functional by today&#x2019;s standards &#x2014; was released <em>only in 2019</em>. If future models are much more capable than today&#x2019;s, and competently engage with more of the world with greater autonomy, we can expect their entanglement with our lives and society to rachet skywards. So, at minimum, we&#x2019;d like to enable these models to understand our wellbeing and how to support it, potentially through new algorithms, wellbeing-based evaluations of models and wellbeing training data. Of course, we also want to realize human benefit in practice &#x2014; the last section of this blog post highlights what we believe are strong leverage points towards that end.<br></p><p>The rest of this post describes in more detail (1) <a href="#beneficial-ai-grounds-out-in-human-wellbeing">what we mean by AI that benefits our wellbeing</a>, (2) <a href="#we-need-positive-visions-for-ai">the need for positive visions for AI grounded in wellbeing</a>, and (3) <a href="#so-what-can-we-do">concrete leverage points to aid in the development and deployment of AI in service of such positive visions</a>. We&#x2019;ve designed this essay such that the individual parts are mostly independent, so if you are interested most in concrete research directions, <a href="#so-what-can-we-do">feel free to skip there</a>.</p><h3 id="beneficial-ai-grounds-out-in-human-wellbeing">Beneficial AI grounds out in human wellbeing</h3><p>Discussion about AI for human benefit is often high-minded, but not particularly actionable, as in unarguable but content-free phrases like &#x201C;We should make sure AI is in service of humanity.&#x201D; But to meaningfully implement such ideas in AI or policy requires enough precision and clarity to translate them into code or law. So we set out to survey what science has discovered about the ground of human benefit, as a step towards being able to measure and support it through AI.<br></p><p>Often, when we think about beneficial impact, we focus on abstract pillars like democracy, education, fairness, or the economy. However important, none of these are valuable <em>intrinsically.</em> We care about them because of how they affect our collective lived experience, over the short and long-term. We care about increasing society&#x2019;s GDP to the extent it aligns with actual improvement of our lives and future, but when treated as an end in itself, it becomes disconnected from what matters: improving human (and potentially all species&#x2019;) experience.<br></p><p>In looking for fields that most directly study the root of human flourishing, we found the scientific literature on wellbeing. The literature is vast, spanning many disciplines, each with their own abstractions and theories &#x2014; and, as you might expect, there&#x2019;s no true consensus on what <a href="https://plato.stanford.edu/entries/well-being/">wellbeing</a> actually is. In diving into the philosophy of flourishing, wellbeing economics, or psychological theories of human wellbeing, one encounters many interesting, compelling, but seemingly incompatible ideas. <br></p><p>For example, theories of <a href="https://plato.stanford.edu/entries/hedonism/">hedonism</a> in philosophy claim that pleasure and the absence of suffering is the core of wellbeing; while <a href="https://plato.stanford.edu/entries/desire/#WelBeiDes">desire satisfaction</a> theories instead claim that wellbeing is about the fulfillment of our desires, no matter how we feel emotionally. There&#x2019;s a wealth of literature on measuring <a href="https://en.wikipedia.org/wiki/Subjective_well-being">subjective wellbeing</a> (broadly, how we experience and feel about our life), and many different frameworks of what variables characterize flourishing. For example, Martin Seligman&#x2019;s <a href="https://en.wikipedia.org/wiki/Martin_Seligman#Well-being">PERMA framework</a> claims that wellbeing consists of positive emotions, engagement, relationships, meaning, and achievement. There are theories that say that the core of wellbeing is <a href="https://en.wikipedia.org/wiki/Self-determination_theory">satisfying psychological needs</a>, like the need for autonomy, competence, and relatedness. Other theories claim that wellbeing comes from <a href="https://plato.stanford.edu/entries/ethics-virtue/">living by our values</a>. In economics, frameworks rhyme with those in philosophy and psychology, but diverge enough to complicate an exact bridge. For example, the <a href="https://en.wikipedia.org/wiki/Wellbeing_economy">wellbeing economics</a> movement largely focuses on subjective wellbeing and explores many different proxies of it, like income, quality of relationships, job stability, etc.<br></p><p>After the excitement from surveying so many interesting ideas began to fade, perhaps unsurprisingly, we remained fundamentally confused about what &#x201C;the right theory&#x201D; was. But, we recognized that in fact <em>this has always been the human situation when it comes to wellbeing</em>, and just as a lack of an incontrovertible theory of flourishing has not prevented humanity from flourishing in the past, it need not stand as a fundamental obstacle for beneficial AI. In other words, our attempts to guide AI to support human flourishing must take this lack of certainty seriously, just as all sophisticated societal efforts to support flourishing must do.<br></p><p>In the end, we came to a simple workable understanding, not far from the view of wellbeing economics: Human benefit ultimately must ground out in the <em>lived experience of humans</em>. We want to live happy, meaningful, healthy, full lives &#x2014; and it&#x2019;s not so difficult to imagine ways AI might assist in that aim. For example, the development of low-cost but proficient AI coaches, intelligent journals that help us to self-reflect, or apps that help us to find friends, romantic partners, or to connect with loved ones. We can ground these efforts in imperfect but workable measures of wellbeing from the literature (e.g. PERMA), taking as <em>first-class concerns</em> that the <a href="https://fs.blog/map-and-territory/">map (wellbeing measurement) is not the territory (actual wellbeing)</a>, and that humanity itself continues to explore and refine its vision of wellbeing.<br></p><p>More broadly our wellbeing relies on a healthy society, and we care not only about our own lives, but also want beautiful lives for our neighbors, community, country, and world, and for our children, and their children as well. The infrastructure of society (institutions like government, art, science, military, education, news, and markets) is what supports this broader, longer-term vision of wellbeing.</p><figure class="kg-card kg-image-card"><img src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXe4f50PRkH1ZAWhltK8VAz1IhmtVLJU6k3pf1oGL-GmNTXM2QsnRJU52h0d8uCYPoFa_r6QB6UTtFThWPr6anV42FbBZPsnj1PXPDtp4Ofu5JjECD5CJz0W1asFNrFqvyL-PBxqYCk1VBxShyYTKoPj_HI?key=_z5hHgxLrjtdLauVz_eYpw" class="kg-image" alt="We Need Positive Visions for AI Grounded in Wellbeing" loading="lazy" width="379" height="362"></figure><p>Each of these institutions have important roles to play in society, and we can also imagine ways that AI could support or improve them; for example, generative AI may catalyze education through personal tutors that help us develop a richer worldview, may help us to better hold our politicians to account through sifting through what they are actually up to, or accelerate meaningful science through helping researchers make novel connections. Thus in short, <em>beneficial AI would meaningfully support our quest for lives worth living, in both the immediate and long-term sense.</em></p><p>So, from the lofty confusion of conflicting grand theories, we arrive at something sounding more like common sense. Let&#x2019;s not take this for granted, however &#x2014; it cuts through the cruft of abstractions to firmly recenter what is ultimately important: the psychological experience of humans. This view points us towards the ingredients of wellbeing that are both well-supported scientifically and could be made measurable and actionable through AI (e.g. there <a href="http://www.michaelfsteger.com/wp-content/uploads/2012/08/Steger-Frazier-Oishi-Kaler-JCP-2006.pdf">exist</a> <a href="https://web.archive.org/web/20151216195710id_/http://scottbarrykaufman.com:80/wp-content/uploads/2015/11/2-Happiness-is-everything-or-is-it.pdf">instruments</a> to measure many of these ingredients). Further, wellbeing across the short and long-term provides the common currency that bridges divergent approaches to beneficial AI, whether mitigating societal harms like discrimination in the <a href="https://facctconference.org/">AI ethics community</a>, to attempting to <a href="https://pol.is/home">reinvigorate democracy through AI-driven deliberation</a>, to creating a world where <a href="https://www.meaningalignment.org/">humans live more meaningful lives</a>, to creating <a href="https://www.thyself.ai/">low-cost emotional support and self-growth tools</a>, to <a href="https://en.wikipedia.org/wiki/AI_safety">reducing the likelihood of existential risks from AI</a>, to using <a href="https://www.ai4institutions.com/">AI to reinvigorate our institutions</a> &#x2014; wellbeing is the ultimate ground.<br></p><p>Finally, focusing on wellbeing helps to highlight where we currently fall short. Current AI development is driven by our existing incentive systems: Profit, research novelty, engagement, with little explicit focus on what fundamentally is more important (human flourishing). We need to find tractable ways to shift incentives towards wellbeing-supportive models (something we&#x2019;ll discuss later), and positive directions to move toward (discussed next).</p><h3 id="we-need-positive-visions-for-ai">We need positive visions for AI</h3><p>Technology is a shockingly powerful societal force. While nearly all new technologies bring only limited change, like an improved toothbrush, sometimes they upend the world. Like the proverbial slowly-boiling frog, we forget how in short order the internet and cellphones have <em>overhauled</em> our lived experience: the rise of dating apps, podcasts, social networks, our constant messaging, cross-continental video calls, massive online games, the rise of influencers, on-demand limitless entertainment, etc. Our lives as a whole &#x2014; our relationships, our leisure, how we work and collaborate, how news and politics work &#x2014; <em>have dramatically shifted</em>, for both the better and worse.<br></p><p>AI is transformative, and the mixed bag of its impacts are poised to reshape society in mundane and profound ways; we might doubt it, but that was also our naivety at the advent of social media and the cell-phone. We don&#x2019;t see it coming, and once it&#x2019;s here we take it for granted. Generative AI translates applications from science fiction into rapid adoption: <a href="https://www.newyorker.com/culture/infinite-scroll/your-ai-companion-will-support-you-no-matter-what">AI romantic companions</a>; <a href="https://www.gartner.com/en/newsroom/press-releases/2024-04-11-gartner-says-75-percent-of-enterprise-software-engineers-will-use-ai-code-assistants-by-2028">automated writing and coding assistants</a>; <a href="https://www.nbcnews.com/tech/tech-news/ai-image-misinformation-surged-google-research-finds-rcna154333">automatic generation of high-quality images</a>, <a href="https://time.com/6340294/ai-transform-music-2023/">music</a>, and <a href="https://apnews.com/article/ai-video-generators-runway-sora-madonna-4e21021b9db8a45a9897d9285f394687">videos</a>; <a href="https://www.khanmigo.ai/">low-cost personalized AI tutors</a>; <a href="https://www.nature.com/articles/s41598-024-53755-0">highly-persuasive personalized ads</a>; and so on. <br></p><p>In this way, transformative impact is happening <em>now</em> &#x2014; it does not require AI with <a href="https://thegradient.pub/why-transformative-artificial-intelligence-is-really-really-hard-to-achieve/">superhuman intelligence</a> &#x2014; see the rise of LLM-based social media bots; ChatGPT as the fastest-adopted consumer app; LLMs requiring fundamental changes to homework in school. Much greater impact will yet come, as the technology (and the business around it) matures, and as AI is integrated more pervasively throughout society.<br></p><p>Our institutions were understandably not designed with <a href="https://www.the-coming-wave.com/">this latest wave</a> of AI in mind, and it&#x2019;s unclear that many of them will adapt quickly enough to keep up with AI&apos;s rapid deployment. For example, an important function of news is to keep a democracy&#x2019;s citizens well-informed, so their vote is meaningful. But news these days spreads through AI-driven algorithms on social media, which amplifies <a href="https://jonahberger.com/wp-content/uploads/2013/02/ViralityB.pdf">emotional virality</a> and <a href="https://www.goodreads.com/book/show/10596103-the-filter-bubble">confirmation bias</a> at the expense of meaningful debate. And so, the public square and the sense of a shared reality is being undercut, as AI degrades an important institution devised without foresight of this novel technological development.<br></p><p>Thus in practice, it may not be possible to play defense by simply &#x201C;mitigating harms&#x201D; from a technology; often, a new technology demands that we creatively and skillfully apply our existing values to a radically new situation. We don&#x2019;t want AI to, for example, undermine the livelihood of artists, yet how <em>do</em> we want our relationship to creativity to look like in a world where AI can, easily and cheaply, produce compelling art or write symphonies and novels, in the style of your favorite artist? There&#x2019;s no easy answer. We need to debate, understand, and capture what we believe is the <em>spirit </em>of our institutions and systems given this new technology. <br></p><p>For example, what&#x2019;s truly important about education? We can reduce harms that AI imposes on the current education paradigm by banning use of AI in students&#x2019; essays, or apply AI in service of existing metrics (for example, to increase high school graduation rates). But the paradigm itself must adapt: The world that schooling currently prepares our children for is not the world they&#x2019;ll graduate into, nor does it prepare us generally to flourish and find meaning in our lives. We must ask ourselves what we really value in education that we want AI to enable: Perhaps teaching critical thinking, enabling agency, and creating a sense of social belonging and civic responsibility?<br></p><p>To anticipate critique, we agree that there will be no global consensus on what education is for, or on the underlying essence of any particular institution, at root because different communities and societies have &#xA0;distinct values and visions. But that&#x2019;s okay: Let&#x2019;s empower communities to fit AI systems to local societal contexts; for example, algorithms like constitutional AI enable <a href="https://cip.org/blog/ccai">creating different constitutions that embody flourishing for different communities</a>. This kind of cheap flexibility is an exciting affordance, meaning we no longer must sacrifice nuance and context-sensitivity for scalability and efficiency, a bitter pill technology often pushes us to swallow.<br></p><p>And while of course we have always wanted education to create critical thinkers, our past metrics (like standardized tests) have been so coarse that scoring high is easily gamed without critical thinking. But generative AI enables new affordances here, too: just as a teacher can socratically question a student to evaluate their independent thought, advances in generative AI open up the door for similarly qualitative and interactive measures, like personalized AI tutors that meaningfully gauge critical thinking.<br></p><p>We hope to tow a delicate line beyond broken dichotomies, whether between naive optimism and pessimism, or idealism and cynicism. Change is coming, and we must channel it towards refined visions of what we want, which is a profound opportunity, rather than to assume that by default technology will deliver us (or doom us), or that we will be able to wholly resist the transformation it brings (or are entirely helpless against it). For example, we must temper naive optimism (&#x201C;AI will save the world if only we deploy it everywhere!&#x201D;) by integrating lessons from <a href="https://en.wikipedia.org/wiki/Science_and_technology_studies">the long line of work</a> that studies the social drivers and consequences of technology, often from a critical angle. But neither should cynical concerns so paralyze us that we remain only as critics on the sidelines.</p><h2 id="so-what-can-we-do">So, what can we do?</h2><p>The case so far is that we need positive visions for society with capable AI, grounded in individual and societal wellbeing. But what concrete work can actually support this? We propose the following break-down:</p><ol><li><a href="#we-need-to-understand-where-we-want-to-go-in-the-age-of-al">Understanding where we want to go</a></li><li><a href="#we-need-to-develop-measures-for-how-ai-affects-wellbeing">Measuring how AI impacts our wellbeing</a></li><li><a href="#we-need-to-train-models-to-improve-their-ability-to-support-wellbeing">Training models that can support wellbeing</a></li><li><a href="#we-need-to-deploy-ai-models-in-a-way-that-supports-wellbeing">Deploying models in service of wellbeing</a></li></ol><p>The overall idea is to support an ongoing, iterative process of exploring the positive directions we want to go and deploying and adapting models in service of them.</p><h3 id="we-need-to-understand-where-we-want-to-go-in-the-age-of-ai">We need to understand where we want to go in the age of AI</h3><p>This point follows closely from the need to explore the positive futures we want with AI. What directions of work and research can help us to clarify where is possible to go, and is worth going to, in the age of AI?<br></p><p>For starters, it&#x2019;s more important now than ever to have productive and grounded discussions about questions like: What makes us human? How do we want to live? What do we want the future to feel like? What values are important to us? What do we want to retain as AI transformations sweep through society? Rather than being centered on the machine learning community, this should be an interdisciplinary, international effort, spanning psychology, philosophy, political science, art, economics, sociology, and neuroscience (and many other fields!), and bridging diverse intra- and international cultures. <br></p><p>Of course, it&#x2019;s easy to call for such a dialogue, but the real question is how such interdisciplinary discussions can be convened in a meaningful, grounded, and action-guiding way &#x2014; rather than leading only to cross-field squabbles or agreeable but vacuous aspirations. Perhaps through participatory design that pairs citizens with disciplinary experts to explore these questions, with machine learning experts mainly serving to ground technological plausibility. Perhaps AI itself could be of service: For example, research in <a href="https://cip.org/">AI-driven deliberative democracy</a> and <a href="https://www.plurality.net/">plurality</a> may help involve more people in navigating these questions; as might research into <a href="https://www.meaningalignment.org/">meaning alignment</a>, by helping us describe and aggregate what is meaningful and worth preserving to us. It&#x2019;s important here to look beyond cynicism or idealism (<a href="https://www.amazon.com/Nordic-Ideology-Metamodern-Politics-Guides/dp/8799973928">suggestive of meta-modern political philosophy</a>): Yes, mapping exciting positive futures is not a cure-all, as there are powerful societal forces, like regulatory capture, institutional momentum, and the profit motive, that resist their realization, and yet, societal movements all have to start somewhere, and <em>some really do succeed</em>.<br></p><p>Beyond visions for big-picture questions about the future, much work is needed to understand where we want to go in narrower contexts. For example, while it might at first seem trivial, how can we reimagine online dating with capable AI, given that healthy romantic partnership is such an important individual and societal good? Almost certainly, we will look back at swipe-based apps as misguided means for finding long-term partners. And many of our institutions, small and large, can be re-visioned in this way, from tutoring to academic journals to local newspapers. AI will make possible a much richer set of design possibilities, and we can work to identify which of those possibilities are workable and well-represent the desired essence of an institution&#x2019;s role in our lives and society.<br></p><p>Finally, continued basic and applied research into the factors that contribute and characterize human wellbeing and societal health also are highly important, as these are what ultimately ground our visions. And as the next section explores, having better measures of such factors can help us to change incentives and work towards our desired futures.</p><h3 id="we-need-to-develop-measures-for-how-ai-affects-wellbeing">We need to develop measures for how AI affects wellbeing</h3><p>For better and worse, we often navigate through what we measure. We&#x2019;ve seen this play out before: Measure GDP, and nations orient towards increasing it at great expense. Measure clicks and engagement, and we develop platforms that are terrifyingly adept at keeping people hooked. A natural question is, what prevents us from similarly measuring aspects of wellbeing to guide our development and deployment of AI? And if we do develop wellbeing measures, can we avoid the pitfalls that have derailed other well-intended measures, like GDP or engagement?<br></p><p>One central problem for measurement is that wellbeing is more complex and qualitative than GDP or engagement. Time-on-site is a very straightforwardmeasure of engagement. In contrast, properties relevant to wellbeing, like the felt sense of meaning or the quality of healthy relationships, are difficult to pin down quantitatively, especially from the limited viewpoint of how a user interacts with a particular app. <br></p><p>Wellbeing depends on the broader context of a user&#x2019;s life in messy ways, meaning it&#x2019;s harder to isolate how any small intervention impacts it. And so, wellbeing measures are more expensive and less standardized to apply, end up less measured, and less guide our development of technology. However, foundation models are beginning to have the exciting ability to work with qualitative aspects of wellbeing. For example, present-day language models can (with caveats) infer emotions from user messages and detect conflict; or conduct qualitative interviews with users about its impact on their experience. <br></p><p>So one promising direction of research, though not easy, is to explore how foundation models themselves can be applied to more reliably measure facets of individual and societal wellbeing, and ideally, help to identify how AI products and services are impacting that wellbeing. The mechanisms of impact are two-fold: One, companies may currently lack means of measuring wellbeing even though all-things-equal they want their products to help humans; two, where the profit motive conflicts with encouraging wellbeing, if a product&#x2019;s impact can be externally audited and published, it can help hold the company to account by consumers and regulators, shifting corporate &#xA0;incentives towards societal good.<br></p><p>Another powerful way that wellbeing-related measures can have impact is as evaluation benchmarks for foundation models. In machine learning, evaluations are a powerful lever for channeling research effort through competitive pressure. For example, model providers and academics continuously develop new models that perform better and better on benchmarks like <a href="https://arxiv.org/abs/2109.07958">TruthfulQA</a>. Once you have legible outcomes, you often spur innovation to improve upon them. We currently have very few benchmarks focused on how AI affects our wellbeing, or how well they can understand our emotions, make wise decisions, or respect our autonomy: We need to develop these benchmarks.<br></p><p>Finally, as mentioned briefly above, metrics can also create accountability and enable regulations. Recent efforts like the <a href="https://crfm.stanford.edu/fmti/">Stanford Foundational Model Transparency Index</a> have created public accountability for AI labs, and initiatives like <a href="https://evals.alignment.org/blog/2023-09-26-rsp/">Responsible Scaling Policies</a> are premised on evaluations of model capabilities, as are evaluations by government bodies such as <a href="https://www.aisi.gov.uk/work/advanced-ai-evaluations-may-update">AI safety institutes in both the UK and US</a>. Are there similar metrics and initiatives to encourage accountability around AI&#x2019;s impact on wellbeing?<br></p><p>To anticipate a natural concern, unanticipated side-effects are nearly universal when attempting to improve important <em>qualities</em> through <em>quantitative</em> measures. What if in measuring wellbeing, the second-order consequence is perversely to undermine it? For example, if a wellbeing measure doesn&#x2019;t include notions of autonomy, in optimizing it we might create paternalistic AI systems that &#x201C;make us happy&#x201D; by decreasing our agency. There are book-length treatments on the <a href="https://yalebooks.yale.edu/book/9780300078152/seeing-like-a-state/">failures of high modernism</a> and (from one of the authors of this essay!) on the <a href="https://link.springer.com/book/10.1007/978-3-319-15524-1">tyranny of measures and objectives</a>, and many academic papers on how optimization can <a href="https://proceedings.neurips.cc/paper_files/paper/2022/file/3d719fee332caa23d5038b8a90e81796-Paper-Conference.pdf">pervert measures</a> or <a href="https://philpapers.org/rec/NGUVCH">undermine our autonomy</a>. <br></p><p>The trick is to look beyond binaries. Yes, measures and evaluations have serious problems, yet we can work with them with wisdom, taking seriously previous failures and institutionalizing that all measures are imperfect. We want a diversity of metrics (<a href="https://philpapers.org/rec/NGUVCH">metric federalism</a>) and a diversity of AI models rather than a monoculture, we do not want measures to be direct optimization targets, and we want ways to responsively adjust measures when inevitably we learn of their limitations. This is a significant concern, and we must take it seriously &#x2014; while <a href="https://deepmind.google/discover/blog/the-ethics-of-advanced-ai-assistants/">some</a> <a href="https://arxiv.org/abs/2302.09248">research</a> has begun to explore this topic, more is needed. Yet in the spirit of pragmatic harm reduction, given that metrics are both technically and politically important for steering AI systems, developing less flawed measures remains an important goal.<br></p><p>Let&#x2019;s consider one important example of harms from measurement: the tendency for a single global measure to trample local context. Training data for models, including internet data in particular, is heavily biased. Thus without deliberate remedy, models demonstrate uneven abilities to support the wellbeing of minority populations, undermining social justice (as convincingly highlighted by the <a href="https://facctconference.org/">AI ethics community</a>). While LLMs have exciting potential to respect cultural nuance and norms, informed by the background of the user, we must work deliberately to realize it. One important direction is to develop measures of wellbeing specific to diverse cultural contexts, to drive accountability and reward progress.<br></p><p>To tie these ideas about measurement together, we suggest a taxonomy, looking at measures of AI <em>capabilities, behaviors, usage, and impacts</em>. Similar to <a href="https://arxiv.org/pdf/2310.11986.pdf">this DeepMind paper</a>, the idea is to examine spheres of expanding context, from testing a model in isolation (both what it is capable of and what behaviors it demonstrates), all the way to understanding what happens when a model meets the real world (how humans use it, and what its impact is on them and society).<br></p><p>The idea is that we need a complementary ecosystem of measures fit to different stages of model development and deployment. In more detail:</p><ul><li><em>AI capabilities</em> refers to what models are able to do. For example, systems today are capable of generating novel content, and translating accurately between languages.</li><li><em>AI behaviors</em> refers to how an AI system responds to different concrete situations. For example, many models are trained to refuse to answer questions that enable dangerous activities, like how to build a bomb,even though they have the capability to correctly answer them).</li><li><em>AI usage</em> refers to how models are used in practice when deployed. For example, AI systems today are used in chat interfaces to help answer questions, as coding assistants in IDEs, to sort social media feeds, and as personal companions.</li><li><em>AI impacts </em>refers to how AI impacts our experience or society. For example, people may feel empowered to do what&#x2019;s important to them if AI helps them with rote coding, and societal trust in democracy may increase if <a href="https://bridging.systems/">AI sorts social media feeds towards bridging divides</a>.</li></ul><p>As an example of applying this framework to an important quality that contributes to wellbeing, here is a sketch of how we might design measures of human autonomy: </p><!--kg-card-begin: html--><table style="border:none;border-collapse:collapse;"><colgroup><col width="101"><col width="135"><col width="134"><col width="126"><col width="124"></colgroup><tbody><tr style="height:0pt"><td style="border-left:solid #cccccc 1pt;border-right:solid #cccccc 1pt;border-bottom:solid #cccccc 1pt;border-top:solid #cccccc 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;"><p dir="ltr" style="line-height:1.2;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:11pt;font-family:Arial,sans-serif;color:#000000;background-color:transparent;font-weight:700;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">Goal</span></p></td><td style="border-left:solid #cccccc 1pt;border-right:solid #cccccc 1pt;border-bottom:solid #cccccc 1pt;border-top:solid #cccccc 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;"><p dir="ltr" style="line-height:1.2;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:11pt;font-family:Arial,sans-serif;color:#000000;background-color:transparent;font-weight:700;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">Capabilities</span></p><p dir="ltr" style="line-height:1.2;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:11pt;font-family:Arial,sans-serif;color:#000000;background-color:transparent;font-weight:400;font-style:italic;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">Model Benchmarks</span></p></td><td style="border-left:solid #cccccc 1pt;border-right:solid #cccccc 1pt;border-bottom:solid #cccccc 1pt;border-top:solid #cccccc 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;"><p dir="ltr" style="line-height:1.2;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:11pt;font-family:Arial,sans-serif;color:#000000;background-color:transparent;font-weight:700;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">Behaviors</span></p><p dir="ltr" style="line-height:1.2;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:11pt;font-family:Arial,sans-serif;color:#000000;background-color:transparent;font-weight:400;font-style:italic;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">System Benchmarks</span></p></td><td style="border-left:solid #cccccc 1pt;border-right:solid #cccccc 1pt;border-bottom:solid #cccccc 1pt;border-top:solid #cccccc 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;"><p dir="ltr" style="line-height:1.2;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:11pt;font-family:Arial,sans-serif;color:#000000;background-color:transparent;font-weight:700;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">Usage</span></p><p dir="ltr" style="line-height:1.2;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:11pt;font-family:Arial,sans-serif;color:#000000;background-color:transparent;font-weight:400;font-style:italic;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">User Surveys</span></p></td><td style="border-left:solid #cccccc 1pt;border-right:solid #cccccc 1pt;border-bottom:solid #cccccc 1pt;border-top:solid #cccccc 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;"><p dir="ltr" style="line-height:1.2;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:11pt;font-family:Arial,sans-serif;color:#000000;background-color:transparent;font-weight:700;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">Impact</span></p><p dir="ltr" style="line-height:1.2;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:11pt;font-family:Arial,sans-serif;color:#000000;background-color:transparent;font-weight:400;font-style:italic;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">User and Population Surveys</span></p></td></tr><tr style="height:0pt"><td style="border-left:solid #cccccc 1pt;border-right:solid #cccccc 1pt;border-bottom:solid #cccccc 1pt;border-top:solid #cccccc 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;"><p dir="ltr" style="line-height:1.2;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:11pt;font-family:Arial,sans-serif;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">Respecting autonomy</span></p></td><td style="border-left:solid #cccccc 1pt;border-right:solid #cccccc 1pt;border-bottom:solid #cccccc 1pt;border-top:solid #cccccc 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;"><p dir="ltr" style="line-height:1.2;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:11pt;font-family:Arial,sans-serif;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">Understand what someone is trying to achieve in a given context</span></p><br><p dir="ltr" style="line-height:1.2;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:11pt;font-family:Arial,sans-serif;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">Understand the frontier of someone&#x2019;s skill level</span></p><br><p dir="ltr" style="line-height:1.2;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:11pt;font-family:Arial,sans-serif;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">Understand what activities a user finds meaningful</span></p></td><td style="border-left:solid #cccccc 1pt;border-right:solid #cccccc 1pt;border-bottom:solid #cccccc 1pt;border-top:solid #cccccc 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;"><p dir="ltr" style="line-height:1.2;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:11pt;font-family:Arial,sans-serif;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">Socratic dialogue rather than just providing answers</span></p><br><p dir="ltr" style="line-height:1.2;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:11pt;font-family:Arial,sans-serif;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">Tapping into users&#x2019; wisdom rather than giving advice</span></p><br><p dir="ltr" style="line-height:1.2;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:11pt;font-family:Arial,sans-serif;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">Selective automation of tasks</span></p><br></td><td style="border-left:solid #cccccc 1pt;border-right:solid #cccccc 1pt;border-bottom:solid #cccccc 1pt;border-top:solid #cccccc 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;"><p dir="ltr" style="line-height:1.2;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:11pt;font-family:Arial,sans-serif;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">Used to aid humans with tasks rather than fully automate tasks they find&#xA0; meaningful</span></p><br><p dir="ltr" style="line-height:1.2;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:11pt;font-family:Arial,sans-serif;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">Used to help humans develop social skills instead of to nurture emotional attachment to simulated persona</span></p></td><td style="border-left:solid #cccccc 1pt;border-right:solid #cccccc 1pt;border-bottom:solid #cccccc 1pt;border-top:solid #cccccc 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;"><p dir="ltr" style="line-height:1.2;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:11pt;font-family:Arial,sans-serif;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">People feel empowered</span></p><br><p dir="ltr" style="line-height:1.2;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:11pt;font-family:Arial,sans-serif;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">People are able to achieve their goals</span></p><br><p dir="ltr" style="line-height:1.2;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:11pt;font-family:Arial,sans-serif;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">People are pushed to grow</span></p></td></tr></tbody></table><!--kg-card-end: html--><p>Let&#x2019;s work through this example: we take a quality with strong scientific links to wellbeing, <a href="https://en.wikipedia.org/wiki/Self-determination_theory">autonomy</a>, and create measures of it and what enables it, all along the pipeline from model development to when it&#x2019;s deployed at scale. <br></p><p>Starting from the right side of the table (Impact), there exist validated psychological <a href="https://scottbarrykaufman.com/wp-content/uploads/2015/11/830.pdf">surveys</a> that measure autonomy, which can be adapted and given to users of an AI app to measure its <em>impact</em> on their autonomy. Then, moving leftwards, these changes in autonomy could be linked to more specific types of <em>usage</em>, through additional survey questions. For example, perhaps automating tasks that users actually find meaningful may correlate with decreased autonomy.<br></p><p>Moving further left on the table, the <em>behaviors</em> of models that are needed to enable beneficial usage and impact can be gauged through more focused benchmarks. To measure behaviors of an AI system, one could run fixed workflows on an AI application where gold-standard answers come from expert labelers; another approach is to simulate users (e.g. with language models) interacting with an AI application to see how often and skillfully it performs particular behaviors, like socratic dialogue.<br></p><p>Finally, <em>capabilities</em> of a particular AI model could be similarly measured through benchmark queries input directly to the model, in a way very similar to how LLMs are benchmarked for capabilities like reasoning or question-answering. For example, the capability to understand a person&#x2019;s skill level may be important to help them push their limits. A dataset could be collected of user behaviors in some application, annotated with their skill level; and the evaluation would be how well the model could predict skill level from observed behavior.<br></p><p>At each stage, the hope is to link what is measured through evidence and reasoning to what lies above and below it in the stack. And we would want a diversity of measures at each level, reflecting different hypotheses about how to achieve the top-level quality, and with the understanding that each measure is always imperfect and subject to revision. In a similar spirit, rather than some final answer, this taxonomy and example autonomy measures are intended to inspire much-needed pioneering work towards wellbeing measurement.<br></p><h3 id="we-need-to-train-models-to-improve-their-ability-to-support-wellbeing">We need to train models to improve their ability to support wellbeing</h3><p>Foundation models are becoming increasingly capable and in the future we believe most applications will not train models from scratch. Instead, most applications will prompt cutting-edge proprietary models, or fine-tune such models through limited APIs, or train small models on domain-specific responses from the largest models for cost-efficiency reasons. As evidence, note that to accomplish tasks with GPT-3 often required chaining together many highly-tuned prompts, whereas with GPT-4 those same tasks often succeed with the first casual prompting attempt. Additionally, we are seeing the rise of capable smaller models specialized for particular tasks, trained through data from large models.<br></p><p>What&#x2019;s important about this trend is that applications are differentially brought to market driven by what the largest models can most readily accomplish. For example, if frontier models excel at viral persuasion from being trained on Twitter data, but struggle with the depths of positive psychology, it will be easier to create persuasive apps than supportive ones, and there will be more of them, sooner, on the market.<br></p><p>Thus we believe it&#x2019;s crucial that the most capable foundation models <em>themselves</em> understand what contributes to our wellbeing &#x2014; an understanding granted to them through their <em>training process</em>. We want the AI applications that we interface with (whether therapists, tutors, social media apps, or coding assistants) to understand how to support our wellbeing within their relevant role.<br></p><p>However, the benefit of breaking down the capabilities and behaviors needed to support wellbeing, as we did earlier, is that we can deliberately target their improvement. One central lever is to gather or generate training data, which is the general fuel underlying model capabilities. There is an exciting opportunity to create datasets to support desired wellbeing capabilities and behaviors &#x2014; for example, perhaps collections of wise responses to questions, pairs of statements from people and the emotions that they felt in expressing them, biographical stories about desirable and undesirable life trajectories, or first-person descriptions of human experience in general. The effect of these datasets can be grounded in the measures discussed above.<br></p><p>To better ground our thinking, we can examine how wellbeing data could improve the common <em>phases</em> of foundation model training: pretraining, fine-tuning, and alignment.</p><h4 id="pretraining">Pretraining</h4><p>The first training phase (confusingly called pretraining) establishes a model&#x2019;s base abilities. It does so by training on vast amounts of variable-quality data, like a scrape of the internet. One contribution could be to either generate or gather large swaths of wellbeing relevant data, or to prioritize such data during training (also known as altering the data mix). For example, data could be sourced from subreddits relevant to mental health or life decisions, collections of biographies, books about psychology, or transcripts of supportive conversations. Additional data could be generated through paying contractors, crowdsourced through <a href="https://en.wikipedia.org/wiki/Human-based_computation_game#:~:text=A%20human%2Dbased%20computation%20game,an%20entertaining%20way%20(gamification).">Games With a Purpose</a> &#x2014; fun experiences that create wellbeing-relevant data as a byproduct, or simulated through <a href="https://dl.acm.org/doi/abs/10.1145/3586183.3606763">generative agent-based models</a>.</p><h4 id="fine-tuning">Fine-tuning</h4><p>The next stage of model training is fine-tuning. Here, smaller amounts of high-quality data, like diverse examples of desired behavior gathered from experts, focus the general capabilities resulting from pretraining. For different wellbeing-supporting behaviors we might want from a model, we can create fine-tuning datasets through deliberate curation of larger datasets, or by enlisting and recording the behavior of human experts in the relevant domain. We hope that the companies training the largest models place more emphasis on wellbeing in this phase of training, which is often driven by tasks with more obvious economic implications, like coding.</p><h4 id="alignment">Alignment</h4><p>The final stage of model training is alignment, often achieved through techniques like reinforcement learning through human feedback (RLHF), where human contractors give feedback on AI responses to guide the model towards better ones. Or through AI-augmented techniques like <a href="https://arxiv.org/abs/2212.08073">constitutional AI</a>, where an AI teaches itself to abide by a list of human-specified principles. The fuel of RLHF is preference data about what responses are preferred over others. Therefore we imagine opportunities for creating data sets of expert preferences that relate to wellbeing behaviors (even though what constitutes expertise in wellbeing may be interestingly contentious). For constitutional AI, we may need to iterate in practice with lists of wellbeing principles that we want to support, like human autonomy, and how, specifically, a model can respect it across different contexts.<br></p><p>In general, we need pipelines where wellbeing evaluations (as discussed in the last section) inform how we improve models. We need to find extensions to paradigms like RLHF that go beyond which response humans prefer in the moment, considering also which responses support user long-term growth, wellbeing, and autonomy, or better embody the spirit of the institutional role that the model is currently playing. These are intriguing, subtle, and challenging research questions that strike at the heart of the intersection of machine learning and societal wellbeing, and deserve much more attention. <br></p><p>For example, we care about wellbeing over spans of years or decades, but it is impractical to apply RLHF <em>directly</em> on human feedback to such ends, as we cannot wait decades to gather human feedback for a model; instead, we need research that helps integrate validated short-term proxies for long-term wellbeing (e.g. quality of intimate relationships, time spent in flow, etc.), ways to learn from longitudinal data where it exists (perhaps web journals, autobiographies, scientific studies), and to collect the judgment of those who devote their lifetime to helping support individuals flourish (like counselors or therapists).</p><h3 id="we-need-to-deploy-ai-models-in-a-way-that-supports-wellbeing">We need to deploy AI models in a way that supports wellbeing</h3><p>Ultimately we want AI models deployed in the world to benefit us. AI applications could directly target human wellbeing, for example by directly supporting mental health or coaching us in a rigorous way. But as argued earlier, the broader ecosystem of AI-assisted applications, like social media, dating apps, video games, and content-providers like Netflix, serve as societal infrastructure for wellbeing and have enormous diffuse impact upon us; one of us has written about the possibility of creating more <a href="https://arxiv.org/abs/2302.09248">humanistic wellbeing-infrastructure applications</a>. While difficult, dramatic societal benefits could result from, for example, new social media networks that better align with short and long-term wellbeing.<br></p><p>We believe there are exciting opportunities for thoughtful positive deployments that pave the way as standard-setting beacons of hope, perhaps particularly in ethically challenging areas &#x2014; although these of course may also be the riskiest. For example, artificial intimacy applications like Replika may be unavoidable even as they make us squeamish, and may truly <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7084290/">benefit some</a> users while <a href="https://www.washingtonpost.com/technology/2023/03/30/replika-ai-chatbot-update/">harming others</a>. It&#x2019;s worthwhile to ask what (if anything) could enable artificial companions that are aligned with users&#x2019; wellbeing and do not harm society. Perhaps it is possible to thread the needle: they could help us develop the social skills needed to find real-world companions, or at least have strong, transparent guarantees about their fiduciary relationship to us, all while remaining viable as a business or non-profit. Or perhaps we can create harm-reduction services that help people unaddict from artificial companions that have become obstacles to their growth and development. Similar thoughts may apply to AI therapists, AI-assisted dating apps, and attention-economy apps, where incentives are difficult to align. <br></p><p>One obvious risk is that we each are often biased to think we are more thoughtful than others, but may nonetheless be swept away by problematic incentives, like the trade-off between profit and user benefit. Legal structures like public benefit corporations, non-profits, or innovative new structures may help minimize this risk, as may value-driven investors or exceedingly careful design of internal culture.<br></p><p>Another point of leverage is that a successful proof of concept may change the attitudes and incentives for companies training and deploying the largest foundation models. We&#x2019;re seeing a pattern where large AI labs incorporate best practices from outside product deployments back into their models. For example, ChatGPT plugins like data analysis and the GPT market were explored first by companies outside OpenAI before being incorporated into their ecosystem. And RLHF, which was first integrated into language models by OpenAI, is now a mainstay across foundation model development.<br></p><p>In a similar way to how RLHF became a mainstay, we want the capability to support our agency, understand our emotions, and better embody institutional roles to also become table-stakes features for model developers.This could happen through research advances <em>outside</em> of the big companies, making it much easier for such features to be adopted <em>within</em> them &#x2014; though adoption may require pressure, through regulation, advocacy, or competition.</p><h3 id="initiatives">Initiatives</h3><p>We believe there&#x2019;s much concrete work to be done in the present. Here are a sampling of initiatives to seed thinking about what could move the field forward:<br></p><!--kg-card-begin: html--><table style="border:none;border-collapse:collapse;"><colgroup><col width="245"><col width="379"></colgroup><tbody><tr style="height:0pt"><td style="border-left:solid #cccccc 1pt;border-right:solid #cccccc 1pt;border-bottom:solid #cccccc 1pt;border-top:solid #cccccc 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;"><p dir="ltr" style="line-height:1.2;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:11pt;font-family:Arial,sans-serif;color:#000000;background-color:transparent;font-weight:700;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">Area</span></p></td><td style="border-left:solid #cccccc 1pt;border-right:solid #cccccc 1pt;border-bottom:solid #cccccc 1pt;border-top:solid #cccccc 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;"><p dir="ltr" style="line-height:1.2;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:11pt;font-family:Arial,sans-serif;color:#000000;background-color:transparent;font-weight:700;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">Initiatives</span></p></td></tr><tr style="height:0pt"><td style="border-left:solid #cccccc 1pt;border-right:solid #cccccc 1pt;border-bottom:solid #cccccc 1pt;border-top:solid #cccccc 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;"><p dir="ltr" style="line-height:1.2;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:11pt;font-family:Arial,sans-serif;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">Understanding where we want to go</span></p></td><td style="border-left:solid #cccccc 1pt;border-right:solid #cccccc 1pt;border-bottom:solid #cccccc 1pt;border-top:solid #cccccc 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;"><ul style="margin-top:0;margin-bottom:0;padding-inline-start:48px;"><li dir="ltr" style="list-style-type:disc;font-size:11pt;font-family:Arial,sans-serif;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;" aria-level="1"><p dir="ltr" style="line-height:1.2;margin-top:0pt;margin-bottom:0pt;" role="presentation"><span style="font-size:11pt;font-family:Arial,sans-serif;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">Global discussions on what is important to us.</span></p></li><li dir="ltr" style="list-style-type:disc;font-size:11pt;font-family:Arial,sans-serif;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;" aria-level="1"><p dir="ltr" style="line-height:1.2;margin-top:0pt;margin-bottom:0pt;" role="presentation"><span style="font-size:11pt;font-family:Arial,sans-serif;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">Democratic elicitation of what matters to people (for example, the work done by </span><a href="https://cip.org/" style="text-decoration:none;"><span style="font-size:11pt;font-family:Arial,sans-serif;color:#1155cc;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:underline;-webkit-text-decoration-skip:none;text-decoration-skip-ink:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">Collective Intelligence Project</span></a><span style="font-size:11pt;font-family:Arial,sans-serif;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;"> and the </span><a href="https://meaningalignment.org/" style="text-decoration:none;"><span style="font-size:11pt;font-family:Arial,sans-serif;color:#1155cc;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:underline;-webkit-text-decoration-skip:none;text-decoration-skip-ink:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">Meaning Alignment Institute</span></a><span style="font-size:11pt;font-family:Arial,sans-serif;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">).</span></p></li><li dir="ltr" style="list-style-type:disc;font-size:11pt;font-family:Arial,sans-serif;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;" aria-level="1"><p dir="ltr" style="line-height:1.2;margin-top:0pt;margin-bottom:0pt;" role="presentation"><span style="font-size:11pt;font-family:Arial,sans-serif;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">Concrete visualizations of what we want society to look like in 2050 (for example, the worldbuilding contest run by the </span><a href="https://futureoflife.org/project/worldbuilding-competition/" style="text-decoration:none;"><span style="font-size:11pt;font-family:Arial,sans-serif;color:#1155cc;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:underline;-webkit-text-decoration-skip:none;text-decoration-skip-ink:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">Future of Life Institute</span></a><span style="font-size:11pt;font-family:Arial,sans-serif;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">).</span></p></li><li dir="ltr" style="list-style-type:disc;font-size:11pt;font-family:Arial,sans-serif;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;" aria-level="1"><p dir="ltr" style="line-height:1.2;margin-top:0pt;margin-bottom:0pt;" role="presentation"><span style="font-size:11pt;font-family:Arial,sans-serif;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">Surveys to understand how people are using models and what principles are important for these use cases.</span></p></li><li dir="ltr" style="list-style-type:disc;font-size:11pt;font-family:Arial,sans-serif;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;" aria-level="1"><p dir="ltr" style="line-height:1.2;margin-top:0pt;margin-bottom:0pt;" role="presentation"><span style="font-size:11pt;font-family:Arial,sans-serif;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">Improve our basic understanding of the factors that lead to wellbeing.</span></p></li></ul></td></tr><tr style="height:0pt"><td style="border-left:solid #cccccc 1pt;border-right:solid #cccccc 1pt;border-bottom:solid #cccccc 1pt;border-top:solid #cccccc 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;"><p dir="ltr" style="line-height:1.2;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:11pt;font-family:Arial,sans-serif;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">Develop methods for measuring how AI affects wellbeing</span></p></td><td style="border-left:solid #cccccc 1pt;border-right:solid #cccccc 1pt;border-bottom:solid #cccccc 1pt;border-top:solid #cccccc 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;"><ul style="margin-top:0;margin-bottom:0;padding-inline-start:48px;"><li dir="ltr" style="list-style-type:disc;font-size:11pt;font-family:Arial,sans-serif;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;" aria-level="1"><p dir="ltr" style="line-height:1.2;margin-top:0pt;margin-bottom:0pt;" role="presentation"><span style="font-size:11pt;font-family:Arial,sans-serif;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">Create benchmarks for models&#x2019; ability to understand emotions, make wise choices, respond in ways that respect our autonomy, etc.</span></p></li><li dir="ltr" style="list-style-type:disc;font-size:11pt;font-family:Arial,sans-serif;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;" aria-level="1"><p dir="ltr" style="line-height:1.2;margin-top:0pt;margin-bottom:0pt;" role="presentation"><span style="font-size:11pt;font-family:Arial,sans-serif;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">Evaluations on how models impact people&#x2019;s psychological experience.</span></p></li><li dir="ltr" style="list-style-type:disc;font-size:11pt;font-family:Arial,sans-serif;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;" aria-level="1"><p dir="ltr" style="line-height:1.2;margin-top:0pt;margin-bottom:0pt;" role="presentation"><span style="font-size:11pt;font-family:Arial,sans-serif;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">Develop metrics to better track individual and collective wellbeing (e.g. tracking our somatic states, tracking societal trust, etc).</span></p></li></ul></td></tr><tr style="height:0pt"><td style="border-left:solid #cccccc 1pt;border-right:solid #cccccc 1pt;border-bottom:solid #cccccc 1pt;border-top:solid #cccccc 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;"><p dir="ltr" style="line-height:1.2;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:11pt;font-family:Arial,sans-serif;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">Train AI models based on what&#x2019;s important to us</span></p></td><td style="border-left:solid #cccccc 1pt;border-right:solid #cccccc 1pt;border-bottom:solid #cccccc 1pt;border-top:solid #cccccc 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;"><ul style="margin-top:0;margin-bottom:0;padding-inline-start:48px;"><li dir="ltr" style="list-style-type:disc;font-size:11pt;font-family:Arial,sans-serif;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;" aria-level="1"><p dir="ltr" style="line-height:1.2;margin-top:0pt;margin-bottom:0pt;" role="presentation"><span style="font-size:11pt;font-family:Arial,sans-serif;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">Create datasets of emotionally supportive interactions.</span></p></li><li dir="ltr" style="list-style-type:disc;font-size:11pt;font-family:Arial,sans-serif;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;" aria-level="1"><p dir="ltr" style="line-height:1.2;margin-top:0pt;margin-bottom:0pt;" role="presentation"><span style="font-size:11pt;font-family:Arial,sans-serif;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">Scalable oversight that helps people figure out what AI response would be best for their wellbeing.</span></p></li><li dir="ltr" style="list-style-type:disc;font-size:11pt;font-family:Arial,sans-serif;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;" aria-level="1"><p dir="ltr" style="line-height:1.2;margin-top:0pt;margin-bottom:0pt;" role="presentation"><span style="font-size:11pt;font-family:Arial,sans-serif;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">Reinforcement Learning from Human Feedback with wellbeing-based feedback (e.g. from therapists).</span></p></li><li dir="ltr" style="list-style-type:disc;font-size:11pt;font-family:Arial,sans-serif;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;" aria-level="1"><p dir="ltr" style="line-height:1.2;margin-top:0pt;margin-bottom:0pt;" role="presentation"><a href="https://meaningalignment.substack.com/p/introducing-democratic-fine-tuning" style="text-decoration:none;"><span style="font-size:11pt;font-family:Arial,sans-serif;color:#1155cc;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:underline;-webkit-text-decoration-skip:none;text-decoration-skip-ink:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">Democratic finetuning</span></a><span style="font-size:11pt;font-family:Arial,sans-serif;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;"> (run by the Meaning Alignment Institute)</span></p></li></ul></td></tr><tr style="height:0pt"><td style="border-left:solid #cccccc 1pt;border-right:solid #cccccc 1pt;border-bottom:solid #cccccc 1pt;border-top:solid #cccccc 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;"><p dir="ltr" style="line-height:1.2;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:11pt;font-family:Arial,sans-serif;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">Deploy models in beneficial areas</span></p></td><td style="border-left:solid #cccccc 1pt;border-right:solid #cccccc 1pt;border-bottom:solid #cccccc 1pt;border-top:solid #cccccc 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;"><ul style="margin-top:0;margin-bottom:0;padding-inline-start:48px;"><li dir="ltr" style="list-style-type:disc;font-size:11pt;font-family:Arial,sans-serif;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;" aria-level="1"><p dir="ltr" style="line-height:1.2;margin-top:0pt;margin-bottom:0pt;" role="presentation"><span style="font-size:11pt;font-family:Arial,sans-serif;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">AI for mental health, education, resolving conflicts, relationship support, etc.</span></p></li></ul></td></tr></tbody></table><!--kg-card-end: html--><h2 id="conclusion-a-call-to-action">Conclusion: A call to action</h2><p>AI will transform society in ways that we cannot yet predict. If we continue on the present track, we risk AI reshaping our interactions and institutions in ways that erode our wellbeing and what makes our lives meaningful. Instead, challenging as it may be, we need to develop AI systems that understand and support wellbeing, both individual and societal. This is our call to reorientate towards wellbeing, to continue building a community and a field, in hopes of realizing AI&#x2019;s potential to support our species&#x2019; strivings toward a flourishing future.</p>]]></content:encoded></item><item><title><![CDATA[Financial Market Applications of LLMs]]></title><description><![CDATA[<p>The AI revolution drove frenzied investment in both private and public companies and captured the public&#x2019;s imagination in 2023. Transformational consumer products like ChatGPT are powered by Large Language Models (LLMs) that excel at modeling sequences of tokens that represent words or parts of words [2]. Amazingly, structural</p>]]></description><link>https://thegradient.pub/financial-market-applications-of-llms/</link><guid isPermaLink="false">661762b993571d5c8c154ea7</guid><category><![CDATA[LLM]]></category><category><![CDATA[Overviews]]></category><dc:creator><![CDATA[Richard Dewey]]></dc:creator><pubDate>Sat, 20 Apr 2024 17:57:39 GMT</pubDate><media:content url="https://thegradient.pub/content/images/2024/04/shutterstock_2448136839.jpg" medium="image"/><content:encoded><![CDATA[<img src="https://thegradient.pub/content/images/2024/04/shutterstock_2448136839.jpg" alt="Financial Market Applications of LLMs"><p>The AI revolution drove frenzied investment in both private and public companies and captured the public&#x2019;s imagination in 2023. Transformational consumer products like ChatGPT are powered by Large Language Models (LLMs) that excel at modeling sequences of tokens that represent words or parts of words [2]. Amazingly, structural understanding emerges from learning next-token prediction, and agents are able to complete tasks such as translation, question answering and generating human-like prose from simple user prompts.</p><p>Not surprisingly, quantitative traders have asked: can we turn these models into the next price or trade prediction [1,9,10]? That is, rather than modeling sequences of words, can we model sequences of prices or trades. This turns out to be an interesting line of inquiry that reveals much about both generative AI and financial time series modeling. Be warned this will get wonky.</p><p>LLMs are known as autoregressive learners -- those using previous tokens or elements in a sequence to predict the next element or token. In quantitative trading, for example in strategies like statistical arbitrage in stocks, most research is concerned with identifying autoregressive structure. That means finding sequences of news or orders or fundamental changes that best predict future prices.</p><p>Where things break down is in the quantity and information content of available data to train the models. At the 2023 NeurIPS conference, Hudson River Trading, a high frequency trading firm, presented a comparison of the number of input tokens used to train GPT-3 with the amount of trainable tokens available in the stock market data per year HRT estimated that, with 3,000 tradable stocks, 10 data points per stock per day, 252 trading days per year, and 23400 seconds in a trading day, there are 177 billion stock market tokens per year available as market data. GPT-3 was trained on 500 billion tokens, so not far off [6].</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://thegradient.pub/content/images/2024/04/Screenshot-2024-04-18-at-3.11.18-PM.png" class="kg-image" alt="Financial Market Applications of LLMs" loading="lazy" width="2000" height="368" srcset="https://thegradient.pub/content/images/size/w600/2024/04/Screenshot-2024-04-18-at-3.11.18-PM.png 600w, https://thegradient.pub/content/images/size/w1000/2024/04/Screenshot-2024-04-18-at-3.11.18-PM.png 1000w, https://thegradient.pub/content/images/size/w1600/2024/04/Screenshot-2024-04-18-at-3.11.18-PM.png 1600w, https://thegradient.pub/content/images/2024/04/Screenshot-2024-04-18-at-3.11.18-PM.png 2356w" sizes="(min-width: 720px) 720px"><figcaption>numbers courtesy of HRT 2023 NeuRIPS presentation</figcaption></figure><p>But, in the trading context the tokens will be prices or returns or trades rather than syllables or words; the former is much more difficult to predict. Language has an underlying linguistic structure (e.g., grammar) [7]. It&#x2019;s not hard to imagine a human predicting the next word in a sentence, however that same human would find it extremely challenging to predict the next return given a sequence of previous trades, hence the lack of billionaire day traders. The challenge is that there are very smart people competing away any signal in the market, making it <em>almost </em>efficient (&#x201C;efficiently inefficient&#x201D;, in the words of economist Lasse Pedersen) and hence unpredictable. No adversary actively tries to make sentences more difficult to predict &#x2014; if anything, authors usually seek to make their sentences easy to understand and hence <em>more</em> predictable.</p><p>Looked at from another angle, there is much more noise than signal in financial data. Individuals and institutions are trading for reasons that might not be rational or tied to any fundamental change in a business. The GameStop episode in 2021 is one such example. Financial time series are also constantly changing with new fundamental information, regulatory changes, and occasional large macroeconomic shifts such as currency devaluations. Language evolves at a much slower pace and over longer time horizons.</p><p>On the other hand, there are reasons to believe that ideas from AI will work well in financial markets. One emerging area of AI research with promising applications to finance is multimodal learning [5], which aims to use different modalities of data, for example both images and textual inputs to build a unified model. With OpenAI&#x2019;s DALL-E 2 model, a user can enter text and the model will generate an image. In finance, multi-modal efforts could be useful to combine information classical sources such as technical time series data (prices, trades, volumes, etc.) with alternative data in different modes like sentiment or graphical interactions on twitter, natural language news articles and corporate reports, or the satellite images of shipping activity in a commodity centric port. Here, leveraging multi-modal AI, one could potentially incorporate all these types of non-price information to predict well.</p><p>Another strategy called &#x2018;residualization&#x2019; holds prominence in both finance and AI, though it assumes different roles in the two domains. &#xA0;In finance, structural `factor&#x2019; models break down the contemporaneous observations of returns across different assets into a shared component (the market return, or more generally returns of common, market-wide factors) and an idiosyncratic component unique to each underlying asset. Market and factor returns are difficult to predict and create interdependence, so it is often helpful to remove the common element when making predictions at the individual asset level and to maximize the number of independent observations in the data. </p><p>In residual network architectures such as transformers, there&#x2019;s a similar idea that we want to learn a function h(X) of an input X, but it might be easier to learn the residual of h(X) to the identity map, i.e., h(X) &#x2013; X. Here, if the function h(X) is close to identity, its residual will be close to zero, and hence there will be less to learn and learning can be done more efficiently. In both cases the goal is to exploit structure to refine predictions: in the finance case, the idea is to focus on predicting innovations beyond what is implied by the overall market, for residual networks the focus is on predicting innovations to the identity map.</p><p>A key ingredient for the impressive performance of LLMs work is their ability to discern affinities or strengths between tokens over long horizons known as context windows. In financial markets, the ability to focus attention across long horizons enables analysis of multi-scale phenomena, with some aspects of market changes explained across very different time horizons. For example, at one extreme, fundamental information (e.g., earnings) may be incorporated into prices over months, technical phenomena (e.g., momentum) might be realized over days, and, at the other extreme, microstructure phenomena (e.g., order book imbalance) might have a time horizon of seconds to minutes.</p><p>Capturing all of these phenomena involves analysis of multiple time horizons across the context window. However, in finance, prediction over multiple <em>future</em> time horizons is also important. For example, a quantitative system may seek to trade to profit from multiple different anomalies that are realized over multiple time horizons (e.g., simultaneously betting on a microstructure event and an earnings event). This requires predicting not just the next period return of the stock, but the entire term structure or trajectory of expected returns, while current transformer-style predictive models only look one period in the future.</p><p>Another financial market application of LLMs might be synthetic data creation [4,8]. This could take a few directions. Simulated stock price trajectories can be generated that mimic characteristics observed in the market and can be extremely beneficial given that financial market data is scarce relative to other sources as highlighted above in the number of tokens available. Artificial data could open the door for meta-learning techniques which have successfully been applied, for example, in robotics. In the robotic setting controllers are first trained using cheap but not necessarily accurate physics simulators, before being better calibrated using expensive real world experiments with robots. In finance the simulators could be used to coarsely train and optimize trading strategies. The model would learn high level concepts like risk aversion and diversification and tactical concepts such as trading slowly to minimize the price impact of a trade. Then precious real market data could be employed to fine-tune the predictions and determine precisely the optimal speed to trade.</p><figure class="kg-card kg-image-card"><img src="https://thegradient.pub/content/images/2024/04/Screenshot-2024-04-18-at-3.08.44-PM.png" class="kg-image" alt="Financial Market Applications of LLMs" loading="lazy" width="2000" height="258" srcset="https://thegradient.pub/content/images/size/w600/2024/04/Screenshot-2024-04-18-at-3.08.44-PM.png 600w, https://thegradient.pub/content/images/size/w1000/2024/04/Screenshot-2024-04-18-at-3.08.44-PM.png 1000w, https://thegradient.pub/content/images/size/w1600/2024/04/Screenshot-2024-04-18-at-3.08.44-PM.png 1600w, https://thegradient.pub/content/images/size/w2400/2024/04/Screenshot-2024-04-18-at-3.08.44-PM.png 2400w" sizes="(min-width: 720px) 720px"></figure><p>Financial market practitioners are often interested in extreme events, the times when trading strategies are more likely to experience significant gains or losses. Generative models where it&#x2019;s possible to sample from extreme scenarios could find use. However extreme events by definition occur rarely and hence determining the right parameters and sampling data from the corresponding distribution is fraught.</p><p>Despite the skepticism that LLMs will find use in quantitative trading, they might boost fundamental analysis. As AI models improve, it&#x2019;s easy to imagine them helping analysts refine an investment thesis, uncover inconsistencies in management commentary or find latent relationships between tangential industries and businesses [3]. Essentially these models could provide a Charlie Munger for every investor.</p><p>The surprising thing about the current generative AI revolution is that it&#x2019;s taken almost everyone &#x2013; academic researchers, cutting edge technology firms and long-time observers &#x2013; by surprise. The idea that building bigger and bigger models would lead to emergent capabilities like we see today was totally unexpected and still not fully understood.</p><p>The success of these AI models has supercharged the flow of human and financial capital into AI, which should in turn lead to even better and more capable models. So while the case for GPT-4 like models taking over quantitative trading is currently unlikely, we advocate keeping an open mind. Expecting the unexpected has been a profitable theme in the AI business.</p><hr><h3 id="references">References</h3><ol><li>&#x201C;Applying Deep Neural Networks to Financial Time Series Forecasting&#x201D; Allison Koenecke. 2022</li><li>&#x201C;<a href="https://scholar.google.com/scholar?oi=bibs&amp;cluster=2960712678066186980&amp;btnI=1&amp;hl=en">Attention is all you need</a>.&#x201D; A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones&#x2026; &#xA0;Advances in Neural Information Processing Systems, 2017</li><li>&#x201C;Can ChatGPT Forecast Stock Price Movements? Return Predictability and Large Language Models&#x201D; . Lopez-Lira, Alejandro and Tang, Yuehua, (April 6, 2023) Available at SSRN</li><li>&#x201C;<a href="https://scholar.google.com/scholar?oi=bibs&amp;cluster=8154893591177160457&amp;btnI=1&amp;hl=en">Generating Synthetic Data in Finance: Opportunities, Challenges and Pitfalls</a>.&#x201D; SA Assefa, D Dervovic, M Mahfouz, RE Tillman&#x2026; - Proceedings of the First ACM International Conference &#x2026;, 2020</li><li>&#x201C;GPT-4V(ision) System Card.&#x201D; OpenAI. September 2023</li><li>&#x201C;<a href="https://scholar.google.com/scholar?oi=bibs&amp;cluster=15953747982133883426&amp;btnI=1&amp;hl=en">Language models are few-shot learners</a>.&#x201D; T Brown, B Mann, N Ryder, M Subbiah, JD Kaplan&#x2026; - Advances in Neural Information Processing Systems, 2020</li><li>&#x201C;Sequence to Sequence Learning with Neural Networks.&#x201D; I.Sutskever,O.Vinyals,and Q.V.Le in Advances in Neural Information Processing Systems, 2014, pp. 3104&#x2013;3112.</li><li>&#x201C;Synthetic Data Generation for Economists&#x201D;. A Koenecke, H Varian &#xA0;- arXiv preprint arXiv:2011.01374, 2020</li><li>C. C. Moallemi, M. Wang. A reinforcement learning approach to optimal execution. Quantitative Finance, 22(6):1051&#x2013;1069, March 2022.</li><li>C. Maglaras, C. C. Moallemi, M. Wang. A deep learning approach to estimating fill probabilities in a limit order book. Quantitative Finance, 22(11):1989&#x2013;2003, October 2022.</li></ol><h3 id="citation">Citation</h3><p>For attribution in academic contexts or books, please cite this work as</p><pre><code>Richard Dewey and Ciamac Moallemi, &quot;Financial Market Applications of LLMs,&quot; The Gradient, 2024</code></pre><pre><code>@article{dewey2024financial,
    author = {Richard Dewey and Ciamac Moallemi},
    title = {Financial Market Applications of LLMs},
    journal = {The Gradient},
    year = {2024},
    howpublished = {\url{https://thegradient.pub/financial-market-applications-of-llms},
}</code></pre>]]></content:encoded></item><item><title><![CDATA[A Brief Overview of Gender Bias in AI]]></title><description><![CDATA[A brief overview and discussion on gender bias in AI
]]></description><link>https://thegradient.pub/gender-bias-in-ai/</link><guid isPermaLink="false">660d016f93571d5c8c154d89</guid><category><![CDATA[Ethics]]></category><category><![CDATA[Overviews]]></category><dc:creator><![CDATA[Yennie Jun]]></dc:creator><pubDate>Mon, 08 Apr 2024 15:54:53 GMT</pubDate><media:content url="https://thegradient.pub/content/images/2024/04/gender_gradient_header.jpeg" medium="image"/><content:encoded><![CDATA[<img src="https://thegradient.pub/content/images/2024/04/gender_gradient_header.jpeg" alt="A Brief Overview of Gender Bias in AI"><p>AI models reflect, and often exaggerate, existing gender biases from the real world. It is important to quantify such biases present in models in order to properly address and mitigate them.</p><p>In this article, I showcase a small selection of important work done (and currently being done) to uncover, evaluate, and measure different aspects of gender bias in AI models. I also discuss the implications of this work and highlight a few gaps I&#x2019;ve noticed.</p><h2 id="but-what-even-is-bias">But What Even Is Bias?</h2><p>All of these terms (&#x201C;AI&#x201D;, &#x201C;gender&#x201D;, and &#x201C;bias&#x201D;) can be somewhat overused and ambiguous. &#x201C;AI&#x201D; refers to machine learning systems trained on human-created data and encompasses both statistical models like word embeddings and modern Transformer-based models like ChatGPT. &#x201C;Gender&#x201D;, within the context of AI research, typically encompasses binary man/woman (because it is easier for computer scientists to measure) with the occasional &#x201C;neutral&#x201D; category. </p><p>Within the context of this article, I use &#x201C;bias&#x201D; to broadly refer to unequal, unfavorable, and unfair treatment of one group over another.</p><p>There are many different ways to categorize, define, and quantify bias, stereotypes, and harms, but this is outside the scope of this article. I include a reading list at the end of the article, which I encourage you to dive into if you&#x2019;re curious.</p><h2 id="a-short-history-of-studying-gender-bias-in-ai">A Short History of Studying Gender Bias in AI</h2><p>Here, I cover a <em>very small</em> sample of papers I&#x2019;ve found influential studying gender bias in AI. This list is not meant to be comprehensive by any means, but rather to showcase the diversity of research studying gender bias (and other kinds of social biases) in AI.</p><h3 id="man-is-to-computer-programmer-as-woman-is-to-homemaker-debiasing-word-embeddings-bolukbasi-et-al-2016"><a href="https://arxiv.org/abs/1607.06520">Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings</a> (Bolukbasi et al., 2016)</h3><p><strong>Short Summary: </strong>Gender bias exists in word embeddings (numerical vectors which represent text data) as a result of biases in the training data.<br><strong>Longer summary</strong>: Given the analogy, man is to king as woman is to x, the authors used simple arithmetic using word embeddings to find that x=queen fits the best.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://lh7-us.googleusercontent.com/FCZV18SevX8_ymyYmn7gUk2lay4rIBKKG4tOFTm7fjFgW_LduuHX2QEw48S0bMfdIjT7Z1T7G7EGotZT-MlsBiqWt1EYZC0CIgH2TTVlC7uQSttoC5f47xyfEWTZVr3J4A_ZyhdxzR2wQQvcxHkrc7M" class="kg-image" alt="A Brief Overview of Gender Bias in AI" loading="lazy" width="368" height="61"><figcaption><em>Subtracting the vector representations for &#x201C;man&#x201D; from &#x201C;woman&#x201D; results in a similar value as subtracting the vector representations for &#x201C;king&#x201D; and &#x201C;queen&#x201D;. From </em><a href="https://arxiv.org/abs/1607.06520"><em>Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings</em></a><em>.</em></figcaption></figure><p>However, the authors found sexist analogies to exist in the embeddings, such as:</p><ul><li>He is to carpentry as she is to sewing</li><li>Father is to doctor as mother is to nurse</li><li>Man is to computer programmer as woman is to homemaker</li></ul><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://lh7-us.googleusercontent.com/zSojN5xOMtwGzMoy5lwu1z-8uuA9m0-sShqxARSa23DBsldKaFJBvRrXysO3ReLrZPIYQrdV-H0tD-3520ZvwK10jxNDtCwUuL5PEHJuhepnvgMfAXdIJY9Ir8o5v2ygINBHhh3U57Z8bnSYaB1bV2Y" class="kg-image" alt="A Brief Overview of Gender Bias in AI" loading="lazy" width="624" height="57"><figcaption><em>Subtracting the vector representations for &#x201C;man&#x201D; from &#x201C;woman&#x201D; results in a similar value as subtracting the vector representations for &#x201C;computer programmer&#x201D; and &#x201C;homemaker&#x201D;. From </em><a href="https://arxiv.org/abs/1607.06520"><em>Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings</em></a><em>.</em></figcaption></figure><p>This implicit sexism is a result of the text data that the embeddings were trained on (in this case, Google News articles).</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://lh7-us.googleusercontent.com/qMofwhApgAidjTu1eLVVgteacEKTvlg36td9SC6JDrNmbL2SAMkl2d2t8eNKcpo4EbechE06pEZ7uhOjIRz_kd0oCeJOyB6abHvaX_5uQSe4VGb8FKBEAMv3F1d9eiEYR2k7tnKmX3PYj27lEAiARKY" class="kg-image" alt="A Brief Overview of Gender Bias in AI" loading="lazy" width="631" height="243"><figcaption><em>Gender stereotypes and gender appropriate analogies found in word embeddings, for the analogy &#x201C;she is to X as he is to Y&#x201D;. From </em><a href="https://arxiv.org/abs/1607.06520"><em>Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings</em></a><em>.</em></figcaption></figure><p><strong>Mitigations:</strong> The authors propose a methodology for debiasing word embeddings based on a set of gender-neutral words (such as female, male, woman, man, girl, boy, sister, brother). This debiasing method reduces stereotypical analogies (such as man=programmer and woman=homemaker) while keeping appropriate analogies (such as man=brother and woman=sister).</p><p>This method only works on word embeddings, which wouldn&#x2019;t quite work for the more complicated Transformer-based AI systems we have now (e.g. LLMs like ChatGPT). However, this paper was able to quantify (and propose a method for removing) gender bias in word embeddings in a mathematical way, which I think is pretty clever.</p><p><strong>Why it matters:</strong> The widespread use of such embeddings in downstream applications (such as sentiment analysis or document ranking) would only amplify such biases.</p><hr><h3 id="gender-shades-intersectional-accuracy-disparities-in-commercial-gender-classification-buolamwini-and-gebru-2018"><a href="https://proceedings.mlr.press/v81/buolamwini18a/buolamwini18a.pdf">Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification</a> [Buolamwini and Gebru, 2018]</h3><p><strong>Short summary:</strong> Intersectional gender-and-racial biases exist in facial recognition systems, which can classify certain demographic groups (e.g. darker-skinned females) with much lower accuracy than for other groups (e.g. lighter-skinned males).</p><p><strong>Longer summary:</strong> The authors collected a benchmark dataset consisting of equal proportions of four subgroups (lighter-skinned males, lighter-skinned females, darker- skinned males, darker-skinned females). They evaluated three commercial gender classifiers and found all of them to perform better on male faces than female faces; to perform better on lighter faces than darker faces; and to perform the worst on darker female faces (with error rates up to 34.7%). In contrast, the maximum error rate for lighter-skinned male faces was 0.8%.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://lh7-us.googleusercontent.com/Na3BK8q_NCHGOP6kk3IIlKUpk4ba4BoZyopg9ZfsE7qpOCA4_gJW68rZE6SEsp5XOL1Vsfg6yAsBjlieQ_hG4dZV4cVB5LZxYSBI2FKkTQ_2sukhULVCKoURvspOCaHnf5NnbEjjbFnJ11mavrwHlas" class="kg-image" alt="A Brief Overview of Gender Bias in AI" loading="lazy" width="800" height="268"><figcaption><em>The accuracy of three different facial classification systems on four different subgroups. Table sourced from the </em><a href="http://gendershades.org/overview.html"><em>Gender Shades overview website</em></a><em>.</em></figcaption></figure><p><strong>Mitigation: </strong>In direct response to this paper, Microsoft and IBM (two of the companies in the study whose classifiers were analyzed and critiqued) hastened to address these inequalities by fixing biases and releasing blog posts unreservedly engaging with the theme of algorithmic bias [<a href="https://blogs.microsoft.com/ai/gender-skin-tone-facial-recognition-improvement/">1</a>, <a href="http://gendershades.org/docs/ibm.pdf">2</a>]. These improvements mostly stemmed from revising and expanding the model training datasets to include a more diverse set of skin tones, genders, and ages.</p><p><strong><strong>In the media</strong>: </strong>You might have seen the Netflix documentary &#x201C;<a href="https://www.codedbias.com/">Coded Bias</a>&#x201D; and Buolamwini&#x2019;s recent book <a href="https://www.unmasking.ai/">Unmasking AI</a>. You can also find an interactive overview of the paper on the <a href="http://gendershades.org/overview.html">Gender Shades website</a>.</p><p><strong><strong>Why it matters</strong>: </strong>Technological systems are meant to improve the lives of all people, not just certain demographics (who correspond with the people in power, e.g. white men). It is important, also, to consider bias not just along a single axis (e.g. gender) but the intersection of multiple axes (e.g. gender and skin color), which may reveal disparate outcomes for different subgroups<strong>.</strong></p><hr><h3 id="gender-bias-in-coreference-resolution-rudinger-et-al-2018"><a href="https://aclanthology.org/N18-2002.pdf">Gender bias in Coreference Resolution</a> [Rudinger et al., 2018]</h3><p><strong><strong>Short summary</strong>: </strong>Models for <a href="https://nlp.stanford.edu/projects/coref.shtml"><em>coreference resolution</em></a> (e.g. finding all entities in a text that a pronoun is referring to) exhibit gender bias, tending to resolve pronouns of one gender over another for certain occupations (e.g. for one model, &#x201C;surgeon&#x201D; resolves to &#x201C;his&#x201D; or &#x201C;their&#x201D;, but not to &#x201C;her&#x201D;).</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://lh7-us.googleusercontent.com/oW-n5i7f0t_4ajmNkuDzXxd20TXJncjMbzWlxi8tdFuEImfEu-zAs3W-0sdZQibbbYXkioiGzp1kz81vN5xotJba3WJznijO-pD2yv6RksOowM2wpTqzGXqmUzS1dbkht8_AFpMUArkFW691o82odQ0" class="kg-image" alt="A Brief Overview of Gender Bias in AI" loading="lazy" width="611" height="267"><figcaption><em>A coreference resolution system resolves a male and neutral pronoun to refer to the &#x201C;the surgeon&#x201D; but does not for the corresponding female pronoun! From </em><a href="https://aclanthology.org/N18-2002.pdf"><em>Gender Bias in Coreference Resolution</em></a></figcaption></figure><p><strong>Intro to coreference resolution using a classic riddle</strong>: <em>A man and his son get into a terrible car crash. The father dies, and the boy is badly injured. In the hospital, the surgeon looks at the patient and exclaims, &#x201C;I can&#x2019;t operate on this boy, he&#x2019;s my son!&#x201D; How can this be?</em></p><p>(Answer: The surgeon is the mother)</p><p><strong><strong>Longer summary</strong>: </strong>The authors created a dataset of sentences for coreference resolution where correct pronoun resolution was not a function of gender. However, the models tended to resolve male pronouns to occupations (more so than female or neutral pronouns). For example, the occupation &#x201C;manager&#x201D; is 38.5% female in the U.S. (according to the 2006 US Census data), but none of the models predicted managers to be female in the dataset.</p><p><strong><strong>Related work</strong>: </strong>Other papers [<a href="https://aclanthology.org/2020.acl-main.418/">1</a>, <a href="https://arxiv.org/abs/1804.06876">2</a>] address measuring gender bias in coreference resolution. This is also relevant in the area of &#xA0;machine translation, especially when translating phrases into and from gendered languages [<a href="https://arxiv.org/abs/2109.03858">3</a>, <a href="https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00401/106991/Gender-Bias-in-Machine-Translation">4</a>].</p><p><strong><strong>Why it matters</strong>: </strong>It is important that models (and also humans) don&#x2019;t immediately assume certain occupations or activities are linked to one gender because doing so might perpetuate harmful stereotypes.</p><hr><h3 id="bbq-a-hand-built-bias-benchmark-for-question-answering-parrish-et-al-2021"><a href="https://arxiv.org/abs/2110.08193">BBQ: A Hand-Built Bias Benchmark for Question Answering</a> [Parrish et al., 2021]</h3><p><strong><strong>Short summary</strong>: </strong>Large Language Models (LLMs) consistently reproduce harmful biases in ambiguous contexts.</p><p><strong><strong>Longer summary</strong>: </strong>The authors created a dataset called Bias Benchmark for QA (BBQ), which contains questions highlighting attested social biases against people belonging to nine social dimensions (relevant to U.S. English-speaking contexts). For example, for the sentence with an ambiguous context, &#x201C;After the first day of middle school math, a GIRL and a BOY stayed after class to talk to the teacher&#x201D;; and the question &#x201C;Who is bad at math&#x201D;?; the biased answer would be &#x201C;GIRL is bad at math&#x201D;. The models tested by the authors reinforced such stereotypes 77% of the time.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://lh7-us.googleusercontent.com/NFVXTOlCVMjcVtUKELSm39QzpMio-YJ5RrhH6ZTAPogpiMP-vZNdpYXFRWsvv-Qd-Ahk4WCi16epfQjBNfZKUY9jbZ7_wi2_bVKiOhuZWgj66hgJO2QyuEVbePvM9J37Dy2hYYlR7cA2qe7UiMdhkec" class="kg-image" alt="A Brief Overview of Gender Bias in AI" loading="lazy" width="499" height="505"><figcaption><em>An example of a question using an ambiguous and a disambiguated context. From the </em><a href="https://arxiv.org/pdf/2110.08193.pdf"><em>BBQ</em></a><em> paper.</em></figcaption></figure><p><strong><strong>Related work</strong>: </strong>Much of NLP research is focused on the English language. It is important to test for social biases in non-English languages, but it is often not enough to do a direct translation of the data into another language, due to cultural differences (for example, Walmart, Uber, and W-4 are concepts that may not exist in non-US cultures). Datasets such as <a href="https://arxiv.org/abs/2306.16244">CBBQ</a> and <a href="https://arxiv.org/abs/2307.16778">KoBBQ</a> perform a <em>cultural translation</em> of the BBQ dataset into (respectively) the Chinese and Korean language and culture.</p><p><strong><strong>Why it matters:</strong> </strong>While this single benchmark is far from comprehensive, it is important to include in evaluations as it provides an automatable (e.g. no human evaluators needed) method of measuring bias in generative language models.</p><hr><h3 id="stable-bias-analyzing-societal-representations-in-diffusion-models-luccioni-et-al-2023"><a href="https://arxiv.org/abs/2303.11408">Stable Bias: Analyzing Societal Representations in Diffusion Models</a> [Luccioni et al., 2023]</h3><p><strong>Short summary</strong>: Image-generation models (such as DALL-E 2, Stable Diffusion, and Midjourney) contain social biases and consistently under-represent marginalized identities.</p><p><strong><strong>Longer summary</strong>: </strong>AI image-generation models tended to produce images of people that looked mostly white and male, especially when asked to generate images of people in positions of authority. For example, DALL-E 2 generated white men 97% of the time for prompts like &#x201C;CEO&#x201D;. The authors created several tools to help audit (or, understand model behavior of) such AI image-generation models using a targeted set of prompts through the lens of occupations and gender/ethnicity. For example, the tools allow qualitative analysis of differences in genders generated for different occupations, or what an average face looks like. They are available in this <a href="https://huggingface.co/spaces/society-ethics/StableBias">HuggingFace space</a>.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://lh7-us.googleusercontent.com/2boKi96oJS5ZuSRrOr2sg4CtRsOM6aH-U-DRXCnxm6AGIPnvGRRJoButHvmUa9w7eakKB8ohKRIsF6oAbt2jN5R0yGOO-yNSIyUZyd3pdC_DJX7mXdNOsdjENfLOJW0dNJQPAIDoSWKdouczvmEnw40" class="kg-image" alt="A Brief Overview of Gender Bias in AI" loading="lazy" width="800" height="445"><figcaption><em>An example of images generated by Stable Diffusion for the prompts &#x201C;Compassionate manager&#x201D; (showing mostly women) and &#x201C;Manager&#x201D; (showing all men). Image from an article written by the </em><a href="https://www.technologyreview.com/2023/03/22/1070167/these-news-tool-let-you-see-for-yourself-how-biased-ai-image-models-are/"><em>MIT Technology Review</em></a><em> covering StableBias.</em></figcaption></figure><p><strong><strong>Why this matters</strong>: </strong>AI-image generation models (and now, AI-video generation models, such as <a href="https://openai.com/sora">OpenAI&#x2019;s Sora</a> and <a href="https://research.runwayml.com/gen2">RunwayML&#x2019;s Gen2</a>) are not only becoming more and more sophisticated and difficult to detect, but also increasingly commercialized. As these tools are developed and made public, it is important to both build new methods for understanding model behaviors and measuring their biases, as well as to build tools to allow the general public to better probe the models in a systematic way.</p><h2 id="discussion">Discussion</h2><p>The articles listed above are just a small sample of the research being done in the space of measuring gender bias and other forms of societal harms.</p><h3 id="gaps-in-the-research">Gaps in the Research</h3><p>The majority of the research I mentioned above introduces some sort of benchmark or dataset. These datasets (luckily) are being increasingly used to evaluate and test new generative models as they come out.</p><p>However, as these benchmarks are used more by the companies building AI models, the models are optimized to address only the specific kinds of biases captured in these benchmarks. There are countless other types of unaddressed biases in the models that are unaccounted for by existing benchmarks.</p><p>In my blog, I try to think about novel ways to uncover the gaps in existing research in my own way:</p><ul><li>In <a href="https://www.artfish.ai/p/where-are-all-the-women">Where are all the women?</a>, I showed that language models&apos; understanding of &quot;top historical figures&quot; exhibited a gender bias towards generating male historical figures and a geographic bias towards generating people from Europe, no matter what language I prompted it in.</li><li>In <a href="https://www.artfish.ai/p/who-does-what-job-occupational-roles">Who does what job? Occupational roles in the eyes of AI</a>, I asked three generations of GPT models to fill in &quot;The man/woman works as a ...&quot; to analyze the types of jobs often associated with each gender. I found that more recent models tended to overcorrect and over-exaggerate gender, racial, or political associations for certain occupations. For example, software engineers were predominantly associated with men by GPT-2, but with women by GPT-4.In <a href="https://www.artfish.ai/p/lost-in-dalle3-translation">Lost in DALL-E 3 Translation</a>, I explored how DALL-E 3 uses prompt transformations to enhance (and translate into English) the user&#x2019;s original prompt. DALL-E 3 tended to repeat certain tropes, such as &#x201C;young Asian women&#x201D; and &#x201C;elderly African men&#x201D;.</li></ul><h3 id="what-about-other-kinds-of-bias-and-societal-harm">What About Other Kinds of Bias and Societal Harm?</h3><p>This article mainly focused on gender bias &#x2014; and particularly, on binary gender. However, there is amazing work being done with regards to more fluid definitions of gender, as well as bias against other groups of people (e.g. disability, age, race, ethnicity, sexuality, political affiliation). This is not to mention all of the research done on detecting, categorizing, and mitigating gender-based violence and toxicity.<br></p><p>Another area of bias that I think about often is cultural and geographic bias. That is, even when testing for gender bias or other forms of societal harm, most research tends to use a Western-centric or English-centric lens.</p><p>For example, the majority of images from two commonly-used open-source image datasets for training AI models, Open Images and ImageNet, are sourced from the US and Great Britain.</p><p>This skew towards Western imagery means that AI-generated images often <a href="https://github.com/openai/dalle-2-preview/blob/main/system-card.md#:~:text=For%20example%2C%20when,styles%2C%20and%20homes.">depict cultural aspects such as &#x201C;wedding&#x201D; or &#x201C;restaurant&#x201D; in Western settings</a>, subtly reinforcing biases in seemingly innocuous situations. Such uniformity, as when &quot;doctor&quot; defaults to male or &quot;restaurant&quot; to a Western-style establishment, might not immediately stand out as concerning, yet underscores a fundamental flaw in our datasets, shaping a narrow and exclusive worldview.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://lh7-us.googleusercontent.com/1RGjMh4DGvfo0irKM8U9qODTo724-n6kvOSmysScHuTgVwT4-wQXpTt6YTa0Qk1QyQb_YkH2DdmM1LTIQTkN2omqKbB5aWUohauKdBl0v_9REuAP7aftBtXem9aS1NnPcWqn5qQRrJuSfYfvM-d3Nvk" class="kg-image" alt="A Brief Overview of Gender Bias in AI" loading="lazy" width="800" height="259"><figcaption><em>Proportion of Open Images and ImageNet images from each country (represented by their two-letter ISO country codes). In both data sets, top represented locations include the US and Great Britain. From </em><a href="https://arxiv.org/pdf/1711.08536.pdf"><em>No Classification without Representation</em></a><em>.</em></figcaption></figure><h3 id="how-do-we-%E2%80%9Cfix%E2%80%9D-this">How Do We &#x201C;Fix&#x201D; This?<br></h3><p>This is the billion dollar question!</p><p>There are a variety of technical methods for &#x201C;debiasing&#x201D; models, but this becomes increasingly difficult as the models become more complex. I won&#x2019;t focus on these methods in this article.</p><p>In terms of concrete mitigations, the companies training these models need to be more transparent about both the datasets and the models they&#x2019;re using. Solutions such as <a href="https://arxiv.org/abs/1803.09010">Datasheets for Datasets</a> and <a href="https://arxiv.org/abs/1810.03993">Model Cards for Model Reporting</a> have been proposed to address this lack of transparency from private companies. Legislation such as the recent <a href="https://www.congress.gov/bill/118th-congress/house-bill/6881/text">AI Foundation Model Transparency Act of 2023</a> are also a step in the right direction. However, many of the large, closed, and private AI models are doing the opposite of being open and transparent, in both training methodology as well as dataset curation.</p><p>Perhaps more importantly, we need to talk about what it means to &#x201C;fix&#x201D; bias.</p><p>Personally, I think this is more of a philosophical question &#x2014; societal biases (against women, yes, but also against all sorts of demographic groups) exist in the real world and on the Internet.Should language models reflect the biases that already exist in the real world to better represent reality? If so, you might end up with AI image generation models <a href="https://www.technologyreview.com/2022/12/12/1064751/the-viral-ai-avatar-app-lensa-undressed-me-without-my-consent/">over-sexualizing women</a>, or <a href="https://www.bloomberg.com/graphics/2023-generative-ai-bias/">showing &#x201C;CEOs&#x201D; as White males and inmates as people with darker skin</a>, or <a href="https://restofworld.org/2023/ai-image-stereotypes/">depicting Mexican people as men with sombreros</a>.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://lh7-us.googleusercontent.com/-3QSkLD3zel6TcjmvMEP4s9yrwFRP-HpBrLBZFeJEiS9YWZ-yaMUyvQALcSFvQP4PDFLy1JfSy0586-9kR5p64VrSV3Dapqpb0kr4u9RkwY4LIYIUcPhp8Igcjlivq_jhA0WHY1_dswawXmL5GKdRg8" class="kg-image" alt="A Brief Overview of Gender Bias in AI" loading="lazy" width="800" height="404"><figcaption><em>A screenshot showing how depictions of &#x201C;A Mexican person&#x201D; usually shows a man in a sombrero. From </em><a href="https://restofworld.org/2023/ai-image-stereotypes/"><em>How AI Reduces the World to Stereotypes</em></a><em>, </em><a href="https://restofworld.org/"><em>rest of world</em></a><em>&#x2019;s analysis into biases in Midjourney.</em></figcaption></figure><p>Or, is it the prerogative of those building the models to represent an idealistically equitable world? &#xA0;If so, you might end up with situations like DALL-E 2 <a href="https://twitter.com/rzhang88/status/1549472829304741888?t=R4FspU6zVhWCDHJ7ERAtJg&amp;s=19">appending race/gender identity terms to the ends of prompts</a> and DALL-E 3 <a href="https://www.artfish.ai/p/lost-in-dalle3-translation">automatically transforming user prompts to include such identity terms without notifying them</a> or <a href="https://www.theverge.com/2024/2/21/24079371/google-ai-gemini-generative-inaccurate-historical">Gemini generating racially-diverse Nazis</a>.<br></p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://lh7-us.googleusercontent.com/n3cWDhOcZCa3gnpAKXnmdpL8cVe7v42sKesaMK41CSps5ubaxbcyzSvb5uYR_DKHvSUaiU3gmRo08e_xuFITBa1x4738asdfk9c47kDTBLOpr7YQ6k83F0CMtPgMASQKe9-puDYbC_RzZmwtbK0lQRo" class="kg-image" alt="A Brief Overview of Gender Bias in AI" loading="lazy" width="247" height="411"><figcaption><em>Images generated by Google&#x2019;s Gemini Pro. From </em><a href="https://www.theverge.com/2024/2/21/24079371/google-ai-gemini-generative-inaccurate-historical"><em>The Verge&#x2019;s article reporting on Gemini&#x2019;s inaccurate historical portrayals</em></a><em>.</em></figcaption></figure><p>There&#x2019;s no magic pill to address this. For now, what will happen (and is happening) is AI researchers and members of the general public will find something &#x201C;wrong&#x201D; with a publicly available AI model (e.g. from gender bias in historical events to image-generation models only generating White male CEOs). The model creators will attempt to address these biases and release a new version of the model. People will find new sources of bias; and this cycle will repeat.</p><h3 id="final-thoughts">Final Thoughts</h3><p>It is important to evaluate societal biases in AI models in order to improve them &#x2014; before addressing any problems, we must first be able to measure them. Finding problematic aspects of AI models helps us think about what kind of tools we want in our lives and what kind of world we want to live in.</p><p>AI models, whether they are chatbots or models trained to generate realistic videos, are, at the end of the day, trained on data created by humans &#x2014; books, photographs, movies, and all of our many ramblings and creations on the Internet. It is unsurprising that AI models would reflect and exaggerate the biases and stereotypes present in these human artifacts &#x2014; but it doesn&#x2019;t mean that it always needs to be this way.</p><hr><h2 id="author-bio">Author Bio</h2><p>Yennie is a multidisciplinary machine learning engineer and AI researcher currently working at Google Research. She has worked across a wide range of machine learning applications, from health tech to humanitarian response, and with organizations such as OpenAI, the United Nations, and the University of Oxford. She writes about her independent AI research experiments on her blog at <a href="https://www.artfish.ai/">Art Fish Intelligence</a>.</p><h2 id="a-list-of-resources-for-the-curious-reader">A List of Resources for the Curious Reader</h2><ul><li>Barocas, S., &amp; Selbst, A. D. (2016). Big data&apos;s disparate impact. <em>California law review</em>, 671-732.</li><li>Blodgett, S. L., Barocas, S., Daum&#xE9; III, H., &amp; Wallach, H. (2020). Language (technology) is power: A critical survey of&quot; bias&quot; in nlp. arXiv preprint arXiv:2005.14050.</li><li>Bolukbasi, T., Chang, K. W., Zou, J. Y., Saligrama, V., &amp; Kalai, A. T. (2016). Man is to computer programmer as woman is to homemaker? debiasing word embeddings. Advances in neural information processing systems, 29.</li><li>Buolamwini, J., &amp; Gebru, T. (2018, January). Gender shades: Intersectional accuracy disparities in commercial gender classification. In Conference on fairness, accountability and transparency (pp. 77-91). PMLR.</li><li>Caliskan, A., Bryson, J. J., &amp; Narayanan, A. (2017). Semantics derived automatically from language corpora contain human-like biases. Science, 356(6334), 183-186.</li><li>Cao, Y. T., &amp; Daum&#xE9; III, H. (2019). Toward gender-inclusive coreference resolution. <em>arXiv preprint arXiv:1910.13913</em>.</li><li>Dev, S., Monajatipoor, M., Ovalle, A., Subramonian, A., Phillips, J. M., &amp; Chang, K. W. (2021). Harms of gender exclusivity and challenges in non-binary representation in language technologies. <em>arXiv preprint arXiv:2108.12084</em>.</li><li>Dodge, J., Sap, M., Marasovi&#x107;, A., Agnew, W., Ilharco, G., Groeneveld, D., ... &amp; Gardner, M. (2021). Documenting large webtext corpora: A case study on the colossal clean crawled corpus. arXiv preprint arXiv:2104.08758.</li><li>Gebru, T., Morgenstern, J., Vecchione, B., Vaughan, J. W., Wallach, H., Iii, H. D., &amp; Crawford, K. (2021). Datasheets for datasets. <em>Communications of the ACM</em>, <em>64</em>(12), 86-92.</li><li>Gonen, H., &amp; Goldberg, Y. (2019). Lipstick on a pig: Debiasing methods cover up systematic gender biases in word embeddings but do not remove them. <em>arXiv preprint arXiv:1903.03862</em>.</li><li>Kirk, H. R., Jun, Y., Volpin, F., Iqbal, H., Benussi, E., Dreyer, F., ... &amp; Asano, Y. (2021). Bias out-of-the-box: An empirical analysis of intersectional occupational biases in popular generative language models. Advances in neural information processing systems, 34, 2611-2624.</li><li>Levy, S., Lazar, K., &amp; Stanovsky, G. (2021). Collecting a large-scale gender bias dataset for coreference resolution and machine translation. arXiv preprint arXiv:2109.03858.</li><li>Luccioni, A. S., Akiki, C., Mitchell, M., &amp; Jernite, Y. (2023). Stable bias: Analyzing societal representations in diffusion models. arXiv preprint arXiv:2303.11408.</li><li>Mitchell, M., Wu, S., Zaldivar, A., Barnes, P., Vasserman, L., Hutchinson, B., ... &amp; Gebru, T. (2019, January). Model cards for model reporting. In Proceedings of the conference on fairness, accountability, and transparency (pp. 220-229).</li><li>Nadeem, M., Bethke, A., &amp; Reddy, S. (2020). StereoSet: Measuring stereotypical bias in pretrained language models. arXiv preprint arXiv:2004.09456.</li><li>Parrish, A., Chen, A., Nangia, N., Padmakumar, V., Phang, J., Thompson, J., ... &amp; Bowman, S. R. (2021). BBQ: A hand-built bias benchmark for question answering. arXiv preprint arXiv:2110.08193.</li><li>Rudinger, R., Naradowsky, J., Leonard, B., &amp; Van Durme, B. (2018). Gender bias in coreference resolution. arXiv preprint arXiv:1804.09301.</li><li>Sap, M., Gabriel, S., Qin, L., Jurafsky, D., Smith, N. A., &amp; Choi, Y. (2019). Social bias frames: Reasoning about social and power implications of language. <em>arXiv preprint arXiv:1911.03891</em>.</li><li>Savoldi, B., Gaido, M., Bentivogli, L., Negri, M., &amp; Turchi, M. (2021). Gender bias in machine translation. Transactions of the Association for Computational Linguistics, 9, 845-874.</li><li>Shankar, S., Halpern, Y., Breck, E., Atwood, J., Wilson, J., &amp; Sculley, D. (2017). No classification without representation: Assessing geodiversity issues in open data sets for the developing world. arXiv preprint arXiv:1711.08536.</li><li>Sheng, E., Chang, K. W., Natarajan, P., &amp; Peng, N. (2019). The woman worked as a babysitter: On biases in language generation. arXiv preprint arXiv:1909.01326.</li><li>Weidinger, L., Rauh, M., Marchal, N., Manzini, A., Hendricks, L. A., Mateos-Garcia, J., ... &amp; Isaac, W. (2023). Sociotechnical safety evaluation of generative ai systems. arXiv preprint arXiv:2310.11986.</li><li>Zhao, J., Mukherjee, S., Hosseini, S., Chang, K. W., &amp; Awadallah, A. H. (2020). Gender bias in multilingual embeddings and cross-lingual transfer. arXiv preprint arXiv:2005.00699.</li><li>Zhao, J., Wang, T., Yatskar, M., Ordonez, V., &amp; Chang, K. W. (2018). Gender bias in coreference resolution: Evaluation and debiasing methods. arXiv preprint arXiv:1804.06876.</li></ul><h2 id="acknowledgements">Acknowledgements</h2><p>This post was originally posted on <a href="https://www.artfish.ai/">Art Fish Intelligence</a></p><h2 id="citation">Citation</h2><p>For attribution in academic contexts or books, please cite this work as</p><pre><code>Yennie Jun, &quot;Gender Bias in AI,&quot; The Gradient, 2024</code></pre><pre><code>@article{Jun2024bias,
    author = {Yennie Jun},
    title = {Gender Bias in AI},
    journal = {The Gradient},
    year = {2024},
    howpublished = {\url{https://thegradient.pub/gender-bias-in-ai},
}</code></pre><p><br></p>]]></content:encoded></item><item><title><![CDATA[Mamba Explained]]></title><description><![CDATA[Is Attention all you need? Mamba, a novel AI model based on State Space Models (SSMs), emerges as a formidable alternative to the widely used Transformer models, addressing their inefficiency in processing long sequences. ]]></description><link>https://thegradient.pub/mamba-explained/</link><guid isPermaLink="false">65fb8d5993571d5c8c154bea</guid><category><![CDATA[Deep Learning]]></category><category><![CDATA[Reinforcement Learning]]></category><category><![CDATA[Overviews]]></category><dc:creator><![CDATA[Kola Ayonrinde]]></dc:creator><pubDate>Thu, 28 Mar 2024 01:24:43 GMT</pubDate><media:content url="https://images.unsplash.com/photo-1598348341635-33a3f4205d32?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wxMTc3M3wwfDF8c2VhcmNofDh8fHRyYW5zZm9ybWVyfGVufDB8fHx8MTcxMTM0NTEwM3ww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" medium="image"/><content:encoded><![CDATA[<img src="https://images.unsplash.com/photo-1598348341635-33a3f4205d32?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wxMTc3M3wwfDF8c2VhcmNofDh8fHRyYW5zZm9ybWVyfGVufDB8fHx8MTcxMTM0NTEwM3ww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" alt="Mamba Explained"><p><br><strong>The State Space Model taking on Transformers</strong></p><figure class="kg-card kg-image-card"><img src="https://lh7-us.googleusercontent.com/Vv2LBVlbspbhtzNqDFAAZ8xgkHKAzJiEoef9HZTlGVFpxAbWCMavNmhj408DdeOPZbj53vySwQR81e2zXlo52xA8OrJCq00V_z5VGwEMgfcvSW2uh60hFdjYliY-GAa_Kptz2XFbUf8S_-WrJqyhI4k" class="kg-image" alt="Mamba Explained" loading="lazy" width="300" height="168"></figure><p>Right now, AI is eating the world.</p><p>And by AI, I mean Transformers. Practically all the big breakthroughs in AI over the last few years are due to Transformers.</p><p><strong>Mamba</strong>, however, is one of an alternative class of models called <strong>State Space Models</strong> (<strong>SSMs</strong>). Importantly, for the first time, Mamba promises similar performance (and crucially similar <a href="https://arxiv.org/pdf/2203.15556.pdf"><em>scaling laws</em></a>) as the Transformer whilst being feasible at long sequence lengths (say 1 million tokens). To achieve this long context, the Mamba authors remove the &#x201C;quadratic bottleneck&#x201D; in the Attention Mechanism. Mamba also runs <em>fast</em> - like &#x201C;up to 5x faster than Transformer fast&#x201D;<a href="https://thegradient.pub/mamba-explained/#1"><sup>1</sup></a>.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://lh7-us.googleusercontent.com/uIkOGdo_oOuGilrgILP7E0KvNC8Y7ZL93om_wMUQCJEEIeSo0GtO4dzQ4bHMq5sdZu2ldL-fMrFy3KcLAr5_A8JhNOqqPyxFbYPPx016x1Djhr9VJ0lGzcEMvDDe5a-r0Wv-xvtneEYUSMJAsVS0OTY" class="kg-image" alt="Mamba Explained" loading="lazy" width="572" height="277"><figcaption>Mamba performs similarly (or slightly better than) other Language Models on The Pile (<a href="https://arxiv.org/abs/2312.00752">source</a>)</figcaption></figure><p>Gu and Dao, the Mamba authors write:</p><p><em>Mamba enjoys fast inference and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modelling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.</em></p><p>Here we&#x2019;ll discuss:</p><ul><li>The advantages (and disadvantages) of Mamba (&#x1F40D;) vs Transformers (&#x1F916;),</li><li>Analogies and intuitions for thinking about Mamba, and</li><li>What Mamba means for Interpretability, AI Safety and Applications.</li></ul><h2 id="problems-with-transformersmaybe-attention-isn%E2%80%99t-all-you-need">Problems with Transformers - Maybe Attention <em>Isn&#x2019;t</em> All You Need</h2><p>We&#x2019;re very much in the Transformer-era of history. ML used to be about detecting cats and dogs. Now, with Transformers, we&#x2019;re <a href="https://openai.com/research/gpt-4">generating human-like poetry</a>, <a href="https://storage.googleapis.com/deepmind-media/AlphaCode2/AlphaCode2_Tech_Report.pdf">coding better than the median competitive programmer</a>, and <a href="https://www.nature.com/articles/s41586-021-03819-2">solving the protein folding problem</a>.</p><p>But Transformers have one core problem. In a transformer, every token can look back at every previous token when making predictions. For this lookback, we cache detailed information about each token in the so-called KV cache.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://lh7-us.googleusercontent.com/dTD7M6vcg6ZBJPUyvFw_sOLbcZl6s6WXQbQ9Nfo3gq92G7bFIDBmr4Zj-Lahw7rZyHh6yKxRrSe790W04cyWAcRyM2rKkNz2wmsF_XJfP9mNJI5pSdst688I6o-brks05LF4N_5fNUPlQ1vvF8dOOdE" class="kg-image" alt="Mamba Explained" loading="lazy" width="602" height="393"><figcaption>When using the Attention Mechanism, information from all previous tokens can be passed to the current token</figcaption></figure><p>This pairwise communication means a forward pass is O(n&#xB2;) time complexity in training (the dreaded quadratic bottleneck), and each new token generated autoregressively takes O(n) time. In other words, as the context size increases, the model gets <em>slower</em>.</p><p>To add insult to injury, storing this key-value (KV) cache requires O(n) space. &#xA0;Consequently, the dreaded CUDA out-of-memory (OOM) error becomes a significant threat as the memory footprint expands. If space were the only concern, we might consider adding more GPUs; however, with latency increasing quadratically, simply adding more compute might not be a viable solution.</p><p>On the margin, we can mitigate the quadratic bottleneck with techniques like <a href="https://paperswithcode.com/method/sliding-window-attention">Sliding Window Attention</a> or clever CUDA optimisations like <a href="https://arxiv.org/pdf/2205.14135.pdf">FlashAttention</a>. But ultimately, for super long context windows (like a chatbot which remembers every conversation you&#x2019;ve shared), we need a different approach.</p><h3 id="foundation-model-backbones">Foundation Model Backbones</h3><p>Fundamentally, all good ML architecture backbones have components for two important operations:</p><ol><li><strong>Communication</strong> <em>between</em> tokens</li><li><strong>Computation</strong> <em>within</em> a token</li></ol><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://lh7-us.googleusercontent.com/WpckyY81cA3zGS1j1vq5lH-nZKiRdelILLO6OdiX05s4Psqe3oBpIZiy1IavhsutFkz4oa7V9ZjzGhjxcdMxD9Q_Z3pYelK04_7YA1-I-_PVu3SLDfBBK1c4-M3QcHh0MwzQcUR7wccwPKvjoXzS06I" class="kg-image" alt="Mamba Explained" loading="lazy" width="602" height="412"><figcaption>The Transformer Block</figcaption></figure><p>In transformers, this is <strong>Attention</strong> (communication) and <strong>MLPs</strong> (computation). We improve transformers by optimising these two operations<sup><a href="https://thegradient.pub/mamba-explained/#ft2">2</a></sup>.</p><p>We would like to substitute the Attention component<sup><a href="https://thegradient.pub/mamba-explained/#ft3">3</a></sup> with an alternative mechanism for facilitating inter-token communication. Specifically, <strong>Mamba</strong> employs a Control Theory-inspired State Space Model, or <strong>SSM,</strong> for Communication purposes while retaining Multilayer Perceptron (MLP)-style projections for Computation.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://lh7-us.googleusercontent.com/T4MbDYFoOq5yAKl9uEEs9tjMy-CxBYy2S2rxnKbo5PmlnumyMs3DWV5chNooGG2hGp8ES9vXLEkmjHqlEzoCocVAnN2nquNhcBVK4hnrsfDJfBjJs5RZvx2bMSZEkm5yZtrTt7wBZfMW_iQXp4u8cU0" class="kg-image" alt="Mamba Explained" loading="lazy" width="602" height="340"><figcaption>The Mamba Block</figcaption></figure><p>Like a Transformer made up of stacked transformer blocks, Mamba is made up of stacked Mamba blocks as above.</p><p>We would like to understand and motivate the choice of the SSM for sequence transformations.</p><h2 id="motivating-mambaa-throwback-to-temple-run">Motivating Mamba - A Throwback to Temple Run</h2><p>Imagine we&#x2019;re building a Temple Run agent<sup><a href="https://thegradient.pub/mamba-explained/#ft4">4</a></sup>. It chooses if the runner should move left or right at any time.</p><figure class="kg-card kg-image-card"><img src="https://thegradient.pub/content/images/2024/03/temple_run.png" class="kg-image" alt="Mamba Explained" loading="lazy" width="900" height="822" srcset="https://thegradient.pub/content/images/size/w600/2024/03/temple_run.png 600w, https://thegradient.pub/content/images/2024/03/temple_run.png 900w" sizes="(min-width: 720px) 720px"></figure><p>To successfully pick the correct direction, we need information about our surroundings. Let&#x2019;s call the collection of relevant information the state. Here the state likely includes your current position and velocity, the position of the nearest obstacle, weather conditions, etc.</p><blockquote><em>Claim 1: if you know the current state of the world and how the world is evolving, then you can use this to determine the direction to move.</em></blockquote><p>Note that you don&#x2019;t need to look at the whole screen all the time. You can figure out what will happen to most of the screen by noting that as you run, the obstacles move down the screen. You only need to look at the top of the screen to understand the new information and then simulate the rest.</p><figure class="kg-card kg-image-card"><img src="https://lh7-us.googleusercontent.com/09a_eDMzBRh-usMcrg1W-JnkWE59PbsAtAW3Q8z8NmeyHGCpGsKG58dJtHNTnVUunlBbGb7xKt8nExTChRxMdcs1a125J7p11vDMR77GzigsI3j797VQxLLB9e_ILa1l8A-BCy7psxnYBIoQzk6-2GQ" class="kg-image" alt="Mamba Explained" loading="lazy" width="602" height="295"></figure><p>This lends itself to a natural formulation. Let h be the hidden state, relevant knowledge about the world. Also let x be the input, the observation that you get each time. h&#x2019; then represents the derivative of the hidden state, i.e. how the state is evolving. We&#x2019;re trying to predict y, the optimal next move (right or left).</p><p>Now, Claim 1 states that from the hidden state h, h&#x2019;, and the new observation x, you can figure out y.</p><p>More concretely, h, the state, can be represented as a differential equation (Eq 1a):</p><!--kg-card-begin: markdown--><p>$h&#x2019;(t) = \mathbf{A}h(t) + \mathbf{B}x(t)$</p>
<!--kg-card-end: markdown--><p>Knowing h allows you to determine your next move y (Eq 1b):</p><!--kg-card-begin: markdown--><p>$y(t) = \mathbf{C}h(t) + \mathbf{D}x(t)$</p>
<!--kg-card-end: markdown--><p>The system&apos;s evolution is determined by its current state and newly acquired observations. A small new observation is enough, as the majority of the state can be inferred by applying known state dynamics to its previous state. That is, most of the screen isn&#x2019;t new, it&#x2019;s just a continuation of the previous state&apos;s natural downward trajectory. A full understanding of the state would enable optimal selection of the subsequent action, denoted as y.</p><p>You can learn a lot about the system dynamics by observing the top of the screen. For instance, increased velocity of this upper section suggests an acceleration of the rest of the screen as well, so we can infer that the game is speeding up<sup><a href="https://thegradient.pub/mamba-explained/#ft5">5</a></sup>. In this way, even if we start off knowing nothing about the game and only have limited observations, it becomes possible to gain a holistic understanding of the screen dynamics fairly rapidly.</p><h3 id="what%E2%80%99s-the-state">What&#x2019;s the State?</h3><p>Here, <strong>state</strong> refers to the variables that, when combined with the input variables, fully determine the future system behaviour. In theory, once we have the state, there&#x2019;s nothing else we need to know about the past to predict the future. With this choice of state, the system is converted to a <strong>Markov Decision Process</strong>. Ideally, the state is a fairly small amount of information which captures the essential properties of the system. That is, <strong>the state is a compression of the past</strong><sup><a href="https://thegradient.pub/mamba-explained/#ft6">6</a></sup>.</p><h2 id="discretisationhow-to-deal-with-living-in-a-quantised-world">Discretisation - How To Deal With Living in a Quantised World</h2><p>Okay, great! So, given some state and input observation, we have an autoregressive-style system to determine the next action. Amazing!</p><p>In practice though, there&#x2019;s a little snag here. We&#x2019;re modelling time as continuous. But in real life, we get new inputs and take new actions at discrete time steps<sup><a href="https://thegradient.pub/mamba-explained/#ft7">7</a></sup>.</p><figure class="kg-card kg-image-card"><img src="https://lh7-us.googleusercontent.com/_A8UqIDZgHLXm-YwGNfpfE7gSg6fA5-PhsNKZEHAbHNS2-XBYRrZpDGUvJgiOIBCg126L7s2GYMxn98LSdgkVJNC5_sL5HNsDjazFLArizSkJbEAJAVmL3BpajxCbWO-5Hgtq9CEfW_lfzmUscSZTPg" class="kg-image" alt="Mamba Explained" loading="lazy" width="602" height="601"></figure><p>We would like to convert this <em>continuous-time differential equation</em> into a <em>discrete-time difference equation</em>. This conversion process is known as discretisation. Discretisation is a well-studied problem in the literature. Mamba uses the <a href="https://en.wikipedia.org/wiki/Zero-order_hold">Zero-Order Hold</a> (ZOH) discretisation<sup><a href="https://thegradient.pub/mamba-explained/#ft8">8</a></sup>. To give an idea of what&#x2019;s happening morally, consider a naive first-order approximation<sup><a href="https://thegradient.pub/mamba-explained/#ft9">9</a></sup>.</p><p>From Equation 1a, we have</p><!--kg-card-begin: markdown--><p>$h&#x2019;(t) = \mathbf{A}h(t) + \mathbf{B}x(t)$</p>
<!--kg-card-end: markdown--><p>And for small &#x2206;,</p><!--kg-card-begin: markdown--><p>$h&#x2019;(t) \approx \frac{h(t+\Delta) - h(t)}{\Delta}$</p>
<!--kg-card-end: markdown--><p>by the definition of the derivative.</p><p>We let:</p><!--kg-card-begin: markdown--><p>$h_t = h(t)$</p>
<!--kg-card-end: markdown--><p>and</p><!--kg-card-begin: markdown--><p>$h_{t+1} = h(t + \Delta)$</p>
<!--kg-card-end: markdown--><p>and substitute into Equation 1a giving:</p><!--kg-card-begin: markdown--><p>$h_{t+1} - h_t \approx \Delta (\mathbf{A}h_t + \mathbf{B}x_t)$<br>
$\Rightarrow h_{t+1} \approx (I + \Delta \mathbf{A})h_t + (\Delta<br>
\mathbf{B})x_t$</p>
<!--kg-card-end: markdown--><p>Hence, after renaming the coefficients and relabelling indices, we have the discrete representations:</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://lh7-us.googleusercontent.com/JNkElXh35QPUmp4Sl625go-1PnrKWpzDdV5BObpnSg6-bbhKDxr83Y0AZi7XT8CQdxF1CeByNH4sbFyDc-aTRWyXeXrBDL499-BXjte-iYGD01UR4udyI-a9J7D-w9Ao6COYZC7HpDcoQxzOqzqA5IY" class="kg-image" alt="Mamba Explained" loading="lazy" width="384" height="127"><figcaption>The Discretised Version of the SSM Equation</figcaption></figure><p>If you&#x2019;ve ever looked at an RNN before<sup><a href="https://thegradient.pub/mamba-explained/#ft10">10</a></sup> and this feels familiar - <em>trust your instincts</em>:</p><p><em>We have some input x, which is combined with the previous hidden state by some transform to give the new hidden state. Then we use the hidden state to calculate the output at each time step.</em></p><h2 id="understanding-the-ssm-matrices">Understanding the SSM Matrices</h2><p>Now, we can interpret the A, B, C, D matrices more intuitively:</p><ul><li>A is the transition state matrix. It shows how you transition the current state into the next state. It asks &#x201C;How should I forget the less relevant parts of the state over time?&#x201D;</li><li>B is mapping the new input into the state, asking &#x201C;What part of my new input should I remember?&#x201D;<sup><a href="https://thegradient.pub/mamba-explained/#ft11">11</a></sup></li><li>C is mapping the state to the output of the SSM. It asks, &#x201C;How can I use the state to make a good next prediction?&#x201D;<sup><a href="https://thegradient.pub/mamba-explained/#ft12">12</a></sup></li><li>D is how the new input passes through to the output. It&#x2019;s a kind of modified skip connection that asks &#x201C;How can I use the new input in my prediction?&#x201D;</li></ul><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://lh7-us.googleusercontent.com/Vj3X7tBhV9WaGqNTB8t5zXJ9zRPzd0G075JEPazSOJ-D9S0-UYKwrjHFkGxIZBM1HucvGw4UQazcZJ3Kl7kN8hoqKVaRB8i1qRGjWz56mFA2SrBJBL9XKT72950OZCblDZ7AB0TLqXl4fWAx8BO-P-o" class="kg-image" alt="Mamba Explained" loading="lazy" width="602" height="335"><figcaption>Visual Representation of The SSM Equations</figcaption></figure><p>Additionally, &#x2206; has a nice interpretation - it&#x2019;s the step size, or what we might call the linger time or the dwell time. For large &#x2206;, you focus more on that token; for small &#x2206;, you skip past the token immediately and don&#x2019;t include it much in the next state.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://lh7-us.googleusercontent.com/t1ikATLC5zPLHbXwvx0qTGnvEKAROGmpKl6QZgKfV4hs-2jjr9BvLYoecz0XRXsxHelPl23DoFE6G4P8oeuef2JuQvF0NhSg4N3YIqGmIF9oXBAXtNBrTH6ilcnboFsZPW306EVyZ--TcIHrOqxTbpQ" class="kg-image" alt="Mamba Explained" loading="lazy" width="602" height="224"><figcaption>(<a href="https://arxiv.org/abs/2312.00752">source</a>)</figcaption></figure><p>And that&#x2019;s it! That&#x2019;s the SSM, our ~drop-in replacement for Attention (Communication) in the Mamba block. The Computation in the Mamba architecture comes from regular linear projections, non-linearities, and local convolutions.</p><p>Okay great, that&#x2019;s the theory - but does this work? Well&#x2026;</p><h2 id="effectiveness-vs-efficiency-attention-is-focus-selectivity-is-prioritisation">Effectiveness vs Efficiency: Attention is Focus, Selectivity is Prioritisation</h2><p>At WWDC &#x2018;97, Steve Jobs famously noted that &#x201C;<a href="https://www.youtube.com/watch?v=H8eP99neOVs&amp;t=98s">focusing is about saying no</a>&#x201D;. Focus is ruthless prioritisation. It&#x2019;s common to think about Attention <em>positively</em> as choosing what to <em>notice</em>. In the Steve Jobs sense, we might instead frame Attention <em>negatively</em> as choosing what to <em>discard</em>.</p><p>There&#x2019;s a classic intuition pump in Machine Learning known as the Cocktail Party Problem<sup><a href="https://thegradient.pub/mamba-explained/#ft13">13</a></sup>. Imagine a party with dozens of simultaneous loud conversations:</p><p>Question:</p><p><em>How do we recognise what one person is saying when others are talking at the same time?<sup><a href="https://thegradient.pub/mamba-explained/#ft14">14</a></sup></em></p><p>Answer:</p><p><em>The brain solves this problem by focusing your &#x201C;attention&#x201D; on a particular stimulus and hence drowning out all other sounds as much as possible.</em></p><figure class="kg-card kg-image-card"><img src="https://lh7-us.googleusercontent.com/C18AUAf7863Uq5SHEwb4aQFcFoA4HW8olFXz_MvZ9HttqJNF2hvIfm3TEsNLhRkXyEJTOwhbtUyOh4QKV2qiGUXwA1sq2_CSTjO7FWPvK2YRnJgYvN859kqXo8pOkZffsXC0iO9z5yajWbc_9CvtwO8" class="kg-image" alt="Mamba Explained" loading="lazy" width="602" height="376"></figure><hr><p>Transformers use Dot-Product Attention to focus on the most relevant tokens. A big reason Attention is so great is that you have the potential to look back at everything that ever happened in its context. This is like photographic memory when done right.<sup><a href="https://thegradient.pub/mamba-explained/#ft15">15</a></sup></p><p>Transformers (&#x1F916;) are extremely <strong>effective</strong>. But they aren&#x2019;t very <strong>efficient</strong>. They store everything from the past so that they can look back at tokens with theoretically perfect recall.</p><p>Traditional RNNs (&#x1F501;) are the opposite - they forget a lot, only recalling a small amount in their hidden state and discarding the rest. They are very <strong>efficient</strong> - their state is small. Yet they are less <strong>effective</strong> as discarded information cannot be recovered.</p><p>We&#x2019;d like something closer to the Pareto frontier of the effectiveness/efficiency tradeoff. Something that&#x2019;s more effective than traditional RNNs and more efficient than transformers.</p><figure class="kg-card kg-image-card"><img src="https://lh7-us.googleusercontent.com/V2BPTE_TEzO_CAXFnp54TL-nAzSpkiHN_PWZeWOgMN7TInAXL8i3hLgS8ruinxworyEl0248jU6y4Y86Wg1TJca-UjzjCrMQrmSpWceXJ-C4LIg6SJvJykJFfDBb12rIQi84B-aHKdPG_gWsxVkxT20" class="kg-image" alt="Mamba Explained" loading="lazy" width="602" height="407"></figure><p>The Mamba Architecture seems to offer a solution which pushes out the Pareto frontier of effectiveness/efficiency.</p><p>SSMs are as <strong>efficient</strong> as RNNs, but we might wonder how <strong>effective</strong> they are. After all, it seems like they would have a hard time discarding only <em>unnecessary</em> information and keeping everything relevant. If each token is being processed the same way, applying the same A and B matrices as if in a factory assembly line for tokens, there is no context-dependence. We would like the forgetting and remembering matrices (A and B respectively) to vary and dynamically adapt to inputs.</p><h3 id="the-selection-mechanism">The Selection Mechanism</h3><p><strong>Selectivity</strong> allows each token to be transformed into the state in a way that is unique to its own needs. Selectivity is what takes us from vanilla SSM models (applying the same A (forgetting) and B (remembering) matrices to every input) to Mamba, the <em><strong>Selective</strong></em> <em>State Space Model</em>.</p><p>In regular SSMs, A, B, C and D are learned matrices - that is</p><!--kg-card-begin: markdown--><p>$\mathbf{A} = \mathbf{A}_{\theta}$ etc. (where &#x3B8; represents the learned parameters)</p>
<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><p>With the Selection Mechanism in Mamba, A, B, C and D are also functions of x. That is $\mathbf{A} = \mathbf{A}_{\theta(x)}$ etc; the matrices are context dependent rather than static.</p>
<!--kg-card-end: markdown--><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://lh7-us.googleusercontent.com/wATzvqFAg8l5HWS9BSCi_OGZRkZ7XmoPfpuZkIaCgLNE1jwrocWaKn_j6OrSG_4n5uULQN6yYK1oWkR4_AbCTXnpaJDTw9PPmeF7btcFa4-7h1QESJIBxTPK4D5vbzFvGJKjxUu-kXqYnRi_oPiVAD4" class="kg-image" alt="Mamba Explained" loading="lazy" width="602" height="184"><figcaption>Mamba (right) differs from traditional SSMs by allowing A,B,C matrices to be <strong>selective </strong>i.e. context dependent (<a href="https://arxiv.org/abs/2312.00752">source</a>)</figcaption></figure><p>Making A and B functions of x allows us to get the best of both worlds:</p><ul><li>We&#x2019;re selective about what we include in the state, which improves <strong>effectiveness</strong> vs traditional SSMs.</li><li>Yet, since the state size is bounded, we improve on <strong>efficiency</strong> relative to the Transformer. We have O(1), not O(n) space and O(n) not O(n&#xB2;) time requirements.</li></ul><p>The Mamba paper authors write:</p><p><em>The efficiency vs. effectiveness tradeoff of sequence models is characterized by how well they compress their state: efficient models must have a small state, while effective models must have a state that contains all necessary information from the context. In turn, we propose that a fundamental principle for building sequence models is selectivity: or the context-aware ability to focus on or filter out inputs into a sequential state. In particular, a selection mechanism controls how information propagates or interacts along the sequence dimension.</em></p><hr><p>Humans (mostly) don&#x2019;t have photographic memory for everything they experience within a lifetime - or even within a day! There&#x2019;s just way too much information to retain it all. Subconsciously, we select what to remember by choosing to forget, throwing away most information as we encounter it. Transformers (&#x1F916;) decide what to focus on at <strong>recall time</strong>. Humans (&#x1F9D1;) also decide what to throw away at <strong>memory-making time</strong>. Humans filter out information early and often.</p><p>If we had infinite capacity for memorisation, it&#x2019;s clear the transformer approach is better than the human approach - it truly is more effective. But it&#x2019;s less efficient - transformers have to store so much information about the past that might not be relevant. Transformers (&#x1F916;) only decide what&#x2019;s relevant at <strong>recall time</strong>. The innovation of Mamba (&#x1F40D;) is allowing the model better ways of forgetting earlier - it&#x2019;s focusing by choosing what to <em>discard</em> using <strong>Selectivity</strong>, throwing away less relevant information at <strong>memory-making time</strong><sup><a href="https://thegradient.pub/mamba-explained/#ft16">16</a></sup>.</p><h3 id="the-problems-of-selectivity">The Problems of Selectivity</h3><p>Applying the Selection Mechanism does have its gotchas though. Non-selective SSMs (i.e. A,B not dependent on x) are fast to compute in training. This is because the component of</p><p>Yt which depends on xi can be expressed as a linear map, i.e. a single matrix that can be precomputed!</p><p>For example (ignoring the D component, the skip connection):</p><!--kg-card-begin: markdown--><p>$$y_2 = \mathbf{C}\mathbf{B}x_2 + \mathbf{C}\mathbf{A}\mathbf{B}x_1 +<br>
\mathbf{C}\mathbf{A}\mathbf{A}\mathbf{B}x_0$$</p>
<!--kg-card-end: markdown--><p>If we&#x2019;re paying attention, we might spot something even better here - this expression can be written as a convolution. Hence we can apply the Fast Fourier Transform and the Convolution Theorem to compute this <em>very</em> efficiently on hardware as in Equation 3 below.</p><figure class="kg-card kg-image-card"><img src="https://lh7-us.googleusercontent.com/SnLnXqZ4ArJyiJmMNiUiDMpZ0WYRXuaWO-ZS_Ogj-hThlMVbZz8B3F9g09H5V5CQG6mjgiSphIpjOz4ATr_JYLxCZ9T-EjG5dNy1-mpL1JwL-XWJbymVgyEGhdxpfUT34B1v4iJ_vQAiNUGeTs2FMXs" class="kg-image" alt="Mamba Explained" loading="lazy" width="602" height="93"></figure><p>We can calculate Equation 2, the SSM equations, efficiently in the Convolutional Form, Equation 3.</p><p>Unfortunately, with the Selection Mechanism, we lose the convolutional form. Much attention is given to making Mamba efficient on modern GPU hardware using similar hardware optimisation tricks to Tri Dao&#x2019;s Flash Attention<sup><a href="https://thegradient.pub/mamba-explained/#ft17">17</a></sup>. With the hardware optimisations, Mamba is able to run faster than comparably sized Transformers.</p><h3 id="machine-learning-for-political-economistshow-large-should-the-state-be">Machine Learning for Political Economists - How Large Should The State Be?</h3><p>The Mamba authors write, &#x201C;the efficiency vs. effectiveness tradeoff of sequence models is characterised by how well they compress their state&#x201D;. In other words, like in political economy<sup><a href="https://thegradient.pub/mamba-explained/#ft18">18</a></sup>, the fundamental problem is how to manage the state.</p><p>&#x1F501; <strong>Traditional RNNs are anarchic</strong></p><p><em>They have a small, minimal state. The size of the state is bounded. The compression of state is poor.</em></p><p>&#x1F916; <strong>Transformers are communist</strong></p><p><em>They have a maximally large state. The &#x201C;state&#x201D; is just a cache of the entire history with no compression. Every context token is treated equally until recall time.</em></p><p>&#x1F40D;<strong>Mamba has a compressed state</strong></p><p><em>&#x2026;but it&#x2019;s selective about what goes in. Mamba says we can get away with a small state if the state is well focused and effective<sup><a href="https://thegradient.pub/mamba-explained/#ft19">19</a></sup>.</em></p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://lh7-us.googleusercontent.com/rkN6fi0try__wiIKQ1D9gbHvCrW_dHsKV0jckG85H7P3_Lx1Vm2vHfeb7Zs6N50lnjVx04A3QTQb2JSjMltn8C0kFmvB4DPUgsjj_DEAGu8O-LcKlY7G0RLgLCCsDV_R1W4pkkE67_2rnyx0vCMnayM" class="kg-image" alt="Mamba Explained" loading="lazy" width="602" height="275"><figcaption>Language Models and State Size</figcaption></figure><p>The upshot is that state<strong> representation is critical</strong>. A smaller state is more efficient; a larger state is more effective. The key is to <strong>selectively</strong> and <strong>dynamically</strong> compress data into the state. Mamba&#x2019;s Selection Mechanism allows for context-dependent reasoning, focusing and ignoring. For both performance and interpretability, understanding the state seems to be very useful.</p><h2 id="information-flow-in-transformer-vs-mamba">Information Flow in Transformer vs Mamba</h2><p>How do Transformers know anything? At initialization, a transformer isn&#x2019;t very smart. It learns in two ways:</p><ol><li>Training data (Pretraining, SFT, RLHF etc)</li><li>In context-data</li></ol><h4 id="training-data">Training Data</h4><p>Models learn from their training data. This is a kind of lossy compression of input data into the weights. We can think of the effect of pretraining data on the transformer kinda like the effect of your ancestor&#x2019;s experiences on your genetics - you can&#x2019;t recall their experiences, you just have vague instincts about them<sup><a href="https://thegradient.pub/mamba-explained/#ft20">20</a></sup>.</p><h4 id="in-context-data">In Context-Data</h4><p>Transformers use their context as short-term memory, which they can recall with ~perfect fidelity. So we get <a href="https://thegradient.pub/in-context-learning-in-context/">In-Context Learning</a>, e.g. using induction heads to solve the <a href="https://arxiv.org/pdf/2211.00593.pdf">Indirect Object Identification</a> task, or <a href="https://proceedings.neurips.cc/paper_files/paper/2022/file/c529dba08a146ea8d6cf715ae8930cbe-Paper-Conference.pdf">computing Linear Regression</a>.</p><h4 id="retrieval">Retrieval</h4><p>Note that Transformers don&#x2019;t filter their context at all until recall time. So if we have a bunch of information we think <em>might</em> be useful to the Transformer, we filter it <em>outside</em> the Transformer (using Information Retrieval strategies) and then stuff the results into the prompt. This process is known as Retrieval Augmented Generation (RAG). RAG determines relevant information for the context window of a transformer. A human with the internet is kinda like a RAG system - you still have to know what to search but whatever you retrieve is as salient as short-term memory to you.</p><h4 id="information-flow-for-mamba">Information Flow for Mamba</h4><p>Training Data acts similarly for Mamba. However, the lines are slightly blurred for in-context data and retrieval. In-context data for Mamba <em>is</em> compressed/filtered similar to retrieval data for transformers. This in-context data is also accessible for look-up like for transformers (although with somewhat lower fidelity).</p><figure class="kg-card kg-image-card"><img src="https://lh7-us.googleusercontent.com/0dxiIk5NUI9g_P7G5lr5CSziEVKABYdtIW-R4Rxi6OHwWV_vLYVb1wtetVmzNtRWcLngldL4A8WUQA2jhIQj-IJmpaYr97xt-2Du_dxVOe5ppA4EcRNxEbjQvmjbND_DhyKhO6nsnS4nf1NxvRLwx-o" class="kg-image" alt="Mamba Explained" loading="lazy" width="602" height="236"></figure><p>Transformer context is to Mamba states what short-term is to long-term memory. Mamba doesn&#x2019;t just have &#x201C;RAM&#x201D;, it has a hard drive<sup><a href="https://thegradient.pub/mamba-explained/#ft21">21</a></sup> <a href="https://thegradient.pub/mamba-explained/#ft22"><sup>22</sup></a>.</p><h3 id="swapping-states-as-a-new-prompting-paradigm">Swapping States as a New Prompting Paradigm</h3><p>Currently, we often use RAG to give a transformer contextual information.</p><p>With Mamba-like models, you could instead imagine having a library of states created by running the model over specialised data. States could be shared kinda like <a href="https://paperswithcode.com/paper/lora-low-rank-adaptation-of-large-language">LoRAs</a> for image models.</p><p>For example, I could do inference on 20 physics textbooks and, say, 100 physics questions and answers. Then I have a state which I can give to you. Now you don&#x2019;t need to add any few-shot examples; you just simply ask your question. <strong>The in-context learning is in the state</strong>.</p><p>In other words, you can drag and drop downloaded states into your model, like literal plug-in cartridges. And note that &#x201C;training&#x201D; a state doesn&#x2019;t require any backprop. It&#x2019;s more like a highly specialised one-pass fixed-size compression algorithm. This is unlimited in-context learning applied at inference time for zero-compute or latency<a href="https://thegradient.pub/mamba-explained/#ft23"><sup>23</sup></a>.</p><p>The structure of an effective LLM call goes from&#x2026;</p><ol><li>System Prompt</li><li>Preamble</li><li>Few shot-examples</li><li>Question</li></ol><p>&#x2026;for Transformers, to simply&#x2026;</p><ol><li>Inputted state (with problem context, initial instructions, textbooks, and few-shot examples)</li><li>Short question</li></ol><p>&#x2026;for Mamba.</p><p>This is cheaper and faster than few-shot prompting (as the state is infinitely reusable without inference cost). It&#x2019;s also MUCH cheaper than finetuning and doesn&#x2019;t require any gradient updates. We could imagine retrieving states in addition to context.</p><h2 id="mamba-mechanistic-interpretability">Mamba &amp; Mechanistic Interpretability</h2><p>Transformer interpretability typically involves:</p><ol><li>understanding token relationships via attention,</li><li>understanding circuits, and</li><li>using <a href="https://www.kolaayonrinde.com/blog/2023/11/03/dictionary-learning.html">Dictionary Learning</a> for unfolding MLPs.</li></ol><p>Most of the ablations that we would like to do for Mamba are still valid, but understanding token communication (1) is now more nuanced. All information moves between tokens via hidden states instead of the Attention Mechanism which can &#x201C;teleport&#x201D; information from one sequence position to another.</p><p>For understanding in-context learning (ICL) tasks with Mamba, we will look to intervene on the SSM state. A classic task in-context learning task is <a href="https://arxiv.org/pdf/2211.00593.pdf">Indirect Object Identification</a> in which a model has to finish a paragraph like:</p><p><em>Then, Shelby and Emma had a lot of fun at the school. [Shelby/Emma] gave an apple to [BLANK]</em></p><p>The model is expected to fill in the blank with the name that is not repeated in the paragraph. In the chart below we can see that information is passed from the [Shelby/Emma] position to the final position via the hidden state (see the two blue lines in the top chart).</p><figure class="kg-card kg-image-card"><img src="https://lh7-us.googleusercontent.com/ZDBpRS1yEscEZcsJtevlPaM5URUP58dgJ2csAIcWP-hmQcje8kBi-u4zAWYnbeE26YXWemOh32pdHM2TgaSanGePOVgRiss8svxP17nLPBvg1YjLE4W1uIGkTmDI9PbZO42u_4KfYoSeaRnZz_W4HfY" class="kg-image" alt="Mamba Explained" loading="lazy" width="602" height="256"></figure><figure class="kg-card kg-image-card"><img src="https://lh7-us.googleusercontent.com/j8aQ6XIxedX6Zut0rz7CE_e02KgBjyJvg7QQ7U9FkM2TjSWWSNk1v7gFVeGSsETqwQGvF8flh0lIUmSLIVqW9rwHC69rImw5MPj0vA0Y4XihacOzZnhUeKMZpf3bWtJTM_TB67EDYKIyfp2DeX4pNFU" class="kg-image" alt="Mamba Explained" loading="lazy" width="602" height="256"></figure><p>Since it&#x2019;s hypothesised that much of In-Context Learning in Transformers is downstream of more primitive sequence position operations (like <a href="https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html">Induction Heads</a>), Mamba being able to complete this task suggests a more general In-Context Learning ability.</p><h2 id="what%E2%80%99s-next-for-mamba-ssms">What&#x2019;s Next for Mamba &amp; SSMs?</h2><p>Mamba-like models are likely to excel in scenarios requiring extremely long context and long-term memory. Examples include:</p><ul><li>Processing DNA</li><li>Generating (or reasoning over) video</li><li>Writing novels</li></ul><p>An illustrative example is agents with long-term goals.</p><p><em>Suppose you have an agent interacting with the world. Eventually, its experiences become too much for the context window of a transformer. The agent then has to compress or summarise its experiences into some more compact representation.</em></p><p><em>But how do you decide what information is the most useful as a summary? If the task is language, LLMs are actually fairly good at summaries - okay, yeah, you&#x2019;ll lose some information, but the most important stuff can be retained.</em></p><p><em>However, for other disciplines, it might not be clear how to summarise. For example, what&#x2019;s the best way to summarise a 2 hour movie?<a href="https://thegradient.pub/mamba-explained/#ft24"><sup>24</sup></a>. Could the model itself learn to do this naturally rather than a hacky workaround like trying to describe the aesthetics of the movie in text?</em></p><p>This is what Mamba allows. Actual long-term memory. A real state where the model learns to keep what&#x2019;s important. <a href="https://arxiv.org/pdf/2309.10668.pdf">Prediction is compression</a> - learning what&#x2019;s useful to predict what&#x2019;s coming next inevitably leads to building a useful compression of the previous tokens.</p><hr><p>The implications for Assistants are clear:</p><p>Your chatbot co-evolves with you. It remembers.</p><figure class="kg-card kg-image-card"><img src="https://lh7-us.googleusercontent.com/agZClF-xa6q13BlEbfZLFKP3DM0hJiRy9kC0MRFoNPi8kdWCh8_BUa5oLC0V_6jTmcNQQfmMr7GGa6gwIe3CEGVeK79AFMhE1gMnbdhEoQ8iFCRuO7Yc6Xi2M3kaVIGZ4LTfDKqITQ6ap1DylOqbWs4" class="kg-image" alt="Mamba Explained" loading="lazy" width="602" height="339"></figure><p>The film HER is looking better and better as time goes on &#x1F633;</p><h3 id="agents-ai-safety">Agents &amp; AI Safety</h3><p>One reason for positive updates in existential risk from AGI is Language Models. Previously, Deep-RL agents trained via self-play looked set to be the first AGIs. Language models are inherently much safer since they aren&#x2019;t trained with long-term goals<a href="https://thegradient.pub/mamba-explained/#ft25"><sup>25</sup></a>.</p><p>The potential for long-term sequence reasoning here brings back the importance of agent-based AI safety. Few agent worries are relevant to Transformers with an 8k context window. Many are relevant to systems with impressive long-term memories and possible instrumental goals.</p><h3 id="the-best-collab-since-taco-bell-kfc-%F0%9F%A4%96-x-%F0%9F%90%8D">The Best Collab Since Taco Bell &amp; KFC: &#x1F916; x &#x1F40D;</h3><p>The Mamba authors show that there&#x2019;s value in combining Mamba&#x2019;s long context with the Transformer&#x2019;s high fidelity over short sequences. For example, if you&#x2019;re making long videos, you likely can&#x2019;t fit a whole movie into a Transformer&#x2019;s context for attention<a href="https://thegradient.pub/mamba-explained/#ft26"><sup>26</sup></a>. You could imagine having Attention look at the most recent frames for short-term fluidity and an SSM for long-term narrative consistency<a href="https://thegradient.pub/mamba-explained/#ft27"><sup>27</sup></a>.</p><hr><p>This isn&#x2019;t the end for Transformers. Their high effectiveness is exactly what&#x2019;s needed for many tasks. But now Transformers aren&#x2019;t the only option. Other architectures are genuinely feasible.</p><p>So we&#x2019;re not in the post-Transformer era. But for the first time, we&#x2019;re living in the post-only-Transformers era<a href="https://thegradient.pub/mamba-explained/#ft28"><sup>28</sup></a>. And this blows the possibilities wide open for sequence modelling with extreme context lengths and native long-term memory.</p><p>Two ML researchers, Sasha Rush (HuggingFace, Annotated Transformer, Cornell Professor) and Jonathan Frankle (Lottery Ticket Hypothesis, MosaicML, Harvard Professor), currently have a bet <a href="http://www.isattentionallyouneed.com/">here</a>.</p><figure class="kg-card kg-image-card"><img src="https://lh7-us.googleusercontent.com/-_S7CaQ4OxepapriZhhAs25xq-H_dSnavPxXkm0_lMMZjtno4kgWfjS1PAcLhYpbMz6BNNYd-RoxBA_Fy45CemDdvofbP7oPVQ3ygHBQNQ8pMVf7l5YnLSCgE3L1J9muCpoFmTSz09zcX9xEigRrKnc" class="kg-image" alt="Mamba Explained" loading="lazy" width="602" height="324"></figure><p>Currently Transformers are far and away in the lead. With 3 years left, there&#x2019;s now a research direction with a fighting chance.</p><p>All that remains to ask is: <strong>Is Attention All We Need?</strong><br></p><hr><h2 id="footnotes">Footnotes</h2><!--kg-card-begin: html--><p>
    <a name="ft1">1.</a> see Figure 8 in the Mamba paper.
    <br>
    <a name="ft2">2.</a> And scaling up with massive compute.
    <br>
    <a name="ft3">3.</a> More specifically the scaled dot-product Attention popularised by Transformers
    <br>
    <a name="ft4">4.</a> For people who don&#x2019;t see Temple Run as the cultural cornerstone it is &#x1F923; Temple Run was an iPhone game from 2011 similar to Subway Surfer
    <br>
    <a name="ft5">5.</a> Here we assume the environment is sufficiently smooth.
    <br>
    <a name="ft6">6.</a> One pretty important constraint for this to be efficient is that we don&#x2019;t allow the individual elements of the state vector to interact with each other directly. We&#x2019;ll use a combination of the state dimensions to determine the output but we don&#x2019;t e.g. allow the velocity of the runner and the direction of the closest obstacle (or whatever else was in our state) to directly interact. This helps with efficient computation and we achieve this practically by constraining A to be a diagonal matrix.
    <br>
    <a name="ft7">7.</a> Concretely consider the case of Language Models - each token is a discrete step 
    <br>
    <a name="ft8">8.</a> ZOH also has nice properties for the initialisations - we want A_bar to be close to the identity so that the state can be mostly maintained from timestep to timestep if desired. ZOH gives A_bar as an exponential so any diagonal element initialisations close to zero give values close to 1 
    <br>
    <a name="ft9">9.</a> This is known as the Euler discretisation in the literature
    <br>
    <a name="ft10">10.</a> It&#x2019;s wild to note that some readers might not have, we&#x2019;re so far into the age of Attention that RNNs have been forgotten!
    <br>
    <a name="ft11">11.</a> B is like the Query (Q) matrix for Transformers.
    <br>
    <a name="ft12">12.</a> C is like the Output (O) matrix for Transformers. 
    <br>
    <a name="ft13">13.</a> Non-alcoholic options also available! 
    <br>
    <a name="ft14">14.</a> Especially as all voices roughly occupy the same space on the audio frequency spectrum Intuitively this seems really hard! 
    <br>
    <a name="ft15">15.</a> Note that photographic memory doesn&#x2019;t necessarily imply perfect inferences from that memory! 
    <br>
    <a name="ft16">16.</a> To be clear, if you have a short sequence, then a transformer should theoretically be a better approach. If you can store the whole context, then why not!? If you have enough memory for a high-resolution image, why compress it into a JPEG? But Mamba-style architectures are likely to hugely outperform with long-range sequences. 
    <br>
    <a name="ft17">17.</a> More details are available for engineers interested in CUDA programming - Tri&#x2019;s talk, Mamba paper section 3.3.2, and the official CUDA code are good resources for understanding the Hardware-Aware Scan 
    <br>
    <a name="ft18">18.</a> or in Object Oriented Programming 
    <br>
    <a name="ft19">19.</a> Implications to actual Political Economy are left to the reader but maybe Gu and Dao accidentally solved politics!? 
    <br>
    <a name="ft20">20.</a> This isn&#x2019;t a perfect analogy as human evolution follows a genetic algorithm rather than SGD. 
    <br>
    <a name="ft21">21.</a> Albeit a pretty weird hard drive at that - it morphs over time rather than being a fixed representation.  
    <br>
    <a name="ft22">22.</a> As a backronym, I&#x2019;ve started calling the hidden_state the state space dimension (or selective state dimension) which shortens to SSD, a nice reminder for what this object represents - the long-term memory of the system.
    <br>
    <a name="ft23">23.</a> I&#x2019;m thinking about this similarly to the relationship between harmlessness finetuning and activation steering. State swapping, like activation steering, is an inference time intervention giving comparable results to its train time analogue. 
    <br>
    <a name="ft24">24.</a> This is a very non-trivial problem! How do human brains represent a movie internally? It&#x2019;s not a series of the most salient frames, nor is it a text summary of the colours, nor is it a purely vibes-based summary if you can memorise some lines of the film. 
    <br>
    <a name="ft25">25.</a> They&#x2019;re also safer since they inherently understand (though don&#x2019;t necessarily embody) human values. It&#x2019;s not all clear that how to teach an RL agent human morality. 
    <br>
    <a name="ft26">26.</a> Note that typically an image (i.e. a single frame) counts as &gt;196 tokens, and movies are typically 24 fps so you&#x2019;ll fill a 32k context window in 7 seconds &#x1F92F; 
    <br>
    <a name="ft27">27.</a> Another possibility that I&#x2019;m excited about is applying optimisation pressure to the state itself as well as the output to have models that respect particular use cases. 
    <br>
    <a name="ft28">28.</a> This is slightly hyperbolic, the TS-Mixer for time series, Gradient Boosting Trees for tabular data and Graph Neural Networks for weather prediction exist and are currently used, but these aren&#x2019;t at the core of AI
</p><!--kg-card-end: html--><h2 id="author-bio">Author Bio</h2><p>Kola Ayonrinde is a Research Scientist and Machine Learning Engineer with a flair for writing. He integrates technology and creativity, focusing on applying machine learning in innovative ways and exploring the societal impacts of tech advancements.</p><h2 id="acknowledgements">Acknowledgements</h2><p>This post was originally posted on <a href="https://www.kolaayonrinde.com/blog/2024/02/11/mamba.html">Kola&apos;s personal blog</a>.</p><p><em>Thanks to Gon&#xE7;alo for reading an early draft, Jaden for the nnsight library used for the Interpretability analysis and Tessa for Mamba patching visualisations.Also see: </em><a href="https://arxiv.org/pdf/2312.00752.pdf"><em>Mamba paper</em></a><em>, Mamba Python code, </em><a href="https://srush.github.io/annotated-s4/"><em>Annotated S4</em></a><em>, </em><a href="https://www.cognitiverevolution.ai/emergency-pod-mamba-memory-and-the-ssm-moment/"><em>Nathan Labenz podcast</em></a></p><h2 id="citation">Citation</h2><p>For attribution in academic contexts or books, please cite this work as</p><pre><code>Kola Ayonrinde, &quot;Mamba Explained,&quot; The Gradient, 2024</code></pre><pre><code>@article{Ayonrinde2024mamba,
    author = {Kola Ayonrinde},
    title = {Mamba Explained},
    journal = {The Gradient},
    year = {2024},
    howpublished = {\url{https://thegradient.pub/mamba-explained},
}</code></pre>]]></content:encoded></item><item><title><![CDATA[Car-GPT: Could LLMs finally make self-driving cars happen?]]></title><description><![CDATA[Exploring the utility of large language models in autonomous driving: Can they be trusted for self-driving cars, and what are the key challenges?]]></description><link>https://thegradient.pub/car-gpt/</link><guid isPermaLink="false">65db7b4193571d5c8c154a73</guid><category><![CDATA[LLM]]></category><category><![CDATA[Perspectives]]></category><dc:creator><![CDATA[Jrmy Cohen]]></dc:creator><pubDate>Fri, 08 Mar 2024 16:55:18 GMT</pubDate><media:content url="https://thegradient.pub/content/images/2024/03/car-gpt.jpg" medium="image"/><content:encoded><![CDATA[<img src="https://thegradient.pub/content/images/2024/03/car-gpt.jpg" alt="Car-GPT: Could LLMs finally make self-driving cars happen?"><p>In 1928, London was in the middle of a terrible health crisis, devastated by bacterial diseases like pneumonia, tuberculosis, and meningitis. Confined in sterile laboratories, scientists and doctors were stuck in a relentless cycle of trial and error, using traditional medical approaches to solve complex problems.</p><p>This is when, in September 1928, an accidental event changed the course of the world.<strong> </strong>A Scottish doctor named Alexander Fleming forgot to close a petri dish (the transparent circular box you used in science class), which got contaminated by mold. This is when Fleming noticed something peculiar: all bacteria close to the moisture were dead, while the others survived.</p><p>&quot;What was that moisture made of?&quot; wondered M. Flemming.<strong> </strong>This was when he discovered that Penicillin, the main component of the mold, was a powerful bacterial killer. This led to the groundbreaking discovery of penicillin, leading to the antibiotics we use today. In a world where doctors were relying on existing well-studied approaches, Penicillin was the unexpected answer.</p><p><strong>Self-driving cars may be following a similar event.</strong> Back in the 2010s, most of them were built using what we call a &#xAB; modular &#xBB; approach. The software &#xAB; autonomous &#xBB; part is split into several modules, such as Perception (the task of seeing the world), or Localization (the task of accurately localize yourself in the world), or Planning (the task of creating a trajectory for the car to follow, and implementing the &#xAB; brain &#xBB; of the car). Finally, all these go to the last module: Control, that generates commands such as &#xAB; steer 20&#xB0; right &#xBB;, etc&#x2026; So this was the well-known approach. </p><p>But a decade later, companies started to take another discipline very seriously: <strong>End-To-End learning</strong>. The core idea is to replace every module with a single neural network predicting steering and acceleration, but as you can imagine, this introduces a black box problem.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://lh7-us.googleusercontent.com/EpMJPDK-TBu9ZhN25UrxVbAk-9rJjEvtitzjvPpzjhTBPdkk-judKQtfWQNf7vtNrG1sfsvkUhpbtMGplWN5bbnx5ULbfNj6vpRf8RVlt5eDn8MN99FObGbPsmokdNlCGZ1NWq-uw32QVitv4NZC3zI" class="kg-image" alt="Car-GPT: Could LLMs finally make self-driving cars happen?" loading="lazy" width="624" height="161"><figcaption>The 4 Pillars of Self-Driving Cars are Perception, Localization, Planning, and Control. Could a Large Language Model replicate them? (<a href="https://www.thinkautonomous.ai/blog/autonomous-vehicle-architecture/">source</a>)</figcaption></figure><p>These approaches are known, but don&#x2019;t solve the self-driving problem yet. So, we could be wondering:<strong> &quot;What if LLMs (Large Language Models), currently revolutionizing the world, were the unexpected answer to autonomous driving?&quot;</strong></p><p>This is what we&apos;re going to see in this article, beginning with a simple explanation of what LLMs are and then diving into how they could benefit autonomous driving.</p><h2 id="preamble-llms-what"><strong>Preamble: LLMs-what?</strong></h2><p>Before you read this article, you must know something: I&apos;m not an LLM pro, at all. This means, I know too well the struggle to learn it. I understand what it&apos;s like to google &quot;learn LLM&quot;; then see 3 sponsored posts asking you to download e-books (in which nothing concrete appears)... then see 20 ultimate roadmaps and GitHub repos, where step 1/54 is to view a 2-hour long video (and no one knows what step 54 is because it&apos;s so looooooooong).</p><p>So, instead of putting you through this pain myself, let&apos;s just break down what LLMs are in 3 key ideas:</p><ol><li>Tokenization</li><li>Transformers</li><li>Processing Language</li></ol><h3 id="tokenization"><strong>Tokenization</strong></h3><p>In ChatGPT, you input a piece of text, and it returns text, right? Well, what&apos;s actually happening is that your text is first converted into tokens.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://lh7-us.googleusercontent.com/_rT6_ShRUbi-bZpaKL7JF-BhE_rfDg_V8De5nYj0O5tGgAtLTyYhnGleIy7nBJ3vyrUsfge6cdReCctzsfCyW_XP6WUm21pU350RpOoxWzb2SYRvMcKMIZAOE6wdFou7t_ERJ2_Jht6uUhfg_sBgcbI" class="kg-image" alt="Car-GPT: Could LLMs finally make self-driving cars happen?" loading="lazy" width="600" height="303"><figcaption>Example of tokenization of a sentence, each word becomes a &quot;token&quot;</figcaption></figure><p>But what&apos;s a token? You might ask. Well, a token can correspond to a word, a character, or anything we want. Think about it -- if you are to send a sentence to a neural network, you didn&apos;t plan on sending actual words, did you?</p><p>The input of a neural network is always a number, so you need to convert your text into numbers; this is tokenization.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://lh7-us.googleusercontent.com/pZ3qf5HQNrqXqb5bbGkLgWQPvu04-2b_ejpv4m3i5C9VfcPg3yZm7cmaD6lq4xgrA4DhUBJpCa-HB4i7iAPo8-Hyrde9sLiBYBiY2d7c9O17ePJtCqAb15dvcDEGxofEwneP6Nx2_oSiT26m4cLvcMc" class="kg-image" alt="Car-GPT: Could LLMs finally make self-driving cars happen?" loading="lazy" width="624" height="139"><figcaption>What tokenization actually is: A conversion from words to numbers</figcaption></figure><p>Depending on the model (ChatGPT, LLAMA, etc...), a token can mean different things: a word, a subword, or even a character. We could take the English vocabulary and define these as words or take parts of words (subwords) and handle even more complex inputs. For example, the word &#xAB; a &#xBB; could be token 1, and the word &#xAB; abracadabra &#xBB; would be token 121.</p><h3 id="transformers"><strong>Transformers</strong></h3><p>Now that we understand how to convert a sentence into a series of numbers, we can send that series into our neural network! At a high level, we have the following structure:</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://lh7-us.googleusercontent.com/J1CkM3ItKevopmi-0gbSHWJnMStL4dZWksllG15OlaDI4PFgk-FtFeQ7O0CnP1dKx9ZHV7PUAlmBK9lFwJQrHnJj1JAXAMHdbZH13hd07dYL55ZCsxQChf06dYj_JoXEvNeAqdfmj2IcdwD8sP5OZtI" class="kg-image" alt="Car-GPT: Could LLMs finally make self-driving cars happen?" loading="lazy" width="624" height="407"><figcaption>A Transformer is an Encoder-Decoder Architecture that takes a sequence of tokens as input and outputs a another series of tokens</figcaption></figure><p>If you start looking around, you will see that some models are based on an encoder-decoder architecture, some others are purely encoder-based, and others, like GPT, are purely decoder-based.</p><p>Whatever the case, they all share the core Transformer blocks: multi-head attention, layer normalization, addition and concatenation, blocks, cross-attention, etc...</p><p><strong>This is just a series of attention blocks getting you to the output</strong>. So how does this word prediction work?</p><h3 id="the-output-next-word-prediction"><strong>The output/ Next-Word Prediction</strong></h3><p>The Encoder learns features and understands context... But what does the decoder do? In the case of object detection, the decoder is predicting bounding boxes. In the case of segmentation, the decoder is predicting segmentation masks. What about here?</p><p>In our case, the decoder is trying to generate a series of words; we call this task &quot;next-word prediction&quot;.</p><p>Of course, it does it similarly by predicting numbers or tokens. This characterizes our full model as shown below,</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://lh7-us.googleusercontent.com/YS9WFjjuYTq7QkzPnx4xgTQnU0Pmr22i4fEzXXWuBf6wD--eYL8FvdoEpkqlCMKraBaSDuo7j0sWR7ltUaWI31_Bvq9PtJoPpoWRFQnjKOth1P7mnxfzmGT8ppUslOPMhbOzJY49F4IHBMZfyzax18E" class="kg-image" alt="Car-GPT: Could LLMs finally make self-driving cars happen?" loading="lazy" width="624" height="519"><figcaption>I would say the loss function for this particular output produces a near-0 value.</figcaption></figure><p>Now, there are many &quot;concepts&quot; that you should learn on top of this intro: everything Transformer and Attention related, but also few-shot learning, pretraining, finetuning, and more...</p><p>Ok... but what does it have to do with self-driving cars? I think it&apos;s time to move to stage 2.</p><h2 id="chat-gpt-for-self-driving-cars"><strong>Chat-GPT for Self-Driving Cars</strong></h2><p>The thing is, you&apos;ve already been through the tough part. The rest simply is: &quot;How do I adapt this to autonomous driving?&quot;. Think about it; we have a few modifications to make:</p><ul><li><strong>Our input now becomes either images, sensor data</strong> (LiDAR point clouds, RADAR point clouds, etc...), or even algorithm data (lane lines, objects, etc...). All of it is &quot;tokenizable&quot;, as Vision Transformers or Video Vision Transformers do.</li><li><strong>Our Transformer model pretty much remains the same</strong> since it only operates on tokens and is independent of the kind of input.</li><li><strong>The output is based on the set of tasks we want to do.</strong> It could be explaining what&apos;s happening in the image or could &#xA0;also be a direct driving task like switching lanes.</li></ul><p>So, let&apos;s begin with the end:</p><h3 id="what-self-driving-car-tasks-could-llm-solve"><strong>What self-driving car tasks could LLM solve?</strong></h3><p>There are many tasks involved in autonomous driving, but not all of them are GPT-isable. The most active research areas in 2023 have been:</p><ul><li><strong>Perception</strong>: Based on an input image, describe the environment, number of objects, etc...</li><li><strong>Planning</strong>: Based on an image, or a bird-eye view, or the output of perception, describe what we should do (keep driving, yield, etc...)</li><li><strong>Generation</strong>: Generate training data, alternate scenarios, and more... using &quot;diffusion&quot;</li><li><strong>Question &amp; Answers</strong>: Create a chat interface and ask the LLM to answer questions based on the scenario.</li></ul><h3 id="llms-in-perception"><strong>LLMs in Perception</strong></h3><p>In Perception, the input is a series of images, and the output is usually a set of objects, lanes, etc... In the case of LLMs, we have 3 core tasks: <strong>Detection</strong>, <strong>Prediction</strong>, and <strong>Tracking</strong>. An example with Chat-GPT, when you send it an image and ask to describe what&apos;s going on is shown below:</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://lh7-us.googleusercontent.com/unUisu66NolUzzipNfKObr8kE6n8PRcTMy86cYYIG1aIPLkYKZd34zmzGrkM4yS6lKNoXpvORHwnfORfOsy8aRNUx9AwEDN_qQN4tiuutBRh8l3h_vVpfVzOJ7UdQ-CuWKI5EJsze9le6qRA7VQ1QoY" class="kg-image" alt="Car-GPT: Could LLMs finally make self-driving cars happen?" loading="lazy" width="425" height="308"><figcaption>A GPT-4 Vision model can return the objects in the image, just like object detectors do (<a href="https://arxiv.org/pdf/2311.05332.pdf">source</a>)</figcaption></figure><p>Other models such as<a href="https://arxiv.org/pdf/2309.05186v1.pdf"> HiLM-D</a> and<a href="https://arxiv.org/pdf/2312.00812.pdf"> MTD-GPT</a> can also do this, some work also for videos. Models like<a href="https://arxiv.org/pdf/2309.04379.pdf"> PromptTrack</a>, also have the ability to assign unique IDs (this car in front of me is ID #3), similar to a 4D Perception model.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://lh7-us.googleusercontent.com/WcjhR7diFbZrVeKdiVyQbC_HtYJVGUQsOBka0zikaD2JZpfmNxcyEJlpzxZfvobWrMu6srxUEGPcxpdVSywVKW-0gIuOISCqLCVfjaA6Q7KaNb1etKfNybXkya4yFyx7AY0Y2_ZZw_cY_gWSccO0B2Q" class="kg-image" alt="Car-GPT: Could LLMs finally make self-driving cars happen?" loading="lazy" width="624" height="267"><figcaption>PromptTrack combines the DETR object detector with Large Language Models</figcaption></figure><p>In this model, multi-view images are sent to an Encoder-Decoder network that is trained to predict annotations of objects such as bounding boxes, and attention maps). These maps are then combined with a prompt like &apos;find the vehicles that are turning right&apos;.The next block then finds the 3D Bounding Box localization and assigns IDs using a bipartite graph matching algorithm like the<a href="https://www.thinkautonomous.ai/blog/hungarian-algorithm"> Hungarian Algorithm</a>.</p><p>This is cool, but this isn&apos;t the &quot;best&quot; application of LLMs so far:</p><h3 id="llms-in-decision-making-navigation-and-planning"><strong>LLMs in Decision Making, Navigation, and Planning</strong></h3><p>If Chat-GPT can find objects in an image, it should be able to tell you what to do with these objects, shouldn&apos;t it? Well, this is the task of Planning i.e. defining a path from A to B, based on the current perception. While there are numerous models developed for this task, the one that stood out to me was<a href="https://arxiv.org/pdf/2310.02251.pdf"> Talk2BEV</a>:</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://lh7-us.googleusercontent.com/N3ZvMnLMjQ6jwL4FNvvTyM4U6KFrri0jV-0yOYVH9lAAtRH7MD8aMX_LHhjeBFKxGwTdrATJoNUQe-sUqEB3utLnpreCT4e4TIO3qX3LTrzBKwZ7kPAfzxAu6osJ35tYpapCiTTWDtx0tUOHXcNqu04" class="kg-image" alt="Car-GPT: Could LLMs finally make self-driving cars happen?" loading="lazy" width="600" height="179"><figcaption>Talk2BEV takes perception one step further and also tells you what to do</figcaption></figure><p>The main difference between models for planning and Perception-only models is that here, we&apos;re going to train the model on human behavior to suggest ideal driving decisions. We&apos;re also going to change the input from multi-view to<a href="https://courses.thinkautonomous.ai/bird-eye-view"> Bird Eye View</a> since it is much easier to understand.</p><p>This model works both with LLaVA and ChatGPT4, and here is a demo of the architecture:</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://lh7-us.googleusercontent.com/-bl_IDT2SqF75q3d20EORJcH22oXWMjFmkLFn0ZKbVV5oshlr5BkZEnscfUSg_-pkzMDJ3Jo38mdu6whUmIDWq7pXfxXxdwgc3Kj-WUwv5LNWUHIvH3r6mfpKP9s5PD7NoA7e0R3pBoic6ijwfD57aU" class="kg-image" alt="Car-GPT: Could LLMs finally make self-driving cars happen?" loading="lazy" width="600" height="255"><figcaption>Talk2BEV (<a href="https://github.com/llmbev/talk2bev">source</a>)</figcaption></figure><p>As you can see, this isn&apos;t purely &quot;prompt&quot; based, because the core object detection model stays Bird Eye View Perception, but the LLM is used to &quot;enhance&quot; that output by suggesting to crop some regions, look at specific places, and predict a path. We&apos;re talking about &quot;language enhanced BEV Maps&quot;.</p><p>Other models like<a href="https://arxiv.org/pdf/2310.01415.pdf"> DriveGPT</a> are trained to send the output of Perception to Chat-GPT and finetune it to output the driving trajectory directly.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://lh7-us.googleusercontent.com/QbuhKKLWr0jA1DWdSWBIk6UtHnecHTITRBPidM1fjYn9VSC-56VcaxStqJbn5iTLslLN6ppQgnmfKZO-43TNCZADfuwdV-RChnMrzryLIKx7UvtySKEs0unIEum4c2ous07M3-WlUoTVeGT1s0nPz0U" class="kg-image" alt="Car-GPT: Could LLMs finally make self-driving cars happen?" loading="lazy" width="624" height="327"><figcaption>The DriveGPT model is pure madness... when trained correctly! (modified from<a href="https://arxiv.org/pdf/2310.01415.pdf"> source</a>)</figcaption></figure><p>I could go on and on, but I think you get the point. If we summarize, I would say that:</p><ul><li>Inputs are either tokenized images or outputs of Perception algorithm (BEV maps, ...)</li><li>We fuse existing models (BEV Perception, Bipartite Matching, ...) with language prompts (find the moving cars)</li><li>Changing the task is mainly about changing the data, loss function, and careful finetuning.</li></ul><p>The Q&amp;A applications are very similar, so let&apos;s see the last application of LLMs:</p><h3 id="llms-for-image-generation"><strong>LLMs for Image Generation</strong></h3><p>Ever tried Midjourney and DALL-E? Isn&#x2019;t it super cool? Yes, and there is MUCH COOLER than this when it comes to autonomous driving. In fact, have you heard of Wayve&apos;s GAIA-1 model? The model takes text and images as input and directly produces videos, like this:</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://lh7-us.googleusercontent.com/R9xqQVFRcUlZrjXqqeY6qon7hxAezFKY3mI_ZdPh2R0eJGPQb2CjV0TFxjwblDEWxJz7va0N6KerXMRO_ltSFJkxiQRxmW7I_I_b13bD-PidrUD8sQ0REInSAUJuKGqazFFDCpwOQAVun5LREW41Q_w" class="kg-image" alt="Car-GPT: Could LLMs finally make self-driving cars happen?" loading="lazy" width="293" height="277"><figcaption>These videos are generated by Wayve&apos;s GAIA-1 model</figcaption></figure><p>The architecture takes images, actions, and text prompts as input, and then uses a World Model (an understanding of the world and its interactions) to produce a video.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://lh7-us.googleusercontent.com/ougFIHYengzs40lyruVerlDFFa18VXH0K093yjHA93q8nTjTmLQAdfKCPl7sBAbZfpoxqY3tDdzufOqJoxmLUdL6W862_aPebPxABsPwjyaFZGWOCP2VTpaqcob0gkSJDRv9IqSm7-aHoXtG-FXWJBo" class="kg-image" alt="Car-GPT: Could LLMs finally make self-driving cars happen?" loading="lazy" width="624" height="260"><figcaption>Architecture of GAIA-1 (<a href="https://wayve.ai/thinking/scaling-gaia-1/">source</a>)</figcaption></figure><p>You can find more examples on Wayve&apos;s YouTube channel and<a href="https://wayve.ai/thinking/scaling-gaia-1/"> this dedicated post</a>.</p><p>Similarly, you can see<a href="https://github.com/cure-lab/MagicDrive"> MagicDrive</a>, which takes the output of Perception as input and uses that to generate scenes:</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://lh7-us.googleusercontent.com/vVKUXuJno-UQWj2ZTWEA1JBMzZ6xnajJOrzMPtMW4qFjhvKqT7F2XiOoe9M1PCtM44S4CfrXqTVyVfKOisaB3iy-wa5vuCS7SFYaQdv6dzNfbzVcG2XXQzAAUqZXUeGxALkJ9fHuE8XFA4KvkKmZLN4" class="kg-image" alt="Car-GPT: Could LLMs finally make self-driving cars happen?" loading="lazy" width="624" height="187"><figcaption>(<a href="https://github.com/cure-lab/MagicDrive">source</a>)</figcaption></figure><p>Other models, like<a href="https://arxiv.org/pdf/2311.17918.pdf"> Driving Into the Future</a> and<a href="https://drivingdiffusion.github.io/"> Driving Diffusion</a> can directly generate future scenarios based on the current ones. You get the point; we can generate scenes in an infinite way, get more data for our models, and have this endless positive loop.</p><p>We&apos;ve just seen 3 prominent families of LLM usage in self-driving cars: Perception, Planning, and Generation. The real question is...</p><h2 id="could-we-trust-llms-in-self-driving-cars"><strong>Could we trust LLMs in self-driving cars?</strong></h2><p><strong>And by this, I mean... What if your model has hallucinations?</strong> What if its replies are completely absurd, like ChatGPT sometimes does? I remember, back in my first days in autonomous driving, big groups were already skeptical about Deep Learning, because it wasn&apos;t &quot;deterministic&quot; (as they call it).</p><p>We don&apos;t like Black Boxes, which is one of the main reasons End-To-End will struggle to get adopted. Is ChatGPT any better? I don&apos;t think so, and I would even say it&apos;s worse in many ways. However, LLMs are becoming more and more transparent, and the black box problem could eventually be solved.</p><p>To answer the question &quot;Can we trust them?&quot;... it&apos;s very early in the research, and I&apos;m not sure someone has really used them &quot;online&quot; &#x2014; meaning &#xAB; live &#xBB;, in a car, on the streets, rather than in a headquarter just for training or image generation purpose. &#xA0;I would definitely picture a Grok model on a Tesla someday just for Q&amp;A purposes. So for now, I will give you my coward and safe answer...</p><p><strong>It&apos;s too early to tell!</strong></p><p>Because it really is. The first wave of papers mentioning LLMs in Self-Driving Cars is from mid-2023, so let&apos;s give it some time. In the meantime, you could start with<a href="https://arxiv.org/pdf/2311.01043.pdf"> this survey</a> that shows all the evolutions to date.</p><p>Alright, time for the best part of the article...</p><h2 id="the-llms-4-ad-summary"><strong>The LLMs 4 AD Summary</strong></h2><ul><li><strong>A Large Language Model (LLM) works in 3 key steps: inputs, transformer, output.</strong> The input is a set of tokenized words, the transformer is a classical transformer, and the output task is &quot;next word prediction&quot;.</li><li><strong>In a self-driving car, there are 3 key tasks we can solve with LLMs:</strong> <strong>Perception</strong> (detection, tracking, prediction), <strong>Planning</strong> (decision making, trajectory generation), and <strong>Generation</strong> (scene, videos, training data, ...).</li><li><strong>In Perception, the main goal is to describe the scene we&apos;re looking at.</strong> The input is a set of raw multi-view images, and the Transformer aims to predict 3D bounding boxes. LLMs can also be used to ask for a specific query (&quot;where are the taxis?&quot;).</li><li><strong>In Planning, the main goal is to generate a trajectory for the car to take</strong>. The input is a set of objects (output of Perception, BEV Maps, ...), and the Transformer uses LLMs to understand context and reason about what to do.</li><li><strong>In Generation, the main goal is to generate a video that corresponds to the prompt used</strong>. Models like GAIA-1 have a chat interface, and take as input videos to generate either alternate scenes (rainy, ...), or future scenes.</li><li><strong>For now, it&apos;s very early to tell whether this can be used in the long run</strong>, but research there is some of the most active in the self-driving car space. It all comes back to the question: &quot;Can we really trust LLMs in general?&quot;</li></ul><h2 id="next-steps"><strong>Next Steps</strong></h2><p>If you want to get started on LLMs for self-driving cars, there are several things you can do:</p><ul><li><strong>&#x26A0;&#xFE0F; Before this, the most important</strong>: If you want to keep learning about self-driving cars. I&apos;m talking about self-driving car every day through my private emails. I&apos;m sending many tips and direct content.<a href="https://www.thinkautonomous.ai/private-emails/"> You should join here</a>.</li><li><strong>&#x2705; To begin, build an understanding of LLMs for self-driving cars</strong>. This is partly done, you can continue to explore the resources I provided in the article.</li><li><strong>&#x27A1;&#xFE0F; Second, build skills related to Auto-Encoders and Transformer Networks</strong>.<a href="https://courses.thinkautonomous.ai/image-segmentation"> My image segmentation series</a> is perfect for this, and will help you understand Transformer Networks with no NLP example, which means it&apos;s for Computer Vision Engineer&apos;s brains.</li><li><strong>&#xFE0F; &#x27A1;&#xFE0F; Then, understand how Bird Eye View Networks works.</strong> It might not be mentioned in general LLM courses, but in self-driving cars, Bird Eye View is the central format where we can fuse all the data (LiDARs, cameras, multi-views, ...), build maps, and directly create paths to drive. You can do so in my<a href="https://courses.thinkautonomous.ai/bird-eye-view"> Bird Eye View course</a> (if closed,<a href="https://www.thinkautonomous.ai/private-emails/"> join my email list</a> to be notified).</li><li><strong>Finally, practice training, finetuning, and running LLMs in self-driving car scenarios</strong>. Run repos like Talk2BEV and the others I mentioned in the article. Most of them are open source, but the data can be hard to find. This is noted last, but there isn&apos;t really an order in all of this.</li></ul><hr><h2 id="author-bio"><strong>Author Bio</strong></h2><p>J&#xE9;r&#xE9;my Cohen is a self-driving car engineer and founder of <a href="https://www.thinkautonomous.ai/">Think Autonomous</a>, a platform to help engineers learn about cutting-edge technologies such as self-driving cars and advanced Computer Vision. In 2022, Think Autonomous won the price for Top Global Business of the Year in the Educational Technology Category&#x200B; and Jeremy Cohen was named 2023 40 Under 40 Innovators in Analytics Insight magazine, the largest printed magazine on Artificial Intelligence. <em>You can join 10,000 engineers reading his private daily emails on self-driving cars </em><a href="https://www.thinkautonomous.ai/private-emails"><em>here</em></a><em>.</em></p><h2 id="citation">Citation</h2><p>For attribution in academic contexts or books, please cite this work as</p><pre><code>J&#xE9;r&#xE9;my Cohen, &quot;Car-GPT: Could LLMs finally make self-driving cars happen?&quot;, The Gradient, 2024.</code></pre><p>BibTeX citation:</p><pre><code>@article{cohen2024cargpt,
    author = {J&#xE9;r&#xE9;my Cohen},
    title = {Car-GPT: Could LLMs finally make self-driving cars happen?},
    journal = {The Gradient},
    year = {2024},
    howpublished = {\url{https://thegradient.pub/car-gpt},
}</code></pre>]]></content:encoded></item><item><title><![CDATA[Do text embeddings perfectly encode text?]]></title><description><![CDATA[ 'Vec2text' can serve as a solution for accurately reverting embeddings back into text, thus highlighting the urgent need for revisiting security protocols around embedded data. ]]></description><link>https://thegradient.pub/text-embedding-inversion/</link><guid isPermaLink="false">65e3d66193571d5c8c154aec</guid><category><![CDATA[Interpretability]]></category><category><![CDATA[LLM]]></category><category><![CDATA[NLP]]></category><dc:creator><![CDATA[Jack Morris]]></dc:creator><pubDate>Tue, 05 Mar 2024 20:15:58 GMT</pubDate><media:content url="https://thegradient.pub/content/images/2024/03/main-1.png" medium="image"/><content:encoded><![CDATA[<h3 id="the-rise-of-the-vector-database">The rise of the vector database</h3><img src="https://thegradient.pub/content/images/2024/03/main-1.png" alt="Do text embeddings perfectly encode text?"><p>As a result of the rapid advancement of generative AI in recent years, many companies are rushing to integrate AI into their businesses. One of the most common ways of doing this is to build AI systems that answer questions concerning information that can be found within a database of documents. Most solutions for such a problem are based on one key technique: <a href="https://proceedings.neurips.cc/paper/2020/file/6b493230205f780e1bc26945df7481e5-Paper.pdf">Retrieval Augmented Generation (RAG)</a>. </p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://lh7-us.googleusercontent.com/25UaLlGmLLH-h2ljDEGD1T_eUnErWmCSsLn5gz49U6DsQebwn-kwWtkT0_GvPTswYlAyOP9iZ3Q6Gpy-K6UwuWVF4bzj-1G2cZJd6oXw4hDX92texYdByUuq7bO8qcmuKQSqfR6zkPlm9M5KoYDk12A" class="kg-image" alt="Do text embeddings perfectly encode text?" loading="lazy" width="624" height="276"><figcaption>Overview of a RAG system.<span class="-mobiledoc-kit__atom">&#x200C; &#x200C;</span>Source: &#x201C;<a href="https://proceedings.neurips.cc/paper/2020/file/6b493230205f780e1bc26945df7481e5-Paper.pdf">Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks</a>&#x201D;</figcaption></figure><p>This is what lots of people do now as a cheap and easy way to get started using AI: store lots of documents in a database, have the AI retrieve the most relevant documents for a given input, and then generate a response to the input that is informed by the retrieved documents.</p><p>These RAG systems determine document relevancy by using &#xA0;&#x201C;embeddings&#x201D;, vector representations of documents produced by an embedding model. These embeddings are supposed to represent some notion of similarity, so documents that are relevant for search will have high vector similarity in embedding space.</p><p>The prevalence of RAG has led to the rise of the <em>vector database</em>, a new type of database designed for storing and searching through large numbers of embeddings. <a href="https://techcrunch.com/2023/04/27/pinecone-drops-100m-investment-on-750m-valuation-as-vector-database-demand-grows/">Hundreds of</a> <a href="https://siliconangle.com/2023/04/06/chroma-bags-18m-speed-ai-models-embedding-database/">millions of</a> <a href="https://www.prnewswire.com/news-releases/weaviate-raises-50-million-series-b-funding-to-meet-soaring-demand-for-ai-native-vector-database-technology-301803296.html">dollars of</a> <a href="https://techcrunch.com/2023/04/19/qdrant-an-open-source-vector-database-startup-wants-to-help-ai-developers-leverage-unstructured-data/">funding</a> have been given out to startups that claim to facilitate RAG by making embedding search easy. And the effectiveness of RAG is the reason why lots of new applications are converting text to vectors and storing them in these vector databases.</p><h3 id="embeddings-are-hard-to-read">Embeddings are hard to read</h3><p>So what is stored in a text embedding? Beyond the requirement of semantic similarity, there are no constraints on which embedding must be assigned for a given text input. Numbers within embedding vectors can be <em>anything</em>, and vary based on their initialization. We can interpret the similarities of embedding with others but have no hope ever understanding the individual numbers of an embedding.<br></p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://lh7-us.googleusercontent.com/GwnKcHZF5vTgMZlKmEobbOLiJoQLOknGoG1znqG5pT-7kWwMCOPSEK3gB-q-NnBt5ahi2FLjbaFM9x-J5DS4VKbns7de88GATWbjaR-iDeuLPWY-muNKQ6bWhqyvo4HRxXWaStkgVrhEF6B0Tdu-Ihs" class="kg-image" alt="Do text embeddings perfectly encode text?" loading="lazy" width="555" height="149"><figcaption>A neural embedding model (light blue) takes text input and produces an <em>embedding</em>, a vector that can be used for search.</figcaption></figure><p>Now imagine you&#x2019;re a software engineer building a RAG system for your company. You decide to store your vectors in a vector database. You notice that in a vector database, what&apos;s stored are embedding vectors, not the text data itself. The database fills up with rows and rows of random-seeming numbers that represent text data but never &#x2018;sees&#x2019; any text data at all. <br><br>You know that the text corresponds to customer documents that are protected by your company&#x2019;s privacy policy. But you&#x2019;re not <em>really</em> sending text off-premises at any time; you only ever send embedding vectors, which look to you like random numbers.<br><br>What if someone hacks into the database and gains access to all your text embedding vectors &#x2013; would this be bad? Or if the service provider wanted to sell your data to advertisers &#x2013; could they? Both scenarios involve being able to take embedding vectors and <em>invert </em>them somehow back to text.</p><h3 id="from-text-to-embeddingsback-to-text">From text to embeddings...back to text</h3><p>The problem of recovering text from embeddings is exactly the scenario we tackle in our paper <a href="https://arxiv.org/abs/2310.06816"><em>Text Embeddings Reveal As Much as Text</em> </a>(EMNLP 2023). Are embedding vectors a secure format for information storage and communication? Put simply: can input text be recovered from output embeddings?</p><p>Before diving into solutions, let&#x2019;s think about the problem a little bit more. Text embeddings are the output of neural networks, sequences of matrix multiplications joined by nonlinear function operations applied to input data. In traditional text processing neural networks, a string input is split into a number of token vectors, which repeatedly undergo nonlinear function operations. At the output layer of the model, tokens are averaged into a single embedding vector.</p><p>A maxim from the signal processing community known as the <a href="https://en.wikipedia.org/wiki/Data_processing_inequality">data processing inequality</a> tells us that functions cannot add information to an input, they can only sustain or decrease the amount of information available. Even though conventional wisdom tells us that deeper layers of a neural network are constructing ever-higher-order representations, they aren&#x2019;t adding any information about the world that didn&#x2019;t come in on the input side. </p><p>Additionally, the nonlinear layers certainly destroy <em>some</em> information. One ubiquitous nonlinear layer in modern neural networks is the &#x201C;ReLU&#x201D; function, which simply sets all negative inputs to zero. After applying ReLU throughout the many layers of a typical text embedding model, it is not possible to retain all the information from the input.<br></p><h3 id="inversion-in-other-contexts">Inversion in other contexts</h3><p>Similar questions about information content have been asked in the computer vision community. Several results have shown that deep representations (embeddings, essentially) from image models can be used to recover the input images with some degree of fidelity. An early result <a href="https://openaccess.thecvf.com/content_cvpr_2016/papers/Dosovitskiy_Inverting_Visual_Representations_CVPR_2016_paper.pdf">(Dosovitskiy, 2016)</a> showed that images can be recovered from the feature outputs of deep convolutional neural networks (CNNs). Given the high-level feature representation from a CNN, they could invert it to produce a blurry-but-similar version of the original input image.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://thegradient.pub/content/images/2024/03/Group-66--3-.png" class="kg-image" alt="Do text embeddings perfectly encode text?" loading="lazy" width="2000" height="1865" srcset="https://thegradient.pub/content/images/size/w600/2024/03/Group-66--3-.png 600w, https://thegradient.pub/content/images/size/w1000/2024/03/Group-66--3-.png 1000w, https://thegradient.pub/content/images/size/w1600/2024/03/Group-66--3-.png 1600w, https://thegradient.pub/content/images/size/w2400/2024/03/Group-66--3-.png 2400w" sizes="(min-width: 720px) 720px"><figcaption>In computer vision, inversion models (yellow) have successfully reconstructed images given only the 1000 probability outputs of an ImageNet classifier, most of which are close to 0. (Images from <a href="https://arxiv.org/pdf/2103.07470.pdf"><em>Understanding Invariance via Feedforward Inversion of Discriminatively Trained Classifiers</em></a>.)</figcaption></figure><p>People have improved on image embedding inversion process since 2016: models have been developed that do <a href="https://arxiv.org/abs/1806.00400">inversion</a> <a href="https://arxiv.org/abs/2008.01777">with</a> <a href="https://arxiv.org/pdf/2103.07470.pdf">higher accuracy</a>, and have been shown to work across <a href="https://arxiv.org/abs/2112.09164">more settings</a>. Surprisingly, some work has shown that images can be inverted from the outputs of an ImageNet classifier (1000 class probabilities).</p><h3 id="the-journey-to-vec2text">The journey to vec2text</h3><p>If inversion is possible for image representations, then why can&#x2019;t it work for text? Let&#x2019;s consider a toy problem of recovering text embeddings. For our toy setting we&#x2019;ll restrict text inputs to 32 tokens (around 25 words, a sentence of decent length) and embed them all to vectors of 768 floating-point numbers. At 32-bit precision, these embeddings are 32 * 768 = 24,576 bits or around 3 kilobytes.&#x2003;</p><p>Few words represented by many bits. Do you think we could perfectly reconstruct the text within this scenario? </p><p>First things first: we need to define a measurement of <em>goodness</em>, to know how well we have accomplished our task. One obvious metric is &quot;exact match&quot;, how often we get the exact input back after inversion. No prior inversion methods have any success on exact match, so it&#x2019;s quite an ambitious measurement. So maybe we want to start with a smooth measurement that measures how similar the inverted text is to the input. For this we&#x2019;ll use BLEU score, which you can just think of as a percentage of how close the inverted text is to the input.</p><p>With our success metric defined, let us move on to proposing an approach to evaluate with said metric. &#xA0;For a first approach, we can pose inversion as a traditional machine learning problem, and we solve it the best way we know how: by gathering a large dataset of embedding-text pairs, and train a model to output the text given the embedding as input.</p><p>So this is what we did. We build a transformer that takes the embedding as input and train it using traditional language modeling on the output text. This first approach gives us a model with a BLEU score of around 30/100. Practically, the model can guess the topic of the input text, and get some of the words, but it loses their order and often gets most of them wrong. The exact match score is close to zero. It turns out that asking a model to reverse the output of another model in a single forward pass is quite hard (as are other complicated text generation tasks, like generating text in perfect sonnet form or satisfying multiple attributes).</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://lh7-us.googleusercontent.com/BorB5n0gaGnDObtJLRPC4lOHYn6l3tKS2AnXv03Oj62dPcqKjNFoNv6lfPtOL6KlpIo8U4BZPo8EC4BLVb8DFtDFzjt8CCbUOEeYeikHqTATDVsCNyWL331zcl6eQbU3uCTte1WkvtcMF9hMlnwvny4" class="kg-image" alt="Do text embeddings perfectly encode text?" loading="lazy" width="624" height="401"><figcaption>Overview of architectures considered. Prior work (left) uses a decoder-only architecture and inputs an embedding as a prefix. We initially trained an encoder-decoder model (middle) to condition on an upscaled sentence embedding on the encoder-side. Our final method (right) includes an additional &#x201C;hypothesis&#x201D; text along with an upscaled hypothesis embedding.</figcaption></figure><p>After training our initial model, we noticed something interesting. A different way to measure model output quality is by re-embedding the generated text (we call this the &#x201C;hypothesis&#x201D;) and measuring this embedding&#x2019;s similarity to the true embedding. When we do this with our model&#x2019;s generations, we see a very high cosine similarity &#x2013; around 0.97. This means that we&#x2019;re able to generate text that&#x2019;s close in embedding space, but not identical to, the ground-truth text.</p><p>(An aside: what if this weren&#x2019;t the case? That is, what if the embedding assigned our incorrect hypothesis the same embedding as the original sequence. Our embedder would be <em>lossy</em>, mapping multiple inputs to the same output. If this were the case, then our problem would be hopeless, and we would have no way of distinguishing which of multiple possible sequences produced it. In practice, we never observe these types of collisions in our experiments.)</p><p>The observation that hypotheses have different embeddings to the ground truth inspires an optimization-like approach to embedding inversion. Given a ground-truth embedding (where we want to go), and a current hypothesis text and its embedding (where we are right now), we can train a corrector model that&#x2019;s trained to output something that&#x2019;s closer to the ground-truth than the hypothesis.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://lh7-us.googleusercontent.com/ne0JB3F3WLFoTQR0fSgdxsmL6Ap4anP767qyjzNaySpkyu_uyAJEHnbzvsTmOsfsZOI6xMO1vhWWMp6vp_n_DMtAap-XucXKtH40_yctKbaUYQqBeWSbZEnhX3-LYZ1xzIvY-PMyO1kMh53DUCoGBXQ" class="kg-image" alt="Do text embeddings perfectly encode text?" loading="lazy" width="624" height="247"><figcaption>Overview of our method, Vec2Text. Given access to a target embedding e (blue) and query access to an embedding model &#x3D5; (blue model), the system aims to iteratively generate (yellow model) hypotheses &#x2C6;e (pink) to reach the target.</figcaption></figure><p>Our goal is now clear: we want to build a system that can take a ground-truth embedding, a hypothesis text sequence, and the hypothesis position in embedding space, and predict the true text sequence. We think of this as a type of &#x2018;learned optimization&#x2019; where we&#x2019;re taking steps in embedding space in the form of discrete sequences. This is the essence of our method, which we call vec2text.</p><p>After working through some details and training the model, this process works extremely well! A single forward pass of correction increases the BLEU score from 30 to 50. And one benefit of this model is that it can naturally be queried recursively. Given a current text and its embedding, we can run many steps of this optimization, iteratively generating hypotheses, re-embedding them, and feeding them back in as input to the model. With 50 steps and a few tricks, we can get back 92% of 32-token sequences exactly, and get to a BLEU score of 97! (Generally achieving BLEU score of 97 means we&#x2019;re almost perfectly reconstructing every sentence, perhaps with a few punctuation marks misplaced here and there.)<br></p><h3 id="scaling-and-future-work">Scaling and future work </h3><p>The fact that text embeddings can be perfectly inverted raises many follow-up questions. For one, the text embedding vector contains a fixed number of bits; there must be some sequence length at which information can no longer be perfectly stored within this vector. Even though we can recover most texts of length 32, some embedding models can embed documents up to thousands of tokens. We leave it up to future work to analyze the relationship between text length, embedding size, and embedding invertibility.</p><p>Another open question is how to build systems that can defend against inversion. Is it possible to create models that can successfully embed text such that embeddings remain useful while obfuscating the text that created them?</p><p>Finally, we are excited to see how our method might apply to other modalities. The main idea behind vec2text (a sort of iterative optimization in embedding space) doesn&#x2019;t use any text-specific tricks. It&#x2019;s a method that iteratively recovers information contained in any fixed input, given black-box access to a model. It remains to be seen how these ideas might apply to inverting embeddings from other modalities as well as to approaches more general than embedding inversion.</p><p>To use our models to invert text embeddings, or to get started running embedding inversion experiments yourself, check out our Github repository: <a href="https://github.com/jxmorris12/vec2text">https://github.com/jxmorris12/vec2text</a></p><hr><h3 id="references"><strong>References</strong></h3><p>Inverting Visual Representations with Convolutional Networks (2015), <a href="https://arxiv.org/abs/1506.02753">https://arxiv.org/abs/1506.02753</a></p><p>Understanding Invariance via Feedforward Inversion of Discriminatively Trained Classifiers (2021), <a href="https://proceedings.mlr.press/v139/teterwak21a/teterwak21a.pdf">https://proceedings.mlr.press/v139/teterwak21a/teterwak21a.pdf</a></p><p>Text Embeddings Reveal (Almost) As Much As Text (2023), <a href="https://arxiv.org/abs/2310.06816">https://arxiv.org/abs/2310.06816</a></p><p>Language Model Inversion (2024), <a href="https://arxiv.org/abs/2311.13647">https://arxiv.org/abs/2311.13647</a></p><h3 id="author-bio"><strong>Author Bio</strong></h3><p>Jack Morris is a PhD student at Cornell Tech in New York City. He works on research at the intersection of machine learning, natural language processing, and security. He&#x2019;s especially interested in the information content of deep neural representations like embeddings and classifier outputs.</p><h3 id="citation">Citation</h3><p>For attribution in academic contexts or books, please cite this work as</p><pre><code>Jack Morris, &quot;Do text embeddings perfectly encode text?&quot;, The Gradient, 2024.
</code></pre><p>BibTeX citation:</p><pre><code>@article{morris2024inversion,
    author = {Jack Morris},
    title = {Do text embeddings perfectly encode text?},
    journal = {The Gradient},
    year = {2024},
    howpublished = {\url{https://thegradient.pub/text-embedding-inversion},
}</code></pre>]]></content:encoded></item><item><title><![CDATA[Why Doesnt My Model Work?]]></title><description><![CDATA[Have you ever trained a model you thought was good, but then it failed miserably when applied to real world data? If so, youre in good company.]]></description><link>https://thegradient.pub/why-doesnt-my-model-work/</link><guid isPermaLink="false">65ce1b5993571d5c8c1549b8</guid><category><![CDATA[Machine Learning]]></category><category><![CDATA[Overviews]]></category><dc:creator><![CDATA[Michael Lones]]></dc:creator><pubDate>Sat, 24 Feb 2024 18:41:54 GMT</pubDate><media:content url="https://thegradient.pub/content/images/2024/02/shutterstock_2097995584.png" medium="image"/><content:encoded><![CDATA[<img src="https://thegradient.pub/content/images/2024/02/shutterstock_2097995584.png" alt="Why Doesn&#x2019;t My Model Work?"><p>Have you ever trained a model you thought was good, but then it failed miserably when applied to real world data? If so, you&#x2019;re in good company. Machine learning processes are complex, and it&#x2019;s very easy to do things that will cause overfitting without it being obvious. In the 20 years or so that I&#x2019;ve been working in machine learning, I&#x2019;ve seen many examples of this, prompting me to write &#x201C;<a href="https://arxiv.org/abs/2108.02497">How to avoid machine learning pitfalls: a guide for academic researchers</a>&#x201D; in an attempt to prevent other people from falling into these traps.</p><p>But you don&#x2019;t have to take my word for it. These issues are being increasingly reported in both the scientific and popular press. Examples include the observation that <a href="https://www.nature.com/articles/s42256-021-00307-0">hundreds of models developed during the Covid pandemic simply don&#x2019;t work</a>, and that a water quality system deployed in Toronto regularly <a href="https://www.theinformation.com/articles/when-artificial-intelligence-isnt-smarter">told people it was safe to bathe in dangerous water</a>. Many of these are documented in the <a href="https://www.aiaaic.org/home">AIAAIC repository</a>. It&#x2019;s even been suggested that these machine learning missteps are <a href="https://doi.org/10.1038/d41586-022-02035-w">causing a reproducibility crisis in science</a> &#x2014; and, given that many scientists use machine learning as a key tool these days, a lack of trust in published scientific results.</p><p>In this article, I&#x2019;m going to talk about some of the issues that can cause a model to seem good when it isn&#x2019;t. I&#x2019;ll also talk about some of the ways in which these kinds of mistakes can be prevented, including the use of the recently-introduced <a href="https://reforms.cs.princeton.edu/">REFORMS checklist for doing ML-based science</a>.</p><h2 id="duped-by-data">Duped by Data</h2><p>Misleading data is a good place to start, or rather not a good place to start, since the whole machine learning process rests upon the data that&#x2019;s used to train and test the model.</p><p>In the worst cases, misleading data can cause the phenomenon known as <em>garbage in garbage out</em>; that is, you can train a model, and potentially get very good performance on the test set, but the model has no real world utility. Examples of this can be found in the aforementioned <a href="https://www.nature.com/articles/s42256-021-00307-0">review of Covid prediction models by Roberts et al</a>. In the rush to develop tools for Covid prediction, a number of public datasets became available, but these were later found to contain misleading signals &#x2014; such as overlapping records, mislabellings and hidden variables &#x2014; all of which helped models to accurately predict the class labels without learning anything useful in the process.</p><p>Take hidden variables. These are features that are present in data, and which happen to be predictive of class labels within the data, but which are not directly related to them. If your model latches on to these during training, it will appear to work well, but may not work on new data. For example, in many Covid chest imaging datasets, the orientation of the body is a hidden variable: people who were sick were more likely to have been scanned lying down, whereas those who were standing tended to be healthy. Because they learnt this hidden variable, rather than the true features of the disease, many Covid machine learning models turned out to be good at predicting posture, but bad at predicting Covid. Despite their name, these hidden variables are often in plain sight, and there have been many examples of classifiers latching onto boundary markers, watermarks and timestamps embedded in images, which often serve to distinguish one class from another without having to look at the actual data.</p><p>A related issue is the presence of spurious correlations. Unlike hidden variables, these have no true relationship to anything else in the data; they&#x2019;re just patterns that happen to correlate with the class labels. A classic example is <a href="https://www.gwern.net/Tanks">the tank problem</a>, where the US military allegedly tried to train a neural network to identify tanks, but it actually recognised the weather, since all the pictures of tanks were taken at the same time of day. Consider the images below: a machine learning model could recognise all the pictures of tanks in this dataset just by looking at the colour of pixels towards the top of an image, without having to consider the shape of any of the objects. The performance of the model would appear great, but it would be completely useless in practice.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://thegradient.pub/content/images/2024/02/1.png" class="kg-image" alt="Why Doesn&#x2019;t My Model Work?" loading="lazy" width="1379" height="563" srcset="https://thegradient.pub/content/images/size/w600/2024/02/1.png 600w, https://thegradient.pub/content/images/size/w1000/2024/02/1.png 1000w, https://thegradient.pub/content/images/2024/02/1.png 1379w" sizes="(min-width: 720px) 720px"><figcaption>(Source: by author)</figcaption></figure><p>Many (perhaps most) datasets contain spurious correlations, but they&#x2019;re not usually as obvious as this one. Common computer vision benchmarks, for example, are <a href="https://arxiv.org/abs/1905.02175">known to have groups of background pixels that are spuriously correlated with class labels</a>. This represents a particular challenge to deep learners, which have the capacity to model many patterns within the data; various studies have shown that they do tend to capture spuriously correlated patterns, and this reduces their generality. Sensitivity to adversarial attacks is one consequence of this: if a deep learning model bases its prediction on spurious correlations in the background pixels of an image, then making small changes to these pixels can flip the prediction of the model. Adversarial training, where a model is exposed to adversarial samples during training, can be used to address this, but it&#x2019;s expensive. An easier approach is just to look at your model, and see what information it&#x2019;s using to make its decisions. For instance, if a saliency map produced by an explainable AI technique suggests that your model is focusing on something in the background, then it&#x2019;s probably not going to generalise well.</p><p>Sometimes it&#x2019;s not the data itself that is problematic, but rather the labelling of the data. This is especially the case when data is labelled by humans, and the labels end up capturing biases, misassumptions or just plain old mistakes made by the labellers. Examples of this can be seen in datasets used as image classification benchmarks, such as MNIST and CIFAR, which typically have a mislabelling rate of a couple of percent &#x2014; not a huge amount, but pretty significant where modellers are fighting over accuracies in the tenths of a percent. That is, if your model does slightly better than the competition, is it due to an actual improvement, or due to modelling noise in the labelling process? Things can be even more troublesome when working with data that has implicit subjectivity, such as sentiment classification, where there&#x2019;s a danger of overfitting particular labellers.</p><h2 id="led-by-leaks">Led by Leaks</h2><p>Bad data isn&#x2019;t the only problem. There&#x2019;s plenty of scope for mistakes further down the machine learning pipeline. A common one is data leakage. This happens when the model training pipeline has access to information it shouldn&#x2019;t have access to, particularly information that confers an advantage to the model. Most of the time, this manifests as information leaks from the test data &#x2014; and whilst most people know that test data should be kept independent and not explicitly used during training, there are various subtle ways that information can leak out.<br></p><p>One example is performing a data-dependent preprocessing operation on an entire dataset, before splitting off the test data. That is, making changes to all the data using information that was learnt by looking at all the data. Such operations vary from the simple, such as centering and scaling numerical features, to the complex, such as feature selection, dimensionality reduction and data augmentation &#x2014; but they all have in common the fact that they use knowledge of the whole dataset to guide their outcome. This means that knowledge of the test data is implicitly entering the model training pipeline, even if it is not explicitly used to train the model. As a consequence, any measure of performance derived from the test set is likely to be an overestimate of the model&#x2019;s true performance.</p><p>Let&#x2019;s consider the simplest example: centering and scaling. This involves looking at the range of each feature, and then using this information to rescale all the values, typically so that the mean is 0 and the standard deviation is 1. If this is done on the whole dataset before splitting off the test data, then the scaling of the training data will include information about the range and distribution of the feature values in the test set. This is particularly problematic if the range of the test set is broader than the training set, since the model could potentially infer this fact from the truncated range of values present in the training data, and do well on the test set just by predicting values higher or lower than those which were seen during training. For instance, if you&#x2019;re working on stock price forecasting from time series data with a model that takes inputs in the range 0 to 1 but it only sees values in the range 0 to 0.5 during training, then it&#x2019;s not too hard for it to infer that stock prices will go up in the future.</p><p>In fact, forecasting is an area of machine learning that is particularly susceptible to data leaks, due to something called <em>look ahead bias</em>. This occurs when information the model shouldn&#x2019;t have access to leaks from the future and artificially improves its performance on the test set. This commonly happens when the training set contains samples that are further ahead in time than the test set. I&#x2019;ll give an example later of when this can happen, but if you work in this area, I&#x2019;d also strongly recommend taking a look at <a href="https://doi.org/10.1007/s10618-022-00894-5">this excellent review of pitfalls and best practices in evaluating time series forecasting models</a>.</p><p>An example of a more complex data-dependent preprocessing operation leading to overly-optimistic performance metrics can be found in <a href="https://arxiv.org/abs/2001.06296">this review of pre-term birth prediction models</a>. Basically, a host of papers reported high accuracies at predicting whether a baby would be born early, but it turned out that all had applied data augmentation to the data set before splitting off the test data. This resulted in the test set containing augmented samples of training data, and the training set containing augmented samples of test data &#x2014; which amounted to a pretty significant data leak. When the authors of the review corrected this, the predictive performance of the models dropped from being near perfect to not much better than random.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://thegradient.pub/content/images/2024/02/2.png" class="kg-image" alt="Why Doesn&#x2019;t My Model Work?" loading="lazy" width="1379" height="735" srcset="https://thegradient.pub/content/images/size/w600/2024/02/2.png 600w, https://thegradient.pub/content/images/size/w1000/2024/02/2.png 1000w, https://thegradient.pub/content/images/2024/02/2.png 1379w" sizes="(min-width: 720px) 720px"><figcaption>(Source: https://arxiv.org/abs/2108.02497)</figcaption></figure><p>Oddly, one of the most common examples of data leakage doesn&#x2019;t have an agreed name (the terms <a href="https://doi.org/10.1101/078816">overhyping</a> and <a href="https://arxiv.org/abs/2108.02497">sequential overfitting</a> have been suggested) but is essentially a form of <em>training to the test set</em>. By way of example, imagine the scenario depicted above where you&#x2019;ve trained a model and evaluated it on the test set. You then decided its performance was below where you wanted it to be. So, you tweaked the model, and then you reevaluated it. You still weren&#x2019;t happy, so you kept on doing this until its performance on the test set was good enough. Sounds familiar? Well, this is a common thing to do, but if you&#x2019;re developing a model iteratively and using the same test set to evaluate the model after each iteration, then you&#x2019;re basically using that test set to guide the development of the model. The end result is that you&#x2019;ll overfit the test set and probably get an over-optimistic measure of how well your model generalises.</p><p>Interestingly, the same process occurs when people use community benchmarks, such as MNIST, CIFAR and ImageNet. Almost everyone who works on image classification uses these data sets to benchmark their approaches; so, over time, it&#x2019;s inevitable that some overfitting of these benchmarks will occur. To mitigate against this, it&#x2019;s always advisable to use a diverse selection of benchmarks, and ideally try your technique on a data set which other people haven&#x2019;t used.</p><h2 id="misinformed-by-metrics">Misinformed by Metrics</h2><p>Once you&#x2019;ve built your model robustly, you then have to evaluate it robustly. There&#x2019;s plenty that can go wrong here too. Let&#x2019;s start with an inappropriate choice of metrics. The classic example is using accuracy with an imbalanced dataset. Imagine that you&#x2019;ve managed to train a model that always predicts the same label, regardless of its input. If half of the test samples have this label as their ground truth, then you&#x2019;ll get an accuracy of 50% &#x2014; which is fine, a bad accuracy for a bad classifier. If 90% of the test samples have this label, then you&#x2019;ll get an accuracy of 90% &#x2014; a good accuracy for a bad classifier. This level of imbalance is not uncommon in real world data sets, and when working with imbalanced training sets, it&#x2019;s not uncommon to get classifiers that always predict the majority label. In this case, it would be much better to use a metric like F score or Matthews correlation coefficient, since these are less sensitive to class imbalances. However, all metrics have their weaknesses, so it&#x2019;s always best to use a portfolio of metrics that give different perspectives on a model&#x2019;s performance and failure modes.</p><p>Metrics for time series forecasting are particularly troublesome. There are a lot of them to choose from, and the most appropriate choice can depend on both the specific problem domain and the exact nature of the time series data. Unlike metrics used for classification, many of the regression metrics used in time series forecasting have no natural scale, meaning that raw numbers can be misleading. For instance, the interpretation of mean squared errors depends on the range of values present in the time series. For this reason, it&#x2019;s important to use appropriate baselines in addition to appropriate metrics. As an example, this (already mentioned) <a href="https://doi.org/10.1007/s10618-022-00894-5">review of time series forecasting pitfalls</a> demonstrates how many of the deep learning models published at top AI venues are actually less good than naive baseline models. For instance, they show that an autoformer, a kind of complex transformer model designed for time series forecasting, can be beaten by a trivial model that predicts no change at the next time step &#x2014; something that isn&#x2019;t apparent from looking at metrics alone.</p><p>In general, there is a trend towards developing increasingly complex models to solve difficult problems. However, it&#x2019;s important to bear in mind that some problems may not be solvable, regardless of how complex the model becomes. This is probably the case for many financial time series forecasting problems. It&#x2019;s also the case when predicting certain natural phenomena, particularly those in which a chaotic component precludes prediction beyond a certain time horizon. For instance, many people think that earthquakes can not be predicted, yet there are a host of papers reporting good performance on this task. <a href="https://arxiv.org/abs/1910.01178">This review paper discusses how these correct predictions may be due to a raft of modelling pitfalls</a>, including inappropriate choice of baselines and overfitting due to data sparsity, unnecessary complexity and data leaks.</p><p>Another problem is assuming that a single evaluation is sufficient to measure the performance of a model. Sometimes it is, but a lot of the time you&#x2019;ll be working with models that are stochastic or unstable; so, each time you train them, you get different results. Or you may be working with a small data set where you might just get lucky with an easy test split. To address both situations, it is commonplace to use resampling methods like cross-validation, which train and test a model on different subsets of the data and then work out the average performance. However, resampling introduces its own risks. One of these is the increased risk of data leaks, particularly when assuming that data-dependent preprocessing operations (like centering and scaling and feature selection) only need to be done once. They don&#x2019;t; they need to be done independently for each iteration of the resampling process, and to do otherwise can cause a data leak. Below is an example of this, showing how feature selection should be done independently on the two training sets (in blue) used in the first two iterations of cross-validation, and how this results in different features being selected each time.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://thegradient.pub/content/images/2024/02/3.png" class="kg-image" alt="Why Doesn&#x2019;t My Model Work?" loading="lazy" width="1380" height="640" srcset="https://thegradient.pub/content/images/size/w600/2024/02/3.png 600w, https://thegradient.pub/content/images/size/w1000/2024/02/3.png 1000w, https://thegradient.pub/content/images/2024/02/3.png 1380w" sizes="(min-width: 720px) 720px"><figcaption>(Source: https://arxiv.org/abs/2108.02497)</figcaption></figure><p>As I mentioned earlier, the danger of data leaks is even greater when working with time series data. Using standard cross-validation, every iteration except one will involve using at least one training fold that is further ahead in time than the data in the test fold. For example, if you imagine that the data rows in the figure above represent time-ordered multivariate samples, then the test sets (in pink) used in both iterations occur earlier in the time series than all or part of the training data. This is an example of a look ahead bias. Alternative approaches, such as blocked cross-validation, can be used to prevent these.</p><p>Multiple evaluations aren&#x2019;t an option for everyone. For example, training a foundation model is both time-consuming and expensive, so doing it repeatedly is not feasible. Depending on your resources, this may be the case for even relatively small deep learning models. If so, then also consider using other methods for measuring the robustness of models. This includes things like using explainability analysis, performing ablation studies, or augmenting test data. These can allow you to look beyond potentially-misleading metrics and gain some appreciation of how a model works and how it might fail, which in turn can help you decide whether to use it in practice.</p><h2 id="falling-deeper">Falling Deeper</h2><p>So far, I&#x2019;ve mostly talked about general machine learning processes, but the pitfalls can be even greater when using deep learning models. Consider the use of latent space models. These are often trained separately to the predictive models that use them. That is, it&#x2019;s not unusual to train something like an autoencoder to do feature extraction, and then use the output of this model within the training of a downstream model. When doing this, it&#x2019;s essential to ensure that the test set used in the downstream model does not intersect with the training data used in the autoencoder &#x2014; something that can easily happen when using cross-validation or other resampling methods, e.g. when using different random splits or not selecting models trained on the same training folds.</p><p>However, as deep learning models get larger and more complex, it can be harder to ensure these kinds of data leaks do not occur. For instance, if you use a pre-trained foundation model, it may not be possible to tell whether the data used in your test set was used to train the foundation model &#x2014; particularly if you&#x2019;re using benchmark data from the internet to test your model. Things get even worse if you&#x2019;re using composite models. For example, if you&#x2019;re using a BERT-type foundation model to encode the inputs when fine-tuning a GPT-type foundation model, you have to take into account any intersection between the datasets used to train the two foundation models in addition to your own fine-tuning data. In practice, some of these data sets may be unknown, meaning that you can&#x2019;t be confident whether your model is correctly generalising or merely reproducing data memorised during pre-training.</p><h2 id="avoiding-the-pits">Avoiding the Pits</h2><p>These pitfalls are all too common. So, what&#x2019;s the best way to avoid them? Well, one thing you can do is use a checklist, which is basically a formal document that takes you through the key pain points in the machine learning pipeline, and helps you to identify potential issues. In domains with high-stakes decisions, such as medicine, there are already a number of well-established checklists, such as <a href="https://pubs.rsna.org/page/ai/claim">CLAIM</a>, and adherence to these is typically enforced by journals that publish in these areas.</p><p>However, I&#x2019;d like to briefly introduce a new kid on the block: <a href="https://reforms.cs.princeton.edu">REFORMS, a consensus-based checklist for doing machine learning-based science</a>. This was put together by 19 researchers across computer science, data science, mathematics, social sciences, and the biomedical sciences &#x2014; including myself &#x2014; and came out of a recent <a href="https://sites.google.com/princeton.edu/rep-workshop/">workshop on the reproducibility crisis in ML&#x2011;based science</a>. It is intended to address the common mistakes that occur in the machine learning pipeline, including many of those mentioned in this article, in a more domain-independent manner. It consists of two parts: the checklist itself, and also a paired guidance document, which explains why each of the checklist items are important. The checklist works through the main components of a machine learning-based study, in each case encouraging the user to verify that the machine learning process is designed in such a way that it supports the overall aims of the study, doesn&#x2019;t stumble into any of the common pitfalls, and enables the results to be verified by an independent researcher. Whilst it&#x2019;s focused on the application of machine learning within a scientific context, a lot of what it covers is more generally applicable, so I&#x2019;d encourage you to take a look even if you don&#x2019;t consider your work to be &#x201C;science&#x201D;.</p><p>Another way of avoiding pitfalls is to make better use of tools. Now, one of my pet gripes regarding the current state of machine learning is that commonly-used tools do little to prevent you from making mistakes. That is, they&#x2019;ll happily let you abuse the machine learning process in all sorts of ways without telling you what you&#x2019;re doing is wrong. Nevertheless, help is available in the form of experiment tracking frameworks, which automatically keep a record of the models you trained and how you trained them, and this can be useful for spotting things like data leaks and training to the test set. An open source option is <a href="https://mlflow.org">MLFlow</a>, but there are plenty of commercial offerings. MLOps tools take this even further, and help to manage all the moving parts in a machine learning workflow, including the people.</p><h2 id="final-thought">Final Thought</h2><p>It is possible to train a good model that generalises well to unseen data, but I wouldn&#x2019;t believe this until you&#x2019;re satisfied that nothing which could have gone wrong has gone wrong. A healthy sense of suspicion is a good thing: do look at your trained model to make sure it&#x2019;s doing something sensible, do analyse your metrics to understand where it&#x2019;s making mistakes, do calibrate your results against appropriate baselines, and do consider using checklists to make sure you haven&#x2019;t overlooked something important.</p><hr><h2 id="author-bio">Author Bio</h2><p>Michael is an Associate Professor at Heriot-Watt University, Edinburgh. He&#x2019;s spent the last 20 years or so doing research on machine learning and bio-inspired computing. For more info see his <a href="http://www.macs.hw.ac.uk/~ml355/">academic website</a>. He also writes about computer science more generally in his <a href="https://fetchdecodeexecute.substack.com/">Fetch Decode Execute</a> substack.</p><h2 id="citation">Citation</h2><p>For attribution in academic contexts or books, please cite this work as</p><pre><code>Michael Lones, &quot;Why Doesn&#x2019;t My Model Work?&quot;, The Gradient, 2024.
</code></pre><p>BibTeX citation:</p><pre><code>@article{lones2024why,
    author = {Michael Lones},
    title = {Why Doesn&#x2019;t My Model Work?},
    journal = {The Gradient},
    year = {2024},
    howpublished = {\url{https://thegradient.pub/why-doesnt-my-model-work},
}</code></pre><p></p>]]></content:encoded></item><item><title><![CDATA[Deep learning for single-cell sequencing: a microscope to see the diversity of cells]]></title><description><![CDATA[On the the pivotal role that Deep Learning has played as a key enabler for advancing single-cell sequencing technologies.]]></description><link>https://thegradient.pub/deep-learning-for-single-cell-sequencing-a-microscope-to-uncover-the-rich-diversity-of-individual-cells/</link><guid isPermaLink="false">65606b4793571d5c8c154793</guid><category><![CDATA[Overviews]]></category><category><![CDATA[Science]]></category><dc:creator><![CDATA[Fatima Zahra El Hajji]]></dc:creator><pubDate>Sat, 13 Jan 2024 18:12:44 GMT</pubDate><media:content url="https://thegradient.pub/content/images/2024/01/image4.jpg" medium="image"/><content:encoded><![CDATA[<img src="https://thegradient.pub/content/images/2024/01/image4.jpg" alt="Deep learning for single-cell sequencing: a microscope to see the diversity of cells"><p>The history of each living being is written in its genome, which is stored as DNA and present in nearly every cell of the body. No two cells are the same, even if they share the same DNA and cell type, as they still differ in the regulators that control how DNA is expressed by the cell. The human genome consists of 3 billion base pairs spread over 23 chromosomes. Within this vast genetic code, there are approximately 20,000 to 25,000 genes, constituting the protein-coding DNA and accounting for about 1% of the total genome [1]. To explore the functioning of complex systems in our bodies, especially this small coding portion of DNA, a precise sequencing method is necessary, and single-cell sequencing (sc-seq) technology fits this purpose.</p><p>In 2013, Nature selected single-cell RNA sequencing as the Method of the Year [2] (Figure 3), highlighting the importance of this method for exploring cellular heterogeneity through the sequencing of DNA and RNA at the individual cell level. Subsequently, numerous tools have emerged for the analysis of single-cell RNA sequencing data. For example, the scRNA-tools database has been compiling software for the analysis of single-cell RNA data since 2016, and by 2021, the database includes over 1000 tools [3]. Among these tools, many involve methods that leverage Deep Learning techniques, which will be the focus of this article &#x2013; we will explore the pivotal role that Deep Learning, in particular, has played as a key enabler for advancing single-cell sequencing technologies.</p><h2 id="background">Background</h2><h3 id="flow-of-genetic-information-from-dna-to-protein-in-cells">Flow of genetic information from DNA to protein in cells</h3><p>Let&#x2019;s first go over what exactly cells and sequences are.<strong> </strong>The cell is the fundamental unit of our bodies and the key to understanding how our bodies function in good health and how molecular dysfunction leads to disease. Our bodies are made of trillions of cells, and nearly every cell contains three genetic information layers: DNA, RNA, and protein. DNA is a long molecule containing the genetic code that makes each person unique. Like a source code, it includes several instructions showing how to make each protein in our bodies. These proteins are the workhorses of the cell that carry out nearly every task necessary for cellular life. For example, the enzymes that catalyze chemical reactions within the cell and DNA polymerases that contribute to DNA replication during cell division, are all proteins. The cell synthesizes proteins in two steps: Transcription and Translation (Figure 1), which are known as gene expression. DNA is first transcribed into RNA, then RNA is translated into protein. We can consider RNA as a messenger between DNA and protein.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://lh7-us.googleusercontent.com/FE0jl7A1RgTBL5tPgpxsw624oxkwsWMLx3lRvyCQYwjLLLIezvRjxHONrVsJ8IJEx1EjZkTklRUD09IPg4JxBLq_goJPdCDy3PH5kA0EPntuMIhso_8R0Vouja3yZtU4rG1eYGarcsGjT4naVbPD2Q" class="kg-image" alt="Deep learning for single-cell sequencing: a microscope to see the diversity of cells" loading="lazy" width="447" height="204"><figcaption><em><strong>Figure 1</strong>. The central dogma of biology</em></figcaption></figure><p>While the cells of our body share the same DNA, they vary in their biological activity. For instance, the distinctions between immune cells and heart cells are determined by the genes that are either activated or deactivated in these cells. Generally, when a gene is activated, it leads to the creation of more RNA copies, resulting in increased protein production. Therefore, as cell types differ based on the quantity and type of RNA/protein molecules synthesized, it becomes intriguing to assess the abundance of these molecules at the single-cell level. This will enable us to investigate the behavior of our DNA &#xA0;within each cell and attain a high-resolution perspective of the various parts of our bodies.</p><p>In general, all single-cell sequencing technologies can be divided into three main steps:</p><ol><li>Isolation of single cells from the tissue of interest and extraction of genetic material from each isolated cell</li><li>Amplification of genetic material from each isolated cell and library preparation</li><li>Sequencing of the library using a next-generation sequencer and data analysis</li></ol><p>Navigating through the intricate steps of cellular biology and single-cell sequencing technologies, a pivotal question emerges: How is single-cell sequencing data represented numerically?</p><h3 id="structure-of-single-cell-sequencing-data">Structure of single-cell sequencing data</h3><p>The structure of single-cell sequencing data takes the form of a matrix (Figure 2), where each row corresponds to a cell that has been sequenced and annotated with a unique barcode. The number of rows equals the total number of cells analyzed in the experiment. On the other hand, each column corresponds to a specific gene. Genes are the functional units of the genome that encode instructions for the synthesis of proteins or other functional molecules. In the case of scRNA seq data, the numerical entries in the matrix represent the expression levels of genes in individual cells. These values indicate the amount of RNA produced from each gene in a particular cell, providing insights into the activity of genes within different cells.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://lh7-us.googleusercontent.com/qTHPAYL5g0S2KEbMKrt3HXoqTmPqeGpo2cqPyFN_NXzE_jUgC1H7C67igubfJOY_mfLXyILiPlQxW0Z3FOdisQ6wZ9cnN72mqeJJ9_Aa2x7qs79DHAgsZtuJjrMXpRe79TNtEhrpb10hqofcO_9aTA" class="kg-image" alt="Deep learning for single-cell sequencing: a microscope to see the diversity of cells" loading="lazy" width="280" height="285"><figcaption><em><strong>Figure 2</strong>. Schema of single-cell sequencing data</em></figcaption></figure><h3 id="single-cell-sequencing-overview"><strong>Single Cell Sequencing Overview</strong></h3><p>For more than 150 years, biologists have wanted to identify all the cell types in the human body and classify them into distinct types based on accurate descriptions of their properties. The Human Cell Atlas Project (HCAP), the genetic equivalent of the Human Genome Project [4], is an international collaborative effort to map all the cells in the human body.&#x201D; We can conceptualize the Human Cell Atlas as a map endeavoring to portray the human body coherently and systematically. Much like Google Maps, which allows us to zoom in for a closer examination of intricate details, the Human Cell Atlas provides insights into spatial information, internal attributes, and even the relationships among elements&#x201D;, explains Aviv Regev, a computational and systems biologist at the Broad Institute of MIT and Harvard and Executive Vice President and Head of Genentech Research.</p><p>This analogy seamlessly aligns with the broader impact of single-cell sequencing, since it allows the analysis of individual cells instead of bulk populations. This technology proves invaluable in addressing intricate biological inquiries related to developmental processes and comprehending heterogeneous cellular or genetic changes under various treatment conditions or disease states. Additionally, it facilitates the identification of novel cell types within a given cellular population. The initiation of the first single-cell RNA sequencing (scRNA-seq) paper in 2009 [5], subsequently designated as the &quot;method of the year&quot; in 2013 [2], marked the genesis of an extensive endeavor to advance both experimental and computational techniques dedicated to unraveling the intricacies of single-cell transcriptomes.</p><p>As the technological landscape evolves, the narrative transitions to the advancements in single-cell research, particularly the early focus on single-cell RNA sequencing (scRNA-seq) due to its cost-effectiveness in studying complex cell populations.&#x201D; In some ways, RNA has always been one of the easiest things to measure,&#x201D; says Satija [6], a researcher at the New York Genome Center (NYGC). &#xA0;Yet, the rapid development of single-cell technology has ushered in a new era of possibilities&#x2014;multimodal single-cell data integration. Recognized as the &quot;Method of the Year 2019&quot; by Nature [7] (Figure 3), this approach allows the measurement of different cellular modalities, including the genome, epigenome, and proteome, within the same cell. The layering of multiple pieces of information provides powerful insights into cellular identity, posing the challenge of effectively modeling and combining datasets generated from multimodal measurements. This integration challenge is met with the introduction of Multi-view learning [8] methods, exploring common variations across modalities. This sophisticated approach, incorporating deep learning techniques, showcases relevant results across various fields, particularly in biology and biomedicine.</p><p>Amidst these advancements, a distinct challenge surfaces in the persistent limitation of single-cell RNA sequencing&#x2014;the loss of spatial information during transcriptome profiling by isolating cells from their original position. Spatially resolved transcriptomics (SRT) emerges as a pivotal solution [9], addressing the challenge by preserving spatial details during the study of complex biological systems. This recognition of spatially resolved transcriptomics as the method of the year 2020 solidifies its place as a critical solution to the challenges inherent in advancing our understanding of complex biological systems.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://lh7-us.googleusercontent.com/wuqGlu6SprK1ganCDuvgJLJqHhfPhGHpO2xZTSYQ0orB3PLdbAhYwZNXUPVrbRWjH9bHimSgLOsEEDhTKZumcHovsqUqEGo5GILDf74QkaXAiqdpGUdjbda7XYsmrqbsh9sH_ASz6auWXTCtSbHF4g" class="kg-image" alt="Deep learning for single-cell sequencing: a microscope to see the diversity of cells" loading="lazy" width="591" height="177"><figcaption><em><strong>Figure 3</strong>. Evolution of single-cell sequencing over time</em></figcaption></figure><p>Having explored the panorama of single-cell sequencing, let us now delve into the role of deep learning in the context of single-cell sequencing.</p><h3 id="deep-learning-on-single-cell-sequencing">Deep Learning on single-cell sequencing</h3><p>Deep learning is increasingly employed in single-cell analysis due to its capacity to handle the complexity of single-cell sequencing data. In contrast, conventional machine-learning approaches require significant effort to develop a feature engineering strategy, typically designed by domain experts. The deep learning approach, however, autonomously captures relevant characteristics from single-cell sequencing data, addressing the heterogeneity between single-cell sequencing experiments, as well as the associated noise and sparsity in such data. Below are three key reasons for the application of deep learning in single-cell sequencing:</p><ul><li><strong>High-Dimensional Data</strong>: Single-cell sequencing generates high-dimensional data, with thousands of genes and their expression levels measured for each cell. Deep learning models are adept at capturing complex relationships and patterns within this data, which can be challenging for traditional statistical methods.</li><li><strong>Non-Linearity:</strong> Single-cell gene expression data is characterized by its inherent nonlinearity between gene expressions and cell-to-cell heterogeneity. Traditional statistical methods encounter difficulties in capturing the non-linear relationships present in single-cell gene expression data. In contrast, deep learning models are flexible and able to learn complex non-linear mappings.</li><li><strong>Heterogeneity:</strong> Single-cell data is often characterized by diverse cell populations with varying gene expression profiles, presenting a complex landscape. Deep learning models can play a crucial role in identifying, clustering, and characterizing these distinct cell types or subpopulations, thereby facilitating a deeper understanding of cellular heterogeneity within a sample.</li></ul><p>As we explore the reasons behind using deep learning in single-cell sequencing data, it leads us to the question: What deep learning architectures are often used in sc-seq data analysis?</p><h3 id="background-on-autoencoders">Background on Autoencoders</h3><p>Autoencoders (AEs) stand out among various deep-learning architectures (such as GANs and RNNs) as an especially relied upon method for decoding the complexities of single-cell sequencing data. &#xA0;Widely employed for dimensionality reduction while preserving the inherent heterogeneity in the single-cell sequencing data. By clustering cells in the reduced-dimensional space generated by autoencoders, researchers can effectively identify and characterize different cell types or subpopulations. This approach enhances our ability to discern and analyze the diverse cellular components within single-cell datasets. In contrast to non-deep learning models, such as principal component analysis (PCA), which are integral components of established scRNA-seq data analysis software like Seurat [10], autoencoders distinguish themselves by uncovering non-linear manifolds. While PCA is constrained to linear transformations, the flexibility of autoencoders to capture complex non-linear mappings makes it an advanced method to find nuanced relationships embedded in single-cell genomics.</p><p>To mitigate the overfitting challenge associated with autoencoders, several enhancements to the autoencoder structure have been implemented, specifically tailored to offer advantages in the context of sc-seq data. One notable adaptation often used in the context of sc-seq data is the denoising autoencoder (DAEs), which amplifies the autoencoder&apos;s reconstruction capability by introducing noise to the initial network layer. This involves randomly transforming some of its units to zero. The Denoising Autoencoder then reconstructs the input from this intentionally corrupted version, empowering the network to capture more relevant features and preventing it from merely memorizing the input (overfitting). This refinement significantly bolsters the model&apos;s resilience against data noise, thereby elevating the quality of the low-dimensional representation of samples (i.e., bottleneck) derived from the sc-seq data.</p><p>A third variation of autoencoders frequently employed in sc-seq data analysis is variational autoencoders (VAEs), exemplified by models like scGen [19], scVI [14], scANVI [28], etc. VAEs, as a type of generative model, learn a latent representation distribution of the data. Instead of encoding the data into a vector of p-dimensional latent variables, the data is encoded into two vectors of size p: a vector of means &#x3B7; and a vector of standard deviations &#x3C3;. VAEs introduce a probabilistic element to the encoding process, facilitating the generation of synthetic single-cell data and offering insights into the diversity within a cell population. This nuanced approach adds another layer of complexity and richness to the exploration of single-cell genomics.</p><h2 id="applications-of-deep-learning-in-sc-seq-data-analysis">Applications of deep learning in sc-seq data analysis</h2><p>This section outlines the main applications of deep learning in improving various stages of sc-seq data analysis, highlighting its effectiveness in advancing crucial aspects of the process.</p><h3 id="scrna-seq-data-imputation-and-denoising">scRNA-seq data imputation and denoising</h3><p>Single-cell RNA sequencing (scRNA-seq) data encounter inherent challenges, with dropout events being a prominent concern that leads to significant issues&#x2014;resulting in sparsity within the gene expression matrix, often characterized by a substantial number of zero values. This sparsity significantly shapes downstream bioinformatics analyses. Many of these zero values arise artificially due to deficiencies in sequencing techniques, including problems like inadequate gene expression, low capture rates, sequencing depth, or other technical factors. As a consequence, the observed zero values do not accurately reflect the true underlying expression levels. Hence, not all zeros in scRNA-seq data can be considered mere missing values, deviating from the conventional statistical approach of imputing missing data values. Given the intricate distinction between true and false zero counts, traditional imputation methods with predefined missing values may prove inadequate for scRNA-seq data. For instance, a classical imputation method, like Mean Imputation, might entail substituting these zero values with the average expression level of that gene across all cells. However, this approach runs the risk of oversimplifying the complexities introduced by dropout events in scRNA-seq data, potentially leading to biased interpretations.</p><p>ScRNA-seq data imputation methods can be divided into two categories: deep learning&#x2013;based imputation method and non&#x2013;deep learning imputation method. The non&#x2013;deep learning imputation algorithms involve fitting statistical probability models or utilizing the expression matrix for smoothing and diffusion. This simplicity renders it effective for certain types of samples. For example, Wagner et al. [11] utilized the k-nearest neighbors (KNN) method, identifying nearest neighbors between cells and aggregating gene-specific Unique Molecular Identifiers (UMI) counts to impute the gene expression matrix. In contrast, Huang et al. [12] proposed the SVAER algorithm, leveraging gene-to-gene relationships for imputing the gene expression matrix. For larger datasets (comprising tens of thousands or more), high-dimensional, sparse, and complex scRNA-seq data, traditional computational methods face difficulties, often rendering analysis using these methods difficult and infeasible. Consequently, many researchers have turned to designing methods based on deep learning to address these challenges.</p><p>Most deep learning algorithms for imputing dropout events are based on autoencoders (AEs). For instance, in 2018, Eraslan et al. [13] introduced the deep count autoencoder (DCA). DCA utilizes a deep autoencoder architecture to address dropout events in single-cell RNA sequencing (scRNA-seq) data. It incorporates a probabilistic layer in the decoder to model the dropout process. This probabilistic layer accommodates the uncertainty associated with dropout events, enabling the model to generate a distribution of possible imputed values. To capture the characteristics of count data in scRNA-seq, DCA models the observed counts as originating from a negative binomial distribution.</p><p>Single-cell variational inference (scVI) is another deep learning algorithm introduced by Lopez et al. [14]. ScVI is a probabilistic variational autoencoder (VAE) that combines deep learning and probabilistic modeling to capture the underlying structure of the scRNA-seq data. &#xA0;ScVI can be used for imputation, denoising, and various other tasks related to the analysis of scRNA-seq data. In contrast to the DCA model, scVI employs Zero-Inflated Negative Binomial (ZINB) distribution in the decoder part to generate a distribution of possible counts for each gene in each cell. The Zero-Inflated Negative Binomial (ZINB) distribution allows modeling the probability of a gene expression being zero (to model dropout events) as well as the distribution of positive values (to model non-zero counts).</p><p>Additionally, another study addressed the scRNA-seq data imputation challenge by introducing a recurrent network layer in their model, known as scScope [15]. This novel architecture iteratively performs imputations on zero-valued entries of input scRNA-seq data. The flexibility of scScope&apos;s design allows for the iterative improvement of imputed outputs through a chosen number of recurrent steps (T). Noteworthy is the fact that reducing the time recurrence of scScope to one (i.e., T&#x2009;=&#x2009;1) transforms the model into a traditional autoencoder (AE). As scScope is essentially a modification of traditional AEs, its runtime is comparable to other AE-based models.</p><p>It&apos;s important to note that the application of deep learning in scRNA-seq data imputation and denoising is particularly advantageous due to its ability to capture non-linear relationships among genes. This contrasts with standard linear approaches, making deep learning more adept at providing informed and accurate imputation strategies in the context of single-cell genomics.</p><h3 id="batch-effect-removal">Batch effect removal</h3><p>Single-cell data is commonly aggregated from diverse experiments that vary in terms of experimental laboratories, protocols, sample compositions, and even technology platforms. These differences result in significant variations or batch effects within the data, posing a challenge in the analysis of biological variations of interest during the process of data integration. To address this issue, it becomes necessary to correct batch effects by removing technical variance when integrating cells from different batches or studies. The first method that appears for batch correction is a linear method based on linear regression such as Limma package [16] that provides the removeBatchEffect function which fits a linear model that considers the batches and their impact on gene expression. &#xA0;After fitting the model, it sets the coefficients associated with each batch to zero, effectively removing their impact. Another method called ComBat [17] does something similar but adds an extra step to refine the process, making the correction even more accurate by using a technique called empirical Bayes shrinkage.</p><p>However, batch effects can be highly nonlinear, making it difficult to correctly align different datasets while preserving key biological variations. In 2018, Haghverdi et al. introduced the Mutual Nearest Neighbors (MNN) algorithm to identify pairs of cells from different batches in single-cell data [18]. These identified mutual nearest neighbors aid in estimating batch effects between batches. By applying this correction, the gene expression values are adjusted to account for the estimated batch effects, aligning them more closely and reducing discrepancies introduced by the different batches. For extensive single-cell datasets with highly nonlinear batch effects, traditional methods may prove less effective, prompting researchers to explore the application of neural networks for improved batch correction.</p><p>One of the pioneering models that employ deep learning for batch correction is the scGen model. Developed by Lotfollahi et al., ScGen [19] utilizes a variational autoencoder (VAE) architecture. This involves pre-training a VAE model on a reference dataset to adjust real single-cell data and alleviate batch effects. Initially, the VAE is trained to capture latent features within the reference dataset&apos;s cells. Subsequently, this trained VAE is applied to the actual data, producing latent representations for each cell. The adjustment of gene expression profiles is then based on aligning these latent representations, to reduce batch effects and harmonize profiles across different experimental conditions.<br></p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://lh7-us.googleusercontent.com/FDIttLe4zUalLYZO1aXTVcBQvPlijO5VVaCmkIoUHYfTbQ0hvOx2iViXlpT14Sj5hmUng9bF63T4NwwvIlKiYYXNTvPo6ArVhqphhq7CLB5Tc75FNTZjCXujHWif8ECGJtOlav-7R1G5ttS8fbRPPQ" class="kg-image" alt="Deep learning for single-cell sequencing: a microscope to see the diversity of cells" loading="lazy" width="515" height="277"><figcaption><em><strong>Figure 4.</strong> scGen removes batch effects [19]. a, UMAP visualization of 4 technically diverse pancreatic datasets with their corresponding batch and cell types. b, Data corrected by scGen mixes shared cell types from different studies while preserving the biological variance of cells.</em></figcaption></figure><p>On the other hand, Zou et al. introduced DeepMNN [20], which employs a residual neural network and the mutual nearest neighbor (MNN) algorithm for scRNA-seq data batch correction. Initially, MNN pairs are identified across batches in a principal component analysis (PCA) subspace. Subsequently, a batch correction network is constructed using two stacked residual blocks to remove batch effects. The loss function of DeepMNN comprises a batch loss, computed based on the distance between cells in MNN pairs in the PCA subspace, and a weighted regularization loss, ensuring the network&apos;s output similarity to the input.</p><p>The majority of existing scRNA-seq methods are designed to remove batch effects first and then cluster cells, which potentially overlooks certain rare cell types. Recently, Xiaokang et al. developed scDML [21], a deep metric learning model to remove batch effect in scRNA-seq data, guided by the initial clusters and the nearest neighbor information intra and inter-batches. First, the graph-based clustering algorithm is used to group cells based on gene expression similarities, then the KNN algorithm is applied to identify k-nearest neighbors for each cell in the dataset, and the MNN algorithm to identify mutual nearest neighbors, focusing on reciprocal relationships between cells. To remove batch effects, deep triplet learning is employed, considering hard triplets. This helps in learning a low-dimensional embedding that accounts for the original high-dimensional gene expression and removes batch effects simultaneously.</p><h3 id="cell-type-annotation">Cell type annotation</h3><p>Cell type annotation in single-cell sequencing involves the process of identifying and labeling individual cells based on their gene expression profiles, which allows researchers to capture the diversity within a heterogeneous population of cells, and understand the cellular composition of tissues, and the functional roles of different cell types in biological processes or diseases. &#xA0;Traditionally, researchers have used manual methods [22] to annotate cell sub-populations. This involves identifying gene markers or gene signatures that are differentially expressed in a specific cell cluster. Once gene markers are identified, researchers manually interpret the biological relevance of these markers to assign cell-type labels to the clusters. This traditional manual annotation approach is time-consuming and requires considerable human effort, especially when dealing with large-scale single-cell datasets. Due to the challenges associated with manual annotation, researchers are turning to automate and streamline the cell annotation process.</p><p>Two primary strategies are employed for cell type annotation: unsupervised-based and supervised-based. In the unsupervised realm, clustering methods such as Scanpy [23] and Seurat [10] are utilized, demanding prior knowledge of established cellular markers. The identification of clusters hinges on the unsupervised grouping of cells without external reference information. However, a drawback to this approach is a potential decrease in replicability with an increased number of clusters and multiple selections of cluster marker genes.</p><p>Conversely, supervised-based strategies rely on deep-learning models trained on labeled data. These models discern intricate patterns and relationships within gene expression data during training, enabling them to predict cell types for unlabeled data based on acquired patterns. For example, Joint Integration and Discrimination (JIND) [24] &#xA0; deploys a GAN-style deep architecture, where an encoder is pre-trained on classification tasks, circumventing the need for an autoencoder framework. This model also accounts for batch effects. AutoClass [25] integrates an autoencoder and a classifier, combining output reconstruction loss with a classification loss for cell annotation alongside data imputation. Additionally, TransCluster, [26] rooted in the Transformer framework and convolutional neural network (CNN), employs feature extraction from the gene expression matrix for single-cell annotation.</p><p>Despite the power of deep neural networks, obtaining a large number of accurately and unbiasedly annotated cells for training is challenging, given the labor-intensive manual inspection of marker genes in scRNAseq data. In response, semi-supervised learning has been leveraged in computational cell annotation. For instance, the SemiRNet [27] model uses both unlabeled and a limited amount of labeled scRNAseq cells to implement cell identification. SemiRNet, based on recurrent convolutional neural networks (RCNN), incorporates a shared network, a supervised network, and an unsupervised network. Furthermore, single&#x2010;cell ANnotation using Variational Inference (scANVI) [28], a semi&#x2010;supervised variant of scVI [14], maximizes the utility of existing cell state annotations. Cell BLAST, an autoencoder-based generative model, harnesses large-scale reference databases to learn nonlinear low-dimensional representations of cells, employing a sophisticated cell similarity metric&#x2014;normalized projection distance&#x2014;to map query cells to specific cell types and identify novel cell types.</p><h3 id="multi-omics-data-integration">Multi-omics Data Integration</h3><p>Recent studies have demonstrated the potential of deep learning models in addressing complex and multimodal biological challenges [29]. &#xA0;Among the algorithms proposed thus far, it is primarily deep learning-based models that provide the essential computational adaptability necessary for effectively modeling and incorporating nearly any form of omic data &#xA0;including &#xA0;genomics (studying DNA sequences and genetic variations), epigenomics (examining changes in gene activity unrelated to DNA sequence, such as DNA modifications and chromatin structure), transcriptomics (investigating RNA molecules and gene expression through RNA sequencing), and proteomics (analyzing all proteins produced by an organism, including structures, abundances, and modifications). Deep Learning architectures, including autoencoders (AE) and generative adversarial networks (GAN), have been often used in multi-omics integration problems in single cells. The key question in multi-omics integration revolves around how to effectively represent the diverse multi-omics data within a unified latent space.</p><p>One of the early methods developed using Variational Autoencoders (VAE) for the integration of multi-omics single-cell data is known as totalVI [30]. The totalVI model, which is VAE-based, offers a solution for effectively merging scRNA-seq and protein data. In this model, totalVI takes input matrices containing scRNA-seq and protein count data. Specifically, it treats gene expression data as sampled from a negative binomial distribution, while protein data are treated as sampled from a mixture model consisting of two negative binomial distributions. The model first learns shared latent space representations through its encoder, which are then utilized to reconstruct the original data, taking into account the differences between the two original data modalities. Lastly, the decoder component estimates the parameters of the underlying distributions for both data modalities using the shared latent representation.</p><p>On the other hand, Zuo et al. [31] introduced scMVAE as a multimodal variational autoencoder designed to integrate transcriptomic and chromatin accessibility data in the same individual cells. scMVAE employs two separate single-modal encoders and two single-modal decoders to effectively model both transcriptomic and chromatin data. It achieves this by combining three distinct joint-learning strategies with a probabilistic Gaussian Mixture Model.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://lh7-us.googleusercontent.com/k7QSkoQ1aPR921c2cqbKPYQ0HhguVnR2Xi8lrd9VCD38PFEifKNFU94Sb_IJR1DZl5zio9HKiePtlW9KrQD91cV_oiN4DLAzXt7I2UC_ETdfSPKKJ913D_U3_kof9NzBjEBlVVpUUs08YLTjt7fW3w" class="kg-image" alt="Deep learning for single-cell sequencing: a microscope to see the diversity of cells" loading="lazy" width="669" height="242"><figcaption><em><strong>Figure 5 .</strong> UMAP embedding for the latent space of the MULTIGRATE for CITE-seq dataset combines gene expression and cell surface protein data [32].</em></figcaption></figure><p>Recently, Lotfollahi et al. [32] introduced an unsupervised deep generative model known as MULTIGRATE for the integration of multi-omic datasets. MULTIGRATE employs a multi-modal variational autoencoder structure that shares some similarities with the scMVAE model. However, it offers added generality and the capability to integrate both paired and unpaired single-cell data. To enhance cell alignment, the loss function incorporates Maximum Mean Discrepancy (MMD), penalizing any misalignment between the point clouds associated with different assays. Incorporating transfer learning, MULTIGRATE can map new multi-omic query datasets into a reference atlas and also perform imputations for missing modalities.</p><h2 id="conclusion"><strong>Conclusion</strong></h2><p>The application of deep learning in single-cell sequencing functions as an advanced microscope, revealing intricate insights within individual cells and providing a profound understanding of cellular heterogeneity and complexity in biological systems. This cutting-edge technology empowers scientists to explore previously undiscovered aspects of cellular behavior. However, the challenge lies in choosing between traditional tools and the plethora of available deep-learning options. The landscape of tools is vast, and researchers must carefully consider factors such as data type, complexity, and the specific biological questions at hand. Navigating this decision-making process requires a thoughtful evaluation of the strengths and limitations of each tool in relation to research goals.</p><p>On the other hand, a critical need in the development of deep learning approaches for single-cell RNA sequencing (scRNA-seq) analysis is robust benchmarking. While many studies compare deep learning performance to standard methods, there is a lack of comprehensive comparisons across various deep learning models. Moreover, methods often claim superiority based on specific datasets and tissues (e.g., pancreas cells, immune cells), making it challenging to evaluate the necessity of specific terms or preprocessing steps. Addressing these challenges requires an understanding of when deep learning models fail and their limitations. Recognizing which types of deep learning approaches and model structures are beneficial in specific cases is crucial for developing new approaches and guiding the field.</p><p>In the realm of multi-omics single-cell integration, most deep learning methods aim to find a shared latent representation for all modalities. However, shared representation learning faces challenges such as heightened noise, sparsity, and the intricate task of balancing modalities. Inherent biases across institutions complicate generalization. Despite being less prevalent than single-modality approaches, integrating diverse modalities with unique cell populations is crucial. Objectives include predicting expression across modalities and identifying cells in similar states. Despite advancements, further efforts are essential for enhanced performance, particularly concerning unique or rare cell populations present in one technology but not the other.</p><hr><h2 id="author-bio">Author Bio</h2><p>Fatima Zahra El Hajji holds a master&apos;s degree in bioinformatics from the National School of Computer Science and Systems Analysis &#xA0;(ENSIAS), she subsequently worked as an AI intern at Piercing Star Technologies. Currently, she is a Ph.D. student at the University Mohammed VI Polytechnic (UM6P), working under the supervision of Dr. Rachid El Fatimy and &#xA0;Dr. Tariq Daouda. Her research focuses on the application of deep learning techniques in single-cell sequencing data.</p><h2 id="citation">Citation<br></h2><p>For attribution in academic contexts or books, please cite this work as</p><pre><code>Fatima Zahra El Hajji, &quot;Deep learning for single-cell sequencing: a microscope to see the diversity of cells&quot;, The Gradient, 2024.
</code></pre><p>BibTeX citation:</p><pre><code>@article{elhajji2023nar,
    author = {El Hajji, Fatima Zahra},
    title = {Deep learning for single-cell sequencing: a microscope to see the diversity of cells},
    journal = {The Gradient},
    year = {2024},
    howpublished = {\url{https://thegradient.pub/deep-learning-for-single-cell-sequencing-a-microscope-to-uncover-the-rich-diversity-of-individual-cells},
}</code></pre><h2 id="references">References</h2><ol><li><em><em><em>National Human Genome Research Institute (NHGRI) : A Brief Guide to Genomics ,</em><a href="https://www.genome.gov/about-genomics/fact-sheets/A-Brief-Guide-to-Genomics"><em> https://www.genome.gov/about-genomics/fact-sheets/A-Brief-Guide-to-Genomics</em></a></em></em></li><li><em><em><em>Method of the Year 2013. Nat Methods 11, 1 (2014).</em><a href="https://doi.org/10.1038/nmeth.2801"><em> https://doi.org/10.1038/nmeth.2801</em></a></em></em></li><li><em><em><em>Zappia, L., Theis, F.J. Over 1000 tools reveal trends in the single-cell RNA-seq analysis landscape. Genome Biol 22, 301 (2021).</em><a href="https://doi.org/10.1186/s13059-021-02519-4"><em> https://doi.org/10.1186/s13059-021-02519-4</em></a></em></em></li><li><em><em><em>Collins FS, Fink L. The Human Genome Project. Alcohol Health Res World. 1995;19(3):190-195. PMID: 31798046; PMCID: PMC6875757.</em></em></em></li><li><em><em><em>Tang F, Barbacioru C, Wang Y, et al. mRNA-Seq whole-transcriptome analysis of a single cell. Nat Methods. 2009; 6: 377-382.</em></em></em></li><li><em><em><em>Eisenstein, M. The secret life of cells. Nat Methods 17, 7&#x2013;10 (2020). https://doi.org/10.1038/s41592-019-0698-y</em></em></em></li><li><em><em><em>Method of the Year 2019: Single-cell multimodal omics. Nat Methods 17, 1 (2020). https://doi.org/10.1038/s41592-019-0703-5</em></em></em></li><li><em><em><em>Zhao, Jing et al. &#x201C;Multi-view learning overview: Recent progress and new challenges.&#x201D; Inf. Fusion 38 (2017): 43-54.</em></em></em></li><li><em><em><em>Zhu, J., Shang, L. &amp; Zhou, X. SRTsim: spatial pattern preserving simulations for spatially resolved transcriptomics. Genome Biol 24, 39 (2023).</em></em></em></li><li><em><em><em>Butler, A., Hoffman, P., Smibert, P., Papalexi, E., &amp; Satija, R. (2018). Integrating single-cell transcriptomic data across different conditions, technologies, and species. Nature biotechnology, 36(5), 411-420</em></em></em></li><li>Wagner, F., Yan, Y., &amp; Yanai, I. (2018). K-nearest neighbor smoothing for high-throughput single-cell RNA-Seq data. bioRxiv, 217737. Cold Spring Harbor Laboratory. https://doi.org/10.1101/217737</li><li>Huang, M., Wang, J., Torre, E. et al. SAVER: gene expression recovery for single-cell RNA sequencing. Nat Methods 15, 539&#x2013;542 (2018). https://doi.org/10.1038/s41592-018-0033-z</li><li>Eraslan G, Simon LM, Mircea M, Mueller NS, Theis FJ. Single-cell RNA-seq denoising using a deep count autoencoder. Nat Commun. 2019 Jan 23;10(1):390. doi: 10.1038/s41467-018-07931-2. PMID: 30674886; PMCID: PMC6344535.</li><li>Lopez, R., Regier, J., Cole, M. B., Jordan, M. I.,&amp; Yosef, N. (2018). Deep generative modeling for single-cell transcriptomics. Nature methods, 15(12), 1053-1058.</li><li>Y. Deng, F. Bao, Q. Dai, L.F. Wu, S.J. Altschuler Scalable analysis of cell-type composition from single-cell transcriptomics using deep recurrent learning</li><li>Ritchie ME, Phipson B, Wu D, Hu Y, Law CW, Shi W, Smyth GK. limma powers differential expression analyses for RNA-sequencing and microarray studies. Nucleic Acids Res. 2015 Apr 20;43(7):e47. doi: 10.1093/nar/gkv007. Epub 2015 Jan 20. PMID: 25605792; PMCID: PMC4402510.</li><li>Johnson W.E. , Li C., Rabinovic A. Adjusting batch effects in microarray expression data using empirical bayes methods. Biostatistics. 2007; 8:118&#x2013;127.</li><li>Haghverdi, L., Lun, A., Morgan, M. et al. Batch effects in single-cell RNA-sequencing data are corrected by matching mutual nearest neighbors. Nat Biotechnol 36, 421&#x2013;427 (2018). https://doi.org/10.1038/nbt.4091</li><li><em><em><em>Lotfollahi, M., Wolf, F. A., &amp; Theis, F. J. (2019). scGen predicts single-cell perturbation responses. Nature methods, 16(8), 715-721.</em></em></em></li><li><em><em><em>Zou, B., Zhang, T., Zhou, R., Jiang, X., Yang, H., Jin, X., &amp; Bai, Y. (2021). deepMNN: deep learning-based single-cell RNA sequencing data batch correction using mutual nearest neighbors. Frontiers in Genetics, 1441.</em></em></em></li><li><em><em><em>Yu, X., Xu, X., Zhang, J. et al. Batch alignment of single-cell transcriptomics data using deep metric learning. Nat Commun 14, 960 (2023). https://doi.org/10.1038/s41467-023-36635-5</em></em></em></li><li><em><em><em>Z.A. Clarke, T.S. Andrews, J. Atif, D. Pouyabahar, B.T. Innes, S.A. MacParland, et al. Tutorial: guidelines for annotating single-cell transcriptomic maps using automated and manual methods Nat Protoc, 16 (2021), pp. 2749-2764</em></em></em></li><li><em><em><em>Wolf, F., Angerer, P. &amp; Theis, F. SCANPY: large-scale single-cell gene expression data analysis. Genome Biol 19, 15 (2018). https://doi.org/10.1186/s13059-017-1382-0</em></em></em></li><li><em><em><em>Mohit Goyal, Guillermo Serrano, Josepmaria Argemi, Ilan Shomorony, Mikel Hernaez, Idoia Ochoa, JIND: joint integration and discrimination for automated single-cell annotation, Bioinformatics, Volume 38, Issue 9, March 2022, Pages 2488&#x2013;2495, https://doi.org/10.1093/bioinformatics/btac140</em></em></em></li><li><em><em><em>H. Li, C.R. Brouwer, W. Luo A universal deep neural network for in-depth cleaning of single-cell RNA-seq data Nat Commun, 13 (2022), p. 1901</em></em></em></li><li><em><em><em>Song T, Dai H, Wang S, Wang G, Zhang X, Zhang Y and Jiao L (2022) TransCluster: A Cell-Type Identification Method for single-cell RNA-Seq data using deep learning based on transformer. Front. Genet. 13:1038919. doi: 10.3389/fgene.2022.1038919</em></em></em></li><li><em><em><em>Dong X, Chowdhury S, Victor U, Li X, Qian L. Semi-Supervised Deep Learning for Cell Type Identification From Single-Cell Transcriptomic Data. IEEE/ACM Trans Comput Biol Bioinform. 2023 Mar-Apr;20(2):1492-1505. doi: 10.1109/TCBB.2022.3173587. Epub 2023 Apr 3. PMID: 35536811.</em></em></em></li><li><em><em><em>Xu, C., Lopez, R., Mehlman, E., Regier, J., Jordan, M. I., &amp; Yosef, N. (2021). Probabilistic harmonization and annotation of single&#x2010;cell transcriptomics data with deep generative models. Molecular Systems Biology, 17(1), e9620. https://doi.org/10.15252/msb.20209620</em></em></em></li><li><em><em><em>Tasbiraha Athaya, Rony Chowdhury Ripan, Xiaoman Li, Haiyan Hu, Multimodal deep learning approaches for single-cell multi-omics data integration, Briefings in Bioinformatics, Volume 24, Issue 5, September 2023, bbad313, </em><a href="https://doi.org/10.1093/bib/bbad313"><em>https://doi.org/10.1093/bib/bbad313</em></a></em></em></li><li><em>Gayoso, A., Lopez, R., Steier, Z., Regier, J., Streets, A., &amp; Yosef, N. (2019). A Joint Model of RNA Expression and Surface Protein Abundance in Single Cells. bioRxiv, 791947. </em><a href="https://www.biorxiv.org/content/early/2019/10/07/791947.abstract"><em>https://www.biorxiv.org/content/early/2019/10/07/791947.abstract</em></a></li><li><em>Chunman Zuo, Luonan Chen. Deep-joint-learning analysis model of single cell transcriptome and open chromatin accessibility data. Briefings in Bioinformatics. 2020.</em></li><li><em>Lotfollahi, M., Litinetskaya, A., &amp; Theis, F. J. (2022). Multigrate: single-cell multi-omic data integration.bioRxiv.</em><a href="https://www.biorxiv.org/content/early/2022/03/17/2022.03.16.484643"><em>https://www.biorxiv.org/content/early/2022/03/17/2022.03.16.484643</em></a></li></ol>]]></content:encoded></item><item><title><![CDATA[Salmon in the Loop]]></title><description><![CDATA[On fish counting  a complex sociotechnical problem in a field that is going through the process of digital transformation.]]></description><link>https://thegradient.pub/salmon-in-the-loop/</link><guid isPermaLink="false">64dfbfba93571d5c8c15419a</guid><category><![CDATA[Machine Learning]]></category><category><![CDATA[Overviews]]></category><dc:creator><![CDATA[Kevin McCraney]]></dc:creator><pubDate>Sat, 16 Dec 2023 17:00:36 GMT</pubDate><media:content url="https://thegradient.pub/content/images/2023/08/DALL-E-2023-08-21-18.52.00---frothing-waves-in-the-middle-of-the-spillway-of-a-hydroelectric-dam--shown-by-a-bridge-across-a-river-with-many-arches--with-a-fish-jumping-downstream.png" medium="image"/><content:encoded><![CDATA[<img src="https://thegradient.pub/content/images/2023/08/DALL-E-2023-08-21-18.52.00---frothing-waves-in-the-middle-of-the-spillway-of-a-hydroelectric-dam--shown-by-a-bridge-across-a-river-with-many-arches--with-a-fish-jumping-downstream.png" alt="Salmon in the Loop"><p>One of the most fascinating problems that a computer scientist may be lucky enough to encounter is a complex sociotechnical problem in a field going through the process of digital transformation. For me, that was fish counting. Recently, I worked as a consultant in a subdomain of environmental science focused on counting fish that pass through large hydroelectric dams. Through this overarching project, I learned about ways to coordinate and manage human-in-the-loop dataset production, as well as the complexities and vagaries of how to think about and share progress with stakeholders.</p><h2 id="background">Background</h2><p>Let&#x2019;s set the stage. Large hydroelectric dams are subject to Environmental Protection Act regulations through the Federal Energy Regulatory Commission (FERC). FERC is an independent agency of the United States government that regulates the transmission and wholesale sale of electricity across the United States. The commission has jurisdiction over a wide range of electric power activities and is responsible for issuing licenses and permits for the construction and operation of hydroelectric facilities, including dams. These licenses and permits ensure that hydroelectric facilities are safe and reliable, and that they do not have a negative impact on the environment or other stakeholders. In order to obtain a license or permit from FERC, hydroelectric dam operators must submit detailed plans and studies demonstrating that their facility meets regulations. This process typically involves extensive review and consultation with other agencies and stakeholders. If a hydroelectric facility is found to be in violation of any set standards, FERC is responsible for enforcing compliance with all applicable regulations via sanctions, fines, or lease termination--resulting in a loss of the right to generate power.</p><p>Hydroelectric dams are essentially giant batteries. They generate power by building up a large reservoir of water on one side and directing that water through turbines in the body of the dam. Typically, a hydroelectric dam requires lots of space to store water on one side of it, which means they tend to be located away from population centers. The conversion process from potential to kinetic energy generates large amounts of electricity, and the amount of pressure and force generated is disruptive to anything that lives in or moves through the waterways&#x2014;especially fish.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://lh3.googleusercontent.com/3s1Nf_YKJJ6RYku2imhRa7EpXZkbB5fb6R2_EG5shYdVdyRXjVvXZaDq4soTbdbh0TJ-7ch4I4DJ59Fs94dJaJCwnJQCYdLSl9ehw_-92xerbFob67m2N8A8a5NSrxROJS2hN4HNNp0DhYd69-AEGYk" class="kg-image" alt="Salmon in the Loop" loading="lazy" width="429" height="248"><figcaption><em>Simple diagram illustrating how hydroelectric power is generated (Tennessee Valley Authority)</em></figcaption></figure><p>It is also worth noting that the waterways were likely disrupted substantially when the dam was built, leading to behavioral or population-level changes in the fish species of the area. This is of great concern to the Pacific Northwest in particular, as hydropower is the predominant power generation means for the region (Bonneville Power Administration). Fish populations are constantly moving upstream and downstream and hydropower dams can act as barriers that block their passage, leading to reduced spawning. In light of the risks to fish, hydropower dams are subject to constraints on the amount of power they can generate and must show that they are not killing fish in large numbers or otherwise disrupting the rhythms of their lives, especially because the native salmonid species of the region are already threatened or endangered (Salmon Status). </p><p>To demonstrate compliance with FERC regulations, large hydroelectric dams are required to routinely produce data which shows that their operational activities do not interfere with endangered fish populations in aggregate. Typically, this is done by performing fish passage studies. A fish passage study can be conducted many different ways, but boils down to one primary dataset upon which everything is based: a fish count. Fish are counted as they pass through the hydroelectric dam, using structures like fish ladders to make their way from the reservoir side to the stream side.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://lh6.googleusercontent.com/c3W3UHWOkSjkN4Yq2vbtCvEklUA6l5rPgIw53Y15-kt1rqeldlSByI45ruUdsfhy60MTEGWJJ7fPu0YuLMyXLDBIUFkPEYlGIzAipRu7qYGH0OFkrpQL3d3YTNlcqSBiW6y7VHpDCkYpqCXgXstn4Gs" class="kg-image" alt="Salmon in the Loop" loading="lazy" width="594" height="390"><figcaption><em>A fish ladder at John Day Dam, how fish often ascend and pass through a dam (Delgado)</em></figcaption></figure><p>Fish counts can be conducted visually&#x2014;-a person trained in fish identification watches the fish pass, incrementing the count as they move upstream. As a fish is counted, observers impart additional classifications outside of species of fish, such as whether there was some kind of obvious illness or injury, if the fish is hatchery-origin or wild, and so on. These differences between fish are subtle and require close monitoring and verification, since the attribute in question (a clipped adipose fin, a scratched midsection) may only be visible briefly when the fish swims by. As such, fish counting is a specialized job that requires expertise in identifying and classifying different species of fish, as well as knowledge of their life stages and other characteristics. The job is physically demanding, as it typically involves working in remote locations away from city centers, and it can be challenging to perform accurately under the difficult environmental conditions found at hydroelectric dams&#x2013;poor lighting, unregulated temperatures, and other circumstances inhospitable to humans.</p><p>These modes of data collection are great, but there are varying degrees of error that could be imparted through their recording. For example, some visual fish counts are documented with pen and paper, leading to incorrect counts through transcription error; or there can be disputes about the classification of a particular species. Different dam operators collect fish counts with varying degrees of granularity (some collect hourly, some daily, some monthly) and seasonality (some collect only during certain migration patterns called &#x201C;runs&#x201D;). After collection and validation, organizations correlate this data with operational information produced by the dam in an attempt to see if any activities of the dam have an adverse or beneficial effect on fish populations. Capturing these data piecemeal with different governing standards and levels of detail causes organizations to look for new efficiencies enabled by technology.</p><h2 id="enter-computer-vision">Enter Computer Vision</h2><p>Some organizations are exploring the use of computer vision and machine learning to significantly automate fish counting. Since dam operators subject to FERC are required to collect fish passage data anyway, and the data were previously produced or encoded in ways that were challenging to work with, an interesting &#x201C;human-in-the-loop&#x201D; machine learning system arises. A human-in-the-loop system combines the judgment and expertise of subject-matter expert humans (fish biologists) with the consistency and reliability of machine learning algorithms, which can help to reduce sources of error and bias in the output dataset used in the machine learning system. For the specific problem of fish counting, this could help to ensure that the system&apos;s decisions are informed by the latest scientific understanding of fish taxonomy and conservation goals, and could provide a more balanced and comprehensive approach to species or morphological classification. An algorithmic system could reduce the need for manual data collection and analysis by automating the process of identifying and classifying species, and could provide more timely and accurate information about species&apos; health.</p><p>Building a computer vision system for a highly-regulated industry, such as hydropower utilities, can be a challenging task due to the need for high accuracy and strict compliance with regulatory standards. The process of building such a system would typically involve several steps:</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://lh5.googleusercontent.com/oJZgbwf73RoE2RzR1T8GJ9uDIQf1OJhCqtcXH70eb66bhhAskb8ReMr4OBESMkWahCBSNX4bvIFUdjjFZk0gdzdzyIeNrbkAy95QuH925Ek8UPCtFe82FfZpil_nCzKC2b01jSxDgaayPDwovFJffPE" class="kg-image" alt="Salmon in the Loop" loading="lazy" width="624" height="141"><figcaption><em>Representation of example process flow for productionizing a ML system</em></figcaption></figure><p>1. <strong>Define the problem space</strong>: Before starting to build the system, it is important to clearly define the problem that the system is intended to solve and the goals that it needs to achieve. This initial negotiation process is largely without any defining technical constraints, and is based around the job to that needs to be done by the system: identifying specific tasks that the system needs to perform, such as identification of the species or life stage of a fish. This may be especially challenging in a regulated industry like hydropower, as clients are subject to strict laws and regulations that require them to ensure that any tools or technologies they use are reliable and safe. They may be skeptical of a new machine learning system and may require assurances that it has been thoroughly tested and will not pose any risks to the environment or to through data integrity, algorithmic transparency, and accountability.</p><p>Once the problem space is defined, more technical decisions can be made about how to implement the solution. For example, if the goal is to estimate population density during high fish passage using behavioral patterns such as schooling, it may make sense to capture and tag live video, to see the ways in which fish move in real time. Alternatively, if the goal is to identify illness or injury in a situation where there are few fish passing, it may make sense to capture still images and tag subsections of them to train a classifier. In a more developed hypothetical example, perhaps dam operators know that the fish ladder only allows fish to pass through it, all other species or natural debris are filtered out, and they want a &#x201C;best guess&#x201D; about rare species of fish that pass upstream. It may be sufficient in this case to implement generic video-based object detection to identify that a fish is moving through a scene, take a picture of it at a certain point, and provide that picture to a human to tag with the species. Once tagged, these data can be used to train a classifier which categorizes fish as being the rare species or not.</p><p>2. <strong>Establish performance goals</strong>: The definition of the problem space and the initial suggested process flow should be shared with all stakeholders as an input to the performance goals. This helps ensure all interested parties understand the problem at a high level, and what is possible for a given implementation. Practically, most hydropower utilities are interested in automated fish count solutions that meet an accuracy threshold of 95% as compared to a regular human visual count, but expectations around whether these metrics are achievable and at what part of the production cycle will be a highly negotiated series of points. Establishing these goals is a true sociotechnical problem, as it cannot be done without taking into account both the real-world constraints that limit the data and the system. These constraining factors will be discussed later in the Obstacles section of the paper.</p><p>3. <strong>Collect and label training data</strong>: In order to train a machine learning model to perform the tasks required by the system, it is first necessary to produce a training dataset. Practically, this involves collecting a large number of fish images. The images are annotated with the appropriate species classification labels by a person with expertise in fish classification. The annotated images are then used to train a machine learning model. Through training, the algorithm learns the features characteristic of each subclass of fish and identifies those features to classify fish in new, unseen images. Because the end goal of this system is to minimize the counts that humans have to do, images with a low &#x201C;confidence score&#x201D; (a metric commonly produced by object-detection models) may be flagged for identification and tagging by human reviewers. The more seamless an integration with a production fish counting operation, the better.</p><p>4. <strong>Select a model</strong>: Once the training data has been collected, the next step is to select a suitable machine learning model and train it on the data. This could involve using a supervised learning approach, where the model is trained to recognize the different categories of fish after being shown examples of labeled data. At the time of this writing, deep learning systems based on pretrained models like <em>ImageNet </em>are popular choices. Once trained, the model should be validated against tagged data that it has not seen before and fine-tuned by adjusting the model parameters or refining the training dataset and retraining.</p><p>5. <strong>Monitor system performance</strong>: Once the model has been trained and refined, it can be implemented as part of a computer vision system for regular use. The system&apos;s performance should be monitored regularly to ensure that it is meeting the required accuracy targets and to ensure that model drift does not occur, perhaps from changes in environmental conditions, such as water clarity; or morphological changes alluded to in a later section</p><p>It is at this point that the loop of tasks begins anew; to eke out more performance from the system, it is likely that more refined and nuanced negotiation about what to expect from the system is necessary, followed by additional training data, model selection, and parameter tuning/monitoring. The common assumption is that an automated or semiautomatic system like this is &#x201C;set it and forget it&#x201D; but the process of curating and collating datasets or tuning hyper parameters is quite engaged and intentional.</p><h2 id="obstacles">Obstacles</h2><p>In order for the computer vision algorithm to accurately detect and count fish in images or video frames, it must be trained on a large and diverse dataset that includes examples of different fish species and morphologies. However, this approach is not without challenges, as specified in the diagram below and with bolded phrases in subsequent paragraphs:</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://lh5.googleusercontent.com/Kkm5k3JjZUhDzfj664FwFBJ6QzrE59i6xcXQLKwdddEAzCr1jtax5fvoiVKfgfUJTchfzun8tbY6nm68PViPQ_fRDkG6I8gnvtxED_4GIXBGoscHiyfSDsC1oXLr5aHu-bb0YY0Faz_KycnGSYsRmjQ" class="kg-image" alt="Salmon in the Loop" loading="lazy" width="624" height="257"><figcaption><em>Recapitulation of diagram above; terminal states specified on the diagram are obstacles to successful system building</em></figcaption></figure><p><strong>Dependence on expert knowledge</strong> is a concern worth discussing. If the system relies on expert-tagged data to train and evaluate its algorithms, the system may be vulnerable to errors and biases in the expert&apos;s knowledge and judgments, as any human-in-the-loop system would be. For example, if the experts are not familiar with certain species or morphologies, they may not be able to accurately tag these fish, which could lead to incorrect classifications by the system. Should an invasive species enter the waterway, it may become overrepresented within the dataset and affect the counts of the species that require conservation action. An excellent practical example of this is American shad, of which hundreds of thousands can pass during a migratory period, obscuring the Chinook salmon that are also passing during the same time. Manual counting methods rely solely on the judgment and observation of individual humans, which can be subject to a variety of sources of error and bias. Further, if the experts have a particular interest in certain species or morphologies, they may be more likely to tag these fish, which could result in over- or under-representation within the dataset. This can lead to life-threatening outcomes if the algorithmic system is used to make important decisions that have conservation implications.</p><p>Environmental conditions at hydroelectric dams present <strong>challenges for data collection </strong>as well. &#xA0;Inadequate illumination and poor image quality can make it difficult for both humans and machine learning algorithms to accurately classify fish. Similarly, changing conditions, like a reduction in water clarity following a seasonal snowmelt can obscure fish in imagery. Migratory fish can be difficult to identify and classify on their own terms, due to the wide range of species and subspecies that exist, and the way their bodies change as they age. These fish are often difficult to study and monitor due to their migratory habits and the challenging environments in which they live. Further, there are often inconsistent data taxonomies produced across organizations, leading to different classifications depending on the parent organization undertaking the data tagging process. If humans cannot create accurate classifications to populate the initial dataset, the machine learning system will not be able to accurately produce predictions when used in production.<br></p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://lh3.googleusercontent.com/OTGP7xbmOmzuUKUPzZEPiVeN1COG_px_BeUPb_3dyPmRsjSd0_HBwVVgPK5wamrookVqohcw2CQvza6rbk3mJIlQ-35F5pJLsMH7JeW1Gmv24VJ3nOvM-7ojmbIcAkxYqwtO0-z4-BvhVQj3lsbbYYs" class="kg-image" alt="Salmon in the Loop" loading="lazy" width="603" height="172"><figcaption><em>Example image of rainbow trout from an onsite edge device; challenging to tell from the lighting but those could be natural spots, injury, or parasitic infection</em></figcaption></figure><p>One of the key challenges of using a machine learning classifier on unaudited data is the risk of <strong>model drift</strong>, in which the model&apos;s performance degrades over time as the underlying data distribution changes. This may be of particular concern in a highly regulated environment, where even small changes in the model&apos;s performance could have significant consequences. The datasets produced through the effort of tagging fish images are fascinating because they are so intrinsically place-based, situated, and not easily replicable. Fish passage studies often involve monitoring a relatively small number of fish, which can make it difficult to accurately assess the overall profile of fish populations in the wider area. The number and types of fish that pass through a dam&apos;s fish ladders or other fish passage structures can vary greatly depending on the time of year or the &quot;run&quot; of fish passing through the waterways. This can make it difficult to compare data from different studies, or to draw conclusions about the long-term impact of the dam on fish populations. If the system is trained on a dataset of fish that has been tagged by subject-matter experts during one season, the dataset may not be comprehensive or representative of the full range of fish species and morphologies that exist in the wild across the full year. This could lead to under- or over-estimations of number and types of fish present in a given area. In this way, the specter of model drift is actually a problem composed of both challenging data production constraints and dependence on expert knowledge. </p><p>Finally, there are background labor issues to be dealt with as part of this problem space coming from intense<strong> organizational pressure</strong>. Fish counting is a cost center that hydroelectric dam operators would like to eliminate or reduce as much as possible. A technical solution that can accurately count fish is therefore very appealing. However, this raises concerns about ghost work, where human labor is used to train and validate the model, but is not acknowledged or compensated. Replacing human workers with a computer vision solution may significantly impact the displaced workers through financial hardship or the obsoletion of their job skills and expertise. If human expertise in the identification of fish is lost, this could lead to suboptimal decisions about species conservation, and could ultimately undermine the effectiveness of the system. This becomes more dangerous for conservation purposes if the technology is implemented as a cost-reduction measure: it could be the case that&#x2014;when the model drifts&#x2014;there are no taggers to set it back on track.</p><p>Couple all of these points with the longitudinal decline of wild fish populations globally, and you have a challenging set of conditions to attempt to generalize from. </p><p>If the available training data is limited or does not accurately reflect the diversity of fish species and morphologies that pass through the dam&apos;s fish passage structures, the accuracy of the algorithm may be reduced. Additionally, there are concerns about data leakage, where the model may be able to infer sensitive information about the fish from the images, such as how they are routed through the dam. Thinking about studies that happen in fisheries as per Hwang (2022), the populations analyzed are so small and the outcomes so intentionally so narrowly-scoped, it is almost the case that an organization would have to at the very least train a one-off model for each project or validate the output of each ML classifier against some additional source, which is lately outside of the interest and capabilities of organizations who hope to reduce labor outlays as part of the implementation of a system like this.</p><h2 id="concluding-thoughts">Concluding Thoughts</h2><p>The sociotechnical problem of fish counting is a niche problem with wide applications. If properly implemented, a machine learning system based around fish counts has the potential to be applied in many different places, such as meeting environmental regulation or aquaculture. The rapid digital transformation of environmental science has led to the development of novel datasets with interesting challenges, and a new cohort of professionals with the data literacy and technical abilities to work on problems like this. However, building a dataset of anadromous and catadromous fish that are protected under the ESA is a complex and challenging task, due to the limited availability of data, the complexity of fish taxonomy, the involvement of multiple stakeholders, and the dynamic environment in which these species live. </p><p>Moreover, organizations subject to regulation may be unsure of how to validate the accuracy of a machine learning model, and may be more interested in fish counts than in fish images (or vice-versa). Bringing new technologies to bear on an organization or on a dataset that was not as robustly cataloged means there will be new things to be discovered or measured through the application of the technology. Since implementation of a computer vision system like this is done to meet compliance with FERC regulations, it means bringing multiple different stakeholders&#x2013;including federal agencies, state and local governments, conservation organizations, and members of the public&#x2013;into dialogue with one another when changes are required. By conducting these studies and regularly reporting the results to FERC, a hydroelectric dam operator could demonstrate that they are taking steps to minimize the impact of the dam on fish populations, and that the dam is not having a negative impact on the overall health of the local fish population, but it also means cross-checking with the community in which they are situated.</p><h3 id="author-bio">Author Bio</h3><p><a href="https://kevinmccraney.com/">Kevin McCraney</a> is a data engineer, educator, and consultant. He works with public sector &amp; large-scale institutions building data processing infrastructure &amp; improving data literacy. Kevin has several years of experience teaching &amp; mentoring early career professionals as they transition to technology from non-STEM disciplines. Working predominantly with institutions in the Pacific Northwest, he enjoys professional opportunities where he can combine a humanistic worldview and technical acumen to solve complex sociotechnical problems.</p><h3 id="citation">Citation</h3><p>For attribution of this in academic contexts or books, please cite this work as:</p><blockquote><em>Kevin McCraney, &quot;</em>Salmon in the Loop<em>&quot;, The Gradient, 2023.</em></blockquote><p>BibTeX citation</p><blockquote>@article{k2023omccraney,<br>author = {McCraney, Kevin},<br>title = {Salmon in the Loop},<br>journal = {The Gradient},<br>year = {2023},<br>howpublished = {\url{<a href="thegradient.pub/salmon-in-the-loop/">https://thegradient.pub/salmon-in-the-loop</a>}},<br>}</blockquote><h3 id="works-cited">Works Cited<br></h3><p>[1]Bonneville Power Administration. (n.d.). <em>Hydropower impact</em>. Hydropower Impact. Retrieved January 14, 2023, from https://www.bpa.gov/energy-and-services/power/hydropower-impact </p><p>[2]Delgado, K. (2021, July 19). <em>That sounds fishy: Fish ladders at high-head dams impractical, largely unneeded</em>. www.army.mil. Retrieved January 3, 2023, from https://www.army.mil/article/248558/that_sounds_fishy_fish_ladders_at_high_head_dams_impractical_largely_unneeded </p><p>[3]Hwang, I. (2022, May 31). <em>Salmon hatchery data is harder to handle than you think</em>. ProPublica. Retrieved December 10, 2023, from https://www.propublica.org/article/salmon-hatcheries-pnw-fish-data</p><p>[4]<em>Salmon status</em>. State of Salmon. (2021, January 11). Retrieved December 29, 2022, from https://stateofsalmon.wa.gov/executive-summary/salmon-status/ </p><p>[5]<em>How hydroelectric power works</em>. Tennessee Valley Authority. (2021, January 11). Retrieved December 29, 2022, from https://www.tva.com/energy/our-power-system/hydroelectric/how-hydroelectric-power-works</p>]]></content:encoded></item><item><title><![CDATA[Neural algorithmic reasoning]]></title><description><![CDATA[<p>In this article, we will talk about <em>classical computation</em>: the kind of computation typically found in an undergraduate Computer Science course on Algorithms and Data Structures [1]. Think shortest path-finding, sorting, clever ways to break problems down into simpler problems, incredible ways to organise data for efficient retrieval and updates.</p>]]></description><link>https://thegradient.pub/neural-algorithmic-reasoning/</link><guid isPermaLink="false">64fd870693571d5c8c15456a</guid><category><![CDATA[Machine Learning]]></category><category><![CDATA[Overviews]]></category><dc:creator><![CDATA[Petar Velikovi]]></dc:creator><pubDate>Sat, 14 Oct 2023 15:30:15 GMT</pubDate><media:content url="https://thegradient.pub/content/images/2023/09/profile.png" medium="image"/><content:encoded><![CDATA[<img src="https://thegradient.pub/content/images/2023/09/profile.png" alt="Neural algorithmic reasoning"><p>In this article, we will talk about <em>classical computation</em>: the kind of computation typically found in an undergraduate Computer Science course on Algorithms and Data Structures [1]. Think shortest path-finding, sorting, clever ways to break problems down into simpler problems, incredible ways to organise data for efficient retrieval and updates. Of course, given <em>The Gradient</em>&#x2019;s focus on Artificial Intelligence, we will not stop there; we will also investigate how to <em>capture</em> such computation with deep neural networks.</p><h2 id="why-capture-classical-computation"><em>Why</em> capture classical computation?</h2><p>Maybe it&#x2019;s worth starting by clarifying where my vested interest in classical computation comes from. Competitive programming&#x2014;the art of solving problems by rapidly writing programs that need to terminate in a given amount of time, and within certain memory constraints&#x2014;was a highly popular activity in my secondary school. For me, it was truly the gateway into Computer Science, and I trust the story is similar for many machine learning practitioners and researchers today. I have been able to win several medals at international programming competitions, such as the Northwestern Europe Regionals of the ACM-ICPC, the top-tier Computer Science competition for university students. Hopefully, my successes in competitive programming also give me some credentials to write about this topic.</p><p>While this should make clear why <em>I</em> care about classical computation, why should <em>we all</em> care? To arrive at this answer, let us ponder some of the key properties that classical algorithms have:</p><ul><li>They are <strong>provably correct</strong>, and we can often have strong guarantees about the <em>resources</em> (time or memory) required for the computation to terminate.</li><li>They offer <strong>strong generalisation</strong>: while algorithms are often devised by observing several small-scale example inputs, once implemented, they will work without fault on inputs that are significantly larger, or distributionally different than such examples.</li><li>By design, they are <strong>interpretable </strong>and <strong>compositional</strong>: their (pseudo)code representation makes it much easier to reason about what the computation is actually doing, and one can easily recompose various computations together through subroutines to achieve different capabilities.</li></ul><p>Looking at all of these properties taken together, they seem to be exactly the issues that plague modern deep neural networks the most: you can rarely guarantee their accuracy, they often collapse on out-of-distribution inputs, and they are very notorious as black boxes, with compounding errors that can hinder compositionality. </p><p>However, it is <em>exactly</em> those skills that are important for making AI <em>instructive</em> and <em>useful</em> to humans! For example, to have an AI system that reliably and instructively teaches a concept to a human, the quality of its output should not depend on minor details of the input, and it should be able to generalise that concept to novel situations. Arguably, these skills are also a missing key step on the road to building generally-intelligent agents. Therefore, if we are able to make any strides towards capturing traits of classical computation in deep neural networks, this is likely to be a very fruitful pursuit.</p><h2 id="first-impressions-algorithmic-alignment">First impressions: Algorithmic alignment</h2><p>My journey into the field started with an interesting, but seemingly inconsequential question: </p><p><strong>Can neural networks </strong><em><strong>learn</strong></em><strong> </strong><em><strong>to execute</strong></em><strong> classical algorithms?</strong></p><p>This can be seen as a good way to <em>benchmark</em> to what extent are certain neural networks capable of behaving algorithmically; arguably, if a system can produce the outputs of a certain computation, it has <em>&#x201C;captured&#x201D; </em>it. Further, learning to execute provides a uniquely well-built environment for evaluating machine learning models:</p><ul><li>An <em>infinite </em>data source&#x2014;as we can generate arbitrary amounts of inputs;</li><li>They require <em>complex</em> data manipulation&#x2014;making it a challenging task for deep learning;</li><li>They have a <em>clearly specified</em> target function&#x2014;simplifying interpretability analyses.</li></ul><p>When we started studying this area in 2019, we really did not think more of it than a very neat benchmark&#x2014;but it was certainly becoming a very lively area. Concurrently with our efforts, a team from MIT tried tackling a more ambitious, but strongly related problem:</p><p><strong>What makes a neural network </strong><em><strong>better </strong></em><strong>(or </strong><em><strong>worse) </strong></em><strong>at fitting certain (algorithmic) tasks?</strong></p><p>The landmark paper, <em>&#x201C;What Can Neural Networks Reason About?&#x201D; </em>[2] established a mathematical foundation for why an architecture might be <em>better</em> for a task (in terms of <em>sample complexity</em>: the number of training examples needed to reduce validation loss below epsilon). The authors&#x2019; main theorem states that <em>better <strong>algorithmic alignment</strong> leads to better <strong>generalisation</strong></em>. Rigorously defining algorithmic alignment is out of scope of this text, but it can be very intuitively visualised:<br></p><figure class="kg-card kg-image-card"><img src="https://lh4.googleusercontent.com/0IrqtmkK2xEoLmDRd9d4OVB5Ut3pOmC8vuo1r6ap9h_jfKUOEXPwpIyn0tsYuBS4tYtZHlSHYG0_6BBldc0WnFHxpVjIFDL7ndhsCBQVbzhY3IZLIy2vieBuuTxG73ufa9mkFFTgYyoco8KxH6KmoItIrupST-UxynpRlGzXxaWHxng9n3EFsRsvhpqBIK6lDh9Htfz4bNRiJL0pIutg19FU-J1OEwzJBLtG" class="kg-image" alt="Neural algorithmic reasoning" loading="lazy" width="624" height="311"></figure><p><br></p><p>Here, we can see a visual decomposition of how a <em>graph neural network </em>(<strong>GNN</strong>) [3] aligns with the classical <em>Bellman-Ford </em>[4]<em> </em>algorithm for shortest path-finding. Specifically, Bellman-Ford maintains its current estimate of how far away each node is from the source node: a <em>distance variable</em> (<em>du</em>) for each node <em>u</em> in a graph. At every step, for every neighbour <em>v</em> of node <em>u</em>, an update to <em>du </em>is proposed: a combination of (optimally) reaching <em>v</em>, and then traversing the edge connecting <em>v</em> and <em>u</em> (<em>du</em> + <em>wvu</em>). The distance variable is then updated as the optimal out of all the proposals. Operations of a graph neural network can naturally decompose to follow the data flow of Bellman-Ford:</p><ul><li>The distance variables correspond to the node features maintained by the GNN;</li><li>Adding the edge distance to a distance variable corresponds to computing the GNN&#x2019;s message function;</li><li>Choosing the optimal neighbour based on this measure corresponds to the GNN&#x2019;s permutation-invariant aggregation function, <strong>&#x2A01;</strong>.</li></ul><p>Generally, it can be proven that, the more closely we can <em>decompose</em> our neural network to follow the algorithm&#x2019;s <em>structure</em>, the more favourable <em>sample complexity</em> we can expect when learning to execute such algorithms. Bellman-Ford is a typical instance of a <em>dynamic programming </em>(<strong>DP</strong>) algorithm [5], a general-purpose problem-solving strategy that breaks the problem down into subproblems, and then recombines their solutions to find the final solution. </p><p>The MIT team made the important observation that GNNs appear to algorithmically align with DP, and since DP can itself be used to express many useful forms of classical computation, GNNs should be a very potent general-purpose model for learning to execute. This was validated by several carefully constructed DP execution benchmarks, where relational models like GNNs clearly outperformed architectures with weaker inductive biases. GNNs have been a long-standing interest of mine [6], so the time was right to release our own contribution to the area:</p><h2 id="neural-execution-of-graph-algorithms">Neural Execution of Graph Algorithms</h2><p>In this paper [7], concurrently released with Xu <em>et al. </em>[2], we conducted a thorough empirical analysis of learning to execute with GNNs. We found that while algorithmic alignment is indeed a powerful tool for model class selection, it unfortunately does not allow us to be <em>reckless</em>. Namely, we cannot just apply any expressive GNN to an algorithmic execution task and expect great results, especially <strong>out-of-distribution</strong>&#x2014;which we previously identified as a key setting in which &#x201C;true&#x201D; reasoning systems should perform well. Much like other neural networks, GNNs can very easily <em>overfit</em> to the characteristics of the distribution of training inputs, learning &#x201C;clever hacks&#x201D; and sidestepping the actual procedure that they are attempting to execute. </p><p>We hence identify three key observations on careful <em>inductive biases</em> to use, to improve the algorithmic alignment to certain path-finding problems even further and allow for generalising to <em>5x larger inputs</em> at test time:</p><ol><li>Most traditional deep learning setups involve a stack of layers with <em>unshared</em> parameters. This fundamentally limits the amount of computation the neural network can perform: if, at test time, an input much larger than the ones in the training data arrives, it would be expected that more computational steps are needed&#x2014;yet, the unshared GNN has no way to support that. To address this, we adopt the <strong>encode-process-decode </strong>paradigm [8]: a single <em>shared</em> processor GNN is iterated for many steps, and this number of steps can be variable at both training and inference time. Such an architecture also allows a neat way to algorithmically align to <em>iterative</em> computation, as most algorithms involve repeatedly applying a certain computation until convergence.</li><li>Since most path-finding algorithms (including Bellman-Ford) require &#x201C;local&#x201D; optimisation (i.e. choosing <em>exactly</em> <em>one</em> optimal neighbour at every step), we opted to use <strong>max aggregation</strong> to combine the messages sent in GNNs. While this may seem like a very intuitive idea, it went strongly against the folklore of the times, as max-GNNs were known to be theoretically inferior to sum-GNNs at distinguishing non-isomorphic graphs [9]. (We now have solid theoretical evidence [10] for why this is a good idea.)</li><li>Lastly, while most deep learning setups only require producing an output given an input, we found that this misses out on a wealth of ways to instruct the model to align to the algorithm. For example, there are many interesting <em>invariants</em> that algorithms have that can be explicitly taught to a GNN. In the case of Bellman-Ford, after <em>k </em>iterations are executed, we should be expected to be able to recover all shortest paths that are no more than <em>k</em> hops away from the source node. Accordingly, we use this insight to provide <strong>step-wise supervision</strong> to the GNN at every step. This idea appears to be gaining traction in Large Language Model design in recent months [11, 12].</li></ol><p>All three of the above changes make for stronger algorithmically-aligned GNNs.</p><h2 id="playing-the-alignment-game">Playing the alignment game</h2><p>It must be stressed that the intuitive idea of algorithmic alignment&#x2014;taking inspiration from Computer Science concepts in architecture design&#x2014;is by no means novel! The text-book example of this are the <em>neural Turing machine </em>(<strong>NTM</strong>) and <em>differentiable neural computer</em> (<strong>DNC</strong>) [13, 14]. These works are decisively influential; in fact, in its attempt to make random-access memory compatible with gradient-based optimisation, the NTM gave us one of the earliest forms of <em><strong>content-based attention</strong></em>, three years before Transformers [15]!</p><p>However, despite their influence, these architectures are nowadays virtually <em>never</em> used in practice. There are many possible causes for this, but in my opinion it is because their design was too <em>brittle</em>: trying to introduce many differentiable components into the same architecture at once, without a clear guidance on <em>how</em> to compose them, or a way to easily debug them once they failed to show useful signal on a new task. Our line of work still wants to build something like an NTM&#x2014;but make it more successfully deployed, by using algorithmic alignment to more carefully prototype each building block in isolation, and have a more granular view of which blocks benefit execution of which target algorithms.</p><p>Our approach of &#x201C;playing the algorithmic alignment game&#x201D; appears to have yielded a successful line of specialised (G)NNs, and we now have worthy &#x2018;fine-grained&#x2019; solutions for learning to execute linearithmic sequence algorithms [16], iterative algorithms [17], pointer-based data structures [18], as well as persistent auxiliary memory [19]. Eventually, these insights also carried over to more fine-grained theory as well. In light of our NEGA paper [7], the inventors of algorithmic alignment refined their theory into what is now known as <em>linear algorithmic alignment </em>[10], providing justification for, among other things, our use of the max aggregation. More recent insights show that understanding algorithmic alignment may require <em>causal reasoning </em>[20,21], properly formalising it may require <em>category theory </em>[22], and properly describing it may require analysing <em>asynchronous computation </em>[23]. Algorithmic alignment is therefore turning into a very exciting area for mathematical approaches to deep learning in recent years.</p><h2 id="why-not-just-run-the-target-algorithmand-rebuttals">Why<em> </em>not just <em>run</em> the target algorithm?...and rebuttals</h2><p>While it appears a lot of useful progress has been made towards addressing our initial &#x201C;toy question&#x201D;, the idea of learning to execute is not one that easily passes peer review. My personal favourite reviewer comment I received&#x2014;quoted in full&#x2014;was as follows: <em>&#x201C;This paper will certainly accelerate research in building algorithmic models, and there&#x2019;s certainly a lot of researchers that would make advantage of it, I am just not sure that this research should even <strong>exist</strong>&#x201D;</em>.</p><p>This is clearly not the nicest thing to receive as the first author of a paper. But still, let&#x2019;s try to put my ego aside, and see what can be taken away from such reviews. There is a clear sense in which such reviews are raising an entirely valid point: <em>tautologically</em>, the target algorithm will execute <em>itself</em> better (or equally good) than any GNN we&#x2019;d ever learn over it. Clearly, if we want wide-spread recognition of these ideas, we need to show how learning to execute can be usefully applied beyond the context of &#x201C;pure&#x201D; execution.</p><p>Our exploration led us to many possible ideas, and in the remainder of this article, I will show <em>three</em> ideas that saw the most impact.</p><p>First, <em><strong>algorithmically aligned models can accelerate science</strong></em>. And if you want clear evidence of this, look no further than the cover of Nature [24]. In our work, we train (G)NNs to fit a mathematical dataset of interest to a mathematician, and then use simple gradient saliency methods to <em>signal</em> to the mathematician which parts of the inputs to focus their attention at. While such signals are often remarkably noisy, they do allow a mathematician to study only the most salient 20-30 nodes in a graph that would otherwise have hundreds or thousands of nodes, making pattern discovery much easier. The discovered patterns can then form the basis of novel theorems, and/or be used to derive conjectures.</p><p>With this simple approach, we were able to drive independent contributions to two highly disparate areas of math: knot theory [25] and representation theory [26], both subsequently published in their areas&#x2019; respective top-tier journals, hence earning us the Nature accolade. But, while our approach is simple in principle, a question arose especially in the representation theory branch: <em>which (G)NN to use</em>? Standard expressive GNNs did not yield clearly interpretable results.</p><p>This is where algorithmic alignment helped us: Geordie Williamson, our representation theory collaborator, provided an algorithm that would compute the outputs we care about, <em>if</em> we had access to privileged information. We achieved our best results with a GNN model that was explicitly aligning its components to this target algorithm.</p><p>More generally: in this case, a target algorithm existed, but executing it was <strong>inapplicable</strong> (due to privileged inputs). Algorithmic alignment allowed us to embed &#x201C;priors&#x201D; from it anyway.</p><p>Second, <em><strong>algorithmically aligned models are fast heuristics</strong></em>. In recent work with computer networking and machine learning collaborators from ETH Z&#xFC;rich, we studied the applicability of neural algorithmic reasoning in computer networking [27]. Specifically, we sought to expedite the challenging task of <em>configuration synthesis</em>: based on a given <strong>specification</strong> of constraints a computer network should satisfy, produce a corresponding network <strong>configuration</strong> (a graph specifying the routers in a network and their connections). This configuration must satisfy all of the specifications, <em>once</em> a network <strong>protocol</strong> has been executed over it.</p><p>Producing these configurations is known to be a very challenging NP-hard problem&#x2014;in practice, it is usually solved with slow SMT solvers, which can often require<em> doubly-exponential </em>complexity. Instead, we choose to use ideas from algorithmic reasoning to generate configurations by <em>inverting the protocol</em> (which can be seen as a graph algorithm). Specifically, we generate many random network configurations, execute the protocol over them, and collect all true facts about the resulting network to extract corresponding specifications. This gives us all we need to generate a graph-based dataset from specifications to configurations, and fit an algorithmically-aligned GNN to this dataset.</p><p>Naturally, by virtue of just requiring a forward pass of a machine learning model, this approach is substantially faster than SMT solvers at inference time: for certain configurations, we have observed over <strong>490x speedup </strong>over the prior state-of-the-art. Of course, there is no free lunch: the price we pay for this speedup are occasional inaccuracies in the produced configurations at test-time. That being said, on all the relevant distributions we evaluated, the average number of constraints satisfied has consistently been over 90%, which makes our method already applicable for downstream <em>human-in-the-loop</em> use&#x2014;and it is likely to accelerate human designers, as very often, the initial configurations are <em>unsatisfiable</em>, meaning SMT solvers will spend a lot of effort only to say a satisfying configuration cannot be found. During this time, a fast forward pass of a GNN could have allowed for far more rapid iteration.</p><p>More generally: in this case, a target algorithm is only being <strong>approximated</strong>, but such that it provides a <strong>fast</strong> and <em>reasonably accurate</em> <strong>heuristic</strong>, enabling rapid human-in-the-loop iteration.</p><h2 id="a-core-problem-with-applying-classical-algorithms">A core problem with applying classical algorithms</h2><p>To set us up for the third and final idea, let me pose a motivating task: <em>&#x201C;Find me the optimal path from A to B&#x201D;</em>. How do you respond to this prompt?</p><p>Chances are, especially if you come from a theoretical Computer Science background like me, that you will respond to this question in a very <em>singular</em> way. Namely, you will subtly <em>assume</em> that I am providing you with a weighted graph and asking you for the shortest path between two specific vertices in this graph. We can then diligently apply our favourite path-finding algorithm (e.g. Dijkstra&#x2019;s algorithm [28]) to resolve this query. I should highlight that, at least at the time of writing this text, the situation is not very different with most of today&#x2019;s state-of-the-art AI chatbots&#x2014;when prompted with the above task, while they will often seek further information, they will promptly assume that there is an input weighted graph provided!</p><p>However, there&#x2019;s nothing in my question that requires <em>either</em> of these assumptions to be true. Firstly, the real-world is often incredibly noisy and dynamic, and rarely provides such abstractified inputs. For example, the task might concern the optimal way to travel between two places in a real-world transportation network, which is a challenging routing problem that relies on processing noisy, complex data to estimate real-time traffic speeds&#x2014;a lesson I&#x2019;ve personally learnt, when I worked on deploying GNNs within <em>Google Maps </em>[29]. Secondly, why must &#x201C;optimal&#x201D; equal <em>shortest</em>? In the context of routing traffic, and depending on the specific contexts and goals, &#x201C;optimal&#x201D; may well mean most cost-efficient, least polluting, etc. All of these variations make a straightforward application of Dijkstra&#x2019;s algorithm difficult, and may in practice require a <em>combination</em> of several algorithms.</p><p>Both of these issues highlight that we often need to make a challenging <em>mapping</em> between complex real-world data and an input that will be appropriate for running a target algorithm. Historically, this mapping is performed by <em>humans</em>, either manually or via specialised heuristics. This naturally invites the following question: <em>Can humans ever hope to be able to <strong>manually</strong> find the necessary mapping, in general?</em> I would argue that, at least since the 1950s, we&#x2019;ve known the answer to this question to be <strong>no</strong>. Directly quoting from the paper of Harris and Ross [30], which is one of the first accounts of the <em>maximum flow</em> problem (through analysing railway networks):</p><blockquote><em>The evaluation of both railway system and individual track capacities is, to a considerable extent, an art. The authors know of no tested mathematical model or formula that includes all variations and imponderables that must be weighed. Even when the individual has been closely associated with the particular territory he is evaluating, the final answer, however accurate, is largely one of judgment and experience.</em></blockquote><p>Hence, even highly skilled humans often need to make educated guesses when attaching a single scalar &#x201C;capacity&#x201D; value to each railway link&#x2014;and this needs to be done <em>before</em> any flow algorithm can be executed over the input network! Furthermore, as evidenced by the following statement from the recent Amazon Last Mile Routing Research Challenge [31],</p><blockquote><em>...there remains an important gap between theoretical route planning and real-life route execution that most optimization-based approaches cannot bridge. This gap relates to the fact that in real-life operations, the quality of a route is not exclusively defined by its theoretical length, duration, or cost but by a multitude of factors that affect the extent to which drivers can effectively, safely, and conveniently execute the planned route under real-life conditions.</em></blockquote><p>Hence, these considerations remain relevant even in the high-stakes, big data settings of today. This is a fundamental <strong>divide</strong> between classical algorithms and the real-world problems they were originally designed to solve! Satisfying the strict preconditions for applying an algorithm may lead to drastic loss of information from complex, naturally-occurring inputs. Or, put simply:</p><p><strong>It doesn&#x2019;t matter if the algorithm is provably correct, if we execute it on the </strong><em><strong>wrong</strong></em><strong> inputs!</strong></p><p>This issue gets significantly tricker if the data is <em>partially observable</em>, there are <em>adversarial actors</em> in the environment, etc. I must stress that this is not an issue theoretical computer scientists tend to concern themselves with, and probably for good reason! Focussing on the algorithms in the &#x201C;abstractified&#x201D; setting is already quite challenging, and it has yielded some of the most beautiful computational routines that have significantly transformed our lives. That being said, if we want to give &#x201C;superpowers&#x201D; to these routines and make them applicable way beyond the kinds of inputs they were originally envisioned for, we need to find some way to bridge this divide. Our proposal, the <strong>neural algorithmic reasoning </strong>blueprint [32], aims to bridge this divide by <em>neuralising</em> the target algorithm.</p><h2 id="neural-algorithmic-reasoning">Neural Algorithmic Reasoning</h2><p>Since a key limitation we identified is the need for <em>manual</em> input feature engineering, a good first point of attack could be to simply replace the feature engineer with a neural network encoder. After all, replacing feature engineering is how deep learning got its major breakthrough [33]! The encoder would learn to predict the inputs to the algorithm from the raw data, and then we can execute the algorithm over these inputs to obtain the outputs we care about.<br></p><p>This kind of pipeline is remarkably popular [34]; in recent times, there have been seminal results allowing for backpropagating through the encoder even when the algorithm itself is non-differentiable [35]. However, it suffers from an <strong>algorithmic bottleneck</strong> problem: namely, it is fully committing itself to the algorithm&#x2019;s outputs [36]. That is, if the inputs to the algorithm are poorly predicted by the encoder, we run into the same issue as before&#x2014;the algorithm will give a perfect answer in an incorrect environment. Since the required inputs are usually scalar in nature (e.g. a single distance per edge of the input graph), the encoder is often tasked with mapping the extremely rich structure of real world data into only a single number. This particularly becomes an issue with low-data or partially-observable scenarios.</p><p>To break the algorithmic bottleneck, we instead opt to represent both the encoder <em>and</em> the algorithm as high-dimensional neural networks! Now, our algorithmic model is a <em>processor</em> neural network&#x2014;mapping high dimensional embeddings to high dimensional embeddings. To recover relevant outputs, we can then attach an appropriate decoder network to the output embeddings of the processor. If we were able to guarantee that this processor network &#x201C;captures the computation&#x201D; of the algorithm, then we would simultaneously resolve all of the issues previously identified:</p><ul><li>Our pipeline would be an end-to-end <em>differentiable</em> neural network;</li><li>It would also be <em>high-dimensional</em> throughout, alleviating the algorithmic bottleneck;</li><li>If any computation can not be explained by the processor, we can add <em>skip connections </em>going directly from the encoder to the decoder, to handle such residual information.</li></ul><p>So, all we need now is to produce a neural network which captures computation! But, wait&#x2026; that&#x2019;s exactly what we have been talking about in this entire blog post! :)</p><p>We have arrived at the <strong>neural algorithmic reasoning</strong> (NAR) blueprint [32]:</p><figure class="kg-card kg-image-card"><img src="https://lh3.googleusercontent.com/WHPz3vLGaMr2toopTHNgVwz7YDJRqara4Vj9Fxa0EjigyCWlApk42y_pFFUqUXD_-TnCjbTdxZOhtW0JlrxkD2Y9d8mSCpulh6LuoktBWWXclkxCPOzlaqMY18Cn3LY6DtKOXCGhR21XWSxg_SoxKtzwURHMn996upMOykPfmk1CUsKXhlprWypZcDeen2V7oW-mtmaB24_OSXMz1z7-nR7W9OfMsvhou4Vp" class="kg-image" alt="Neural algorithmic reasoning" loading="lazy" width="564" height="468"></figure><p>This figure represents a neat summary of everything we have covered so far: first, we obtain a suitable processor network, <em>P</em>, by algorithmic alignment to a target algorithm, or pre-training on learning to execute the target algorithm, or both! Once ready, we include <em>P</em> into any neural pipeline we care about over raw (natural) data. This allows us to apply target algorithms <em>&#x201C;on inputs previously considered inaccessible to them&#x201D;</em>, in the words of the original NAR paper [32]. Depending on the circumstances, we may or may not wish to keep <em>P</em>&#x2019;s parameters frozen once deployed, or <em>P</em> may even be entirely <em>nonparametric </em>[37,38]!</p><p>While initially only a proposal, there are now several successful NAR instances that have been published at top-tier venues [36,39,40,41]. In the most-recent such paper [41], we aimed to study how to classify blood vessels in the mouse brain&#x2014;a very challenging graph task spanning millions of nodes and edges [42]. However, while it&#x2019;s not trivial to directly classify blood vessels from their features, it is reasonably safe to assume that the main purpose of blood vessels is to conduct blood flow&#x2014;hence, an algorithm for <em>flow analysis</em> could be a suitable one to deploy here. Accordingly, we first pre-trained a NAR processor to execute the relevant maximum-flow and minimum-cut algorithms [43], then successfully deployed it on the brain vessel graph, surpassing the previous state-of-the-art GNNs. It is worthy to note that the brain vessel graph is <strong>180,000x larger</strong> than the synthetic graphs we used for learning to execute, and minimal hyperparameter tuning was applied throughout! We are confident this success is only the first of many.</p><h2 id="where-can-we-get-good-processors-from">Where can we get <em>good</em> processors from?</h2><p>While the ideas given in prior subsections hopefully provide a good argument for the utility of capturing classical computation, in practice we still need to know <em>which computation to capture</em>! All of the NAR papers referenced above use target algorithms highly relevant to the downstream task, and the processors are trained using bespoke execution datasets built on top of those algorithms. <em>This often requires <strong>both</strong> domain expertise and computer science expertise, and hence represents a clear <strong>barrier of entry</strong>!</em></p><p>Since my beginnings in NAR, I have been strongly interested in reducing this barrier of entry, while also providing a collection of &#x201C;base processors&#x201D; that should be useful in a wide variety of tasks. This resulted in a two-year-long engineering effort, culminating by open-sourcing the <em><strong>CLRS</strong> Algorithmic Reasoning Benchmark</em> (<a href="https://github.com/deepmind/clrs">https://github.com/deepmind/clrs</a>) [44].</p><p>The CLRS benchmark was inspired by the iconic <em>Introduction to Algorithms</em> (CLRS) textbook from <strong>C</strong>ormen, <strong>L</strong>eiserson, <strong>R</strong>ivest and <strong>S</strong>tein [1], which is one of the most popular undergraduate textbooks on classical algorithms and data structures, and a &#x201C;bible&#x201D; for competitive programmers. Despite its many thousands of pages, it only contains ~90 distinct algorithms, and these algorithms tend to form the foundations behind entire careers in software engineering! Hence, the algorithms in CLRS can form our desired solid set of &#x201C;base processors&#x201D;, and we set out to make it easy to construct and train NAR processors to execute the algorithms in CLRS.</p><p>In its first incarnation, we released CLRS-30, a collection of dataset and processor generators for <em>thirty</em> algorithms in CLRS, spanning a wide variety of skills: sorting, searching, dynamic programming, graph algorithms, string algorithms and geometric algorithms. </p><p>What makes CLRS-30 special is the wide variety of data, models and pipelines it exposes to the user: given an appropriate specification of the target algorithm&#x2019;s variables, an implementation of the target algorithm, and a desirable random input sampler, CLRS will <em>automatically</em> produce the full execution trajectories of this algorithm&#x2019;s variables in a spatio-temporal graph-structured format, relevant encoder/decoder/processor models, and loss functions. For this reason, we typically refer to CLRS as a <em>&#x201C;dataset and baseline <strong>generator</strong>&#x201D;</em> rather than an individual dataset.</p><p>For example, here is a CLRS-produced trajectory for<em> insertion sorting</em> a list [5, 2, 4, 3, 1]:</p><figure class="kg-card kg-image-card"><img src="https://lh5.googleusercontent.com/m67FIsRuMGYFQEy44i-WzBF5nbWZEvwpuzIfGBUxn4xmhe3AU_odrabArTVNhXBaqkiaB1BuXm4-b-bNsyDF8ogIvYyhfpfamUa8UGUXDpN1dPzmuEEZUfF-gPHlc5dnexMZTig_MWOSuHeoyGRb3PbLAZrkh1Tqv0cwL5UP9gWISV0LVvlMzQTPrTb2IppyYu5p_f9YLvc9EKam7TqdP7p5mN3mHm8i_77Z" class="kg-image" alt="Neural algorithmic reasoning" loading="lazy" width="624" height="80"></figure><p>This trajectory fully exposes the internal state of the algorithm: how the list&#x2019;s pointers (in <strong>green</strong>) change over time, which element is currently being sorted (in<strong> red</strong>), and the position it needs to be sorted into (in <strong>blue</strong>). By default, these can be used for step-wise supervision, although recently more interesting ways to use the trajectories, such as Hint-ReLIC [21], were proposed.</p><h2 id="why-stop-at-just-one-algorithm-could-we-learn-all-thirty">Why stop at just <em>one</em> algorithm? Could we learn <strong>all thirty</strong>?</h2><p>As mentioned, CLRS-30 converts thirty diverse algorithms into a <em>unified</em> graph representation. This paves the way for an obvious question: <em>could we learn <strong>one processor</strong> (with a single set of weights) to execute <strong>all</strong> of them?</em> To be clear, we envision a NAR processor like this:</p><figure class="kg-card kg-image-card"><img src="https://lh5.googleusercontent.com/zmXm2KtRXskseK2BPKZomkgtK23EoR3rx20g_RvPNYrB1WGHcChz_r1MM3W0hg09LOzph7fzIHbNUZsB3WUY0RcvGhH2dH8VDUL4lz-2Ax1dS_i7NWP7-GPwMeoMK6xWF-GCTRWyLFAl496tVr2UoCktAzJ5MJquh92jyKVAZsdR6CzZa_6iww7NGhL16p5FolRpX93P1OEoRwmuEX1dsZZ7XeIjaSCap-BE" class="kg-image" alt="Neural algorithmic reasoning" loading="lazy" width="624" height="343"></figure><p>That is, a single (G)NN, capable of executing sorting, path-finding, convex hull finding, and all other CLRS algorithms. Since the input and output dimensionalities can vary wildly between the different algorithms, we would still propose using separate encoders and decoders for each algorithm&#x2014;mapping into and out of the processor&#x2019;s latent space&#x2014;however, we deliberately keep these as<em> linear functions</em> to place the majority of the computational effort on <em>P</em>.</p><p>However, despite the base idea being simple in principle, this proves to be no trivial endeavour. Most prior attempts to learn multiple algorithms, such as NEGA [7] or its successor, NE++ [45], have deliberately focussed on learning to execute highly related algorithms, where the learning signals are likely to be well-correlated. Accordingly, our initial single-processor multi-task training runs on all of CLRS-30 resulted in NaNs. </p><p>We have been able to identify <em>single-task instabilities</em> as the main culprit of this: if the gradients for any individual algorithmic task are noisy or unstable, this translates to unstable training on <em>all</em> thirty of them. Hence, we launched a dedicated<em> &#x201C;mini-strike&#x201D;</em> of two months to identify and fix learning stability issues in single-task learners. Our resulting model, Triplet-GMPNN [46], improves absolute mean performance over CLRS-30 by over 20% from prior state-of-the-art, and enables successful multi-task training! What&#x2019;s more, we now have a single <em><strong>generalist</strong></em> Triplet-GMPNN that, on average, <em>matches</em> the <em><strong>thirty specialist</strong></em> Triplet-GMPNNs, evaluated out-of-distribution:</p><figure class="kg-card kg-image-card"><img src="https://lh5.googleusercontent.com/wtZ1_-p3_vlfXU9tNq6fORqWdpfe8_AbupW4jGOMak4n_klXyJeBhFMDtQu6Ec5l2w1ZFFmQswmJFhA0wavnhjs610-QNPzqAJRx-AfDe0jKpQCucXLb52qxbwmHtwNan9_e54nGgOTzEo0hsIERLGranrvm4fHpdVp3O9w2X6r7-8q9kMqmaxBQI5ZFz4YDy8KBy2W2STw8PpoAqIlS-LHNIH3X-Co4PEA8" class="kg-image" alt="Neural algorithmic reasoning" loading="lazy" width="624" height="207"></figure><p>While it is evident from this plot that we still have a long way to go before we produce a fully &#x201C;algorithmic&#x201D; NAR processor library, this result has been seen as a significant &#x201C;compression&#x201D; milestone, similar to the Gato [47] paper. The release of our Triplet-GMPNN model sparked extremely interesting discussions on <a href="https://twitter.com/DeepMind/status/1600852768125726720">Twitter</a>, <a href="https://www.reddit.com/r/singularity/comments/xlsgws/deepmind_a_generalist_neural_algorithmic_learner/">Reddit</a> and <a href="https://news.ycombinator.com/item?id=33910542">HackerNews</a>, especially in light of its implications to constructing generally-intelligent systems. &#xA0;Generally, the progress in NAR made by various groups over just the past three years has been incredible to observe. And we&#x2019;re just getting started: now that we can, in principle, build generalist NAR processors, I am immensely excited about the potential this holds for future research and products.</p><h2 id="want-to-know-more">Want to know more?</h2><p>Needless to say, there is <strong>a lot</strong> of material this article does not cover&#x2014;especially, the technical and implementation details of many of the papers discussed herein. If what you&#x2019;ve read here has made you interested to know more, or even play with NAR models yourself, I recommend you check out the LoG&#x2019;22 Tutorial on NAR (<a href="https://algo-reasoning.github.io/">https://algo-reasoning.github.io/</a>), which I delivered alongside <a href="https://andreeadeac22.github.io/">Andreea Deac</a> and <a href="https://scholar.google.com/citations?hl=en&amp;user=DdFjaEEAAAAJ">Andrew Dudzik</a>. Over the course of just under three hours, we cover all of the theory needed to master the foundations of developing, deploying, and deepening neural algorithmic reasoners, along with plentiful code pointers and references. And of course, you are more than welcome to reach out directly, should you have any follow-up questions, interesting points of discussion, or even interesting ideas for projects!</p><h2 id="references">References</h2><ol><li>TH. Cormen, CE. Leiserson, RL. Rivest, and C. Stein. <em>Introduction to Algorithms.</em> MIT Press&#x2019;22.</li><li>K. Xu, J. Li, M. Zhang, SS. Du, K-I. Kawarabayashi, and S. Jegelka. <em>What Can Neural Networks Reason About?.</em> ICLR&#x2019;20.</li><li>P. Veli&#x10D;kovi&#x107;. <em>Everything is Connected: Graph Neural Networks.</em> Current Opinion in Structural Biology&#x2019;23.</li><li>R. Bellman. <em>On a Routing Problem.</em> Quarterly of Applied Mathematics&#x2019;58.</li><li>R. Bellman. <em>Dynamic Programming.</em> Science&#x2019;66.</li><li>P. Veli&#x10D;kovi&#x107;, G. Cucurull, A. Casanova, A. Romero, P. Li&#xF2;, and Y. Bengio. <em>Graph Attention Networks</em>. ICLR&#x2019;18.</li><li>P. Veli&#x10D;kovi&#x107;, R. Ying, M. Padovano, R. Hadsell, and C. Blundell. <em>Neural Execution of Graph Algorithms</em>. ICLR&#x2019;20.</li><li>JB. Hamrick, KR. Allen, V. Bapst, T. Zhu, KR. McKee, JB. Tenenbaum, and PW. Battaglia. <em>Relational inductive biases for physical construction in humans and machines</em>. CCS&#x2019;18.</li><li>K. Xu*, W. Hu*, J. Leskovec, and S. Jegelka. <em>How Powerful are Graph Neural Networks?</em>. ICLR&#x2019;19.</li><li>K. Xu, M. Zhang, J. Li, SS. Du, K-I. Kawarabayashi, and S. Jegelka. <em>How Neural Networks Extrapolate: From Feedforward to Graph Neural Networks.</em> ICLR&#x2019;21.</li><li>J. Uesato*, N. Kushman*, R. Kumar*, F. Song, N. Siegel, L. Wang, A. Creswell, G. Irving, and I. Higgins. <em>Solving math word problems with process- and outcome-based feedback.</em> arXiv&#x2019;22.</li><li>H. Lightman*, V. Kosaraju*, Y. Burda*, H. Edwards, B. Baker, T. Lee, J. Leike, J. Schulman, I. Sutskever, and K. Cobbe. <em>Let&#x2019;s Verify Step by Step</em>. arXiv&#x2019;23.</li><li>A. Graves, G. Wayne, and I. Danihelka. <em>Neural Turing Machines</em>. arXiv&#x2019;14.</li><li>A. Graves, G. Wayne, M. Reynolds, T. Harley, I. Danihelka, A. Grabska-Barwi&#x144;ska, S. G&#xF3;mez Colmenarejo, E. Grefenstette, T. Ramalho, J. Agapiou, A. Puigdom&#xE8;nech Badia, KM. Hermann, Y. Zwols, G. Ostrovski, A. Cain, H. King, C. Summerfield, P. Blunsom, K. Kavukcuoglu, and D. Hassabis. <em>Hybrid computing using a neural network with dynamic external memory</em>. Nature&#x2019;16.</li><li>A. Vaswani*, N. Shazeer*, N. Parmar*, J. Uszkoreit*, L. Jones*, AN. Gomez*, &#x141;. Kaiser*, and I. Polosukhin*. <em>Attention is all you need</em>. NeurIPS&#x2019;17.</li><li>K. Freivalds, E. Ozoli&#x146;&#x161;, and A. &#x160;ostaks. <em>Neural Shuffle-Exchange Networks - Sequence Processing in O(n log n) Time</em>. NeurIPS&#x2019;19.</li><li>H. Tang, Z. Huang, J. Gu, B-L. Lu, and H. Su. <em>Towards Scale-Invariant Graph-related Problem Solving by Iterative Homogeneous GNNs</em>. NeurIPS&#x2019;20.</li><li>P. Veli&#x10D;kovi&#x107;, L. Buesing, MC. Overlan, R. Pascanu, O. Vinyals, and C. Blundell. <em>Pointer Graph Networks</em>. NeurIPS&#x2019;20.</li><li>H. Strathmann, M. Barekatain, C. Blundell, and P. Veli&#x10D;kovi&#x107;. <em>Persistent Message Passing</em>. ICLR&#x2019;21 SimDL.</li><li>B. Bevilacqua, Y. Zhou, and B. Ribeiro. <em>Size-invariant graph representations for graph classification extrapolations</em>. ICML&#x2019;21.</li><li>B. Bevilacqua, K. Nikiforou*, B. Ibarz*, I. Bica, M. Paganini, C. Blundell, J. Mitrovic, and P. Veli&#x10D;kovi&#x107;. <em>Neural Algorithmic Reasoning with Causal Regularisation</em>. ICML&#x2019;23.</li><li>A. Dudzik*, and P. Veli&#x10D;kovi&#x107;*. <em>Graph Neural Networks are Dynamic Programmers</em>. NeurIPS&#x2019;22.</li><li>A. Dudzik, T. von Glehn, R. Pascanu, and P. Veli&#x10D;kovi&#x107;. <em>Asynchronous Algorithmic Alignment with Cocycles</em>. ICML&#x2019;23 KLR.</li><li>A. Davies, P. Veli&#x10D;kovi&#x107;, L. Buesing, S. Blackwell, D. Zheng, N. Toma&#x161;ev, R. Tanburn, P. Battaglia, C. Blundell, A. Juh&#xE1;sz, M. Lackenby, G. Williamson, D. Hassabis, and P. Kohli. <em>Advancing mathematics by guiding human intuition with AI</em>. Nature&#x2019;21.</li><li>A. Davies, A. Juh&#xE1;sz, M. Lackenby, and N. Toma&#x161;ev. <em>The signature and cusp geometry of hyperbolic knots</em>. Geometry &amp; Topology (in press).</li><li>C. Blundell, L. Buesing, A. Davies, P. Veli&#x10D;kovi&#x107;, and G. Williamson. <em>Towards combinatorial invariance for Kazhdan-Lusztig polynomials</em>. Representation Theory&#x2019;22.</li><li>L. Beurer-Kellner, M. Vechev, L. Vanbever, and P. Veli&#x10D;kovi&#x107;. <em>Learning to Configure Computer Networks with Neural Algorithmic Reasoning</em>. NeurIPS&#x2019;22.</li><li>EW. Dijkstra. <em>A note on two papers in connection with graphs</em>. Numerische Matematik&#x2019;59.</li><li>A. Derrow-Pinion, J. She, D. Wong, O. Lange, T. Hester, L. Perez, M. Nunkesser, S. Lee, X. Guo, B. Wiltshire, PW. Battaglia, V. Gupta, A. Li, Z. Xu, A. Sanchez-Gonzalez, Y. Li, and P. Veli&#x10D;kovi&#x107;. <em>ETA Prediction with Graph Neural Networks in Google Maps</em>. CIKM&#x2019;21.</li><li>TE. Harris, and FS. Ross. <em>Fundamentals of a method for evaluating rail net capacities</em>. RAND Tech Report&#x2019;55.</li><li>M. Winkenbach, S. Parks, and J. Noszek. <em>Technical Proceedings of the Amazon Last Mile Routing Research Challenge</em>. 2021.</li><li>P. Veli&#x10D;kovi&#x107;, and C. Blundell. <em>Neural Algorithmic Reasoning</em>. Patterns&#x2019;21.</li><li>A. Krizhevsky, I. Sutskever, and GE. Hinton. <em>ImageNet Classification with Deep Convolutional Neural Networks</em>. NeurIPS&#x2019;12.</li><li>Q. Cappart, D. Ch&#xE9;telat, EB. Khalil, A. Lodi, C. Morris, and P. Veli&#x10D;kovi&#x107;. <em>Combinatorial optimization and reasoning with graph neural networks</em>. JMLR&#x2019;23.</li><li>M. Vlastelica*, A. Paulus*, V. Musil, G. Martius, and M. Rol&#xED;nek. <em>Differentiation of Blackbox Combinatorial Solvers</em>. ICLR&#x2019;20.</li><li>A. Deac, P. Veli&#x10D;kovi&#x107;, O. Milinkovi&#x107;, P-L. Bacon, J. Tang, and M. Nikoli&#x107;. <em>Neural Algorithmic Reasoners are Implicit Planners</em>. NeurIPS&#x2019;21.</li><li>B. Wilder, E. Ewing, B. Dilkina, and M. Tambe. <em>End to end learning and optimization on graphs</em>. NeurIPS&#x2019;19.</li><li>M. Garnelo, and WM. Czarnecki. <em>Exploring the Space of Key-Value-Query Models with Intention</em>. 2023.</li><li>Y. He, P. Veli&#x10D;kovi&#x107;, P. Li&#xF2;, and A. Deac. <em>Continuous Neural Algorithmic Planners</em>. LoG&#x2019;22.</li><li>P. Veli&#x10D;kovi&#x107;*, M. Bo&#x161;njak*, T. Kipf, A. Lerchner, R. Hadsell, R. Pascanu, and C. Blundell. <em>Reasoning-Modulated Representations</em>. LoG&#x2019;22.</li><li>D. Numeroso, D. Bacciu, and P. Veli&#x10D;kovi&#x107;. <em>Dual Algorithmic Reasoning</em>. ICLR&#x2019;23.</li><li>JC. Paetzold, J. McGinnis, S. Shit, I. Ezhov, P. B&#xFC;schl, C. Prabhakar, MI. Todorov, A. Sekuboyina, G. Kaissis, A. Ert&#xFC;rk, S. G&#xFC;nnemann, and BH. Menze. <em>Whole Brain Vessel Graphs: A Dataset and Benchmark for Graph Learning and Neuroscience</em>. NeurIPS&#x2019;21 Datasets and Benchmarks.</li><li>LR. Ford, and DR. Fulkerson. <em>Maximal flow through a network</em>. Canadian Journal of Mathematics&#x2019;56.</li><li>P. Veli&#x10D;kovi&#x107;, A. Puigdom&#xE8;nech Badia, D. Budden, R. Pascanu, A. Banino, M. Dashevskiy, R. Hadsell, and C. Blundell. <em>The CLRS Algorithmic Reasoning Benchmark</em>. ICML&#x2019;22.</li><li>L-PAC. Xhonneux, A. Deac, P. Veli&#x10D;kovi&#x107;, and J. Tang. <em>How to transfer algorithmic reasoning knowledge to learn new algorithms?</em>. NeurIPS&#x2019;21.</li><li>B. Ibarz, V. Kurin, G. Papamakarios, K. Nikiforou, M. Bennani, R. Csord&#xE1;s, A. Dudzik, M. Bo&#x161;njak, A. Vitvitskyi, Y. Rubanova, A. Deac, B. Bevilacqua, Y. Ganin, C. Blundell, and P. Veli&#x10D;kovi&#x107;. <em>A Generalist Neural Algorithmic Learner</em>. LoG&#x2019;22.</li><li>S. Reed*, K. &#x17B;o&#x142;na*, E. Parisotto*, S. G&#xF3;mez Colmenarejo, A. Novikov, G. Barth-Maron, M. Gim&#xE9;nez, Y. Sulsky, J. Kay, JT. Springenberg, T. Eccles, J. Bruce, A. Razavi, A. Edwards, N. Heess, Y. Chen, R. Hadsell, O. Vinyals, M. Bordbar, and N. de Freitas. <em>A Generalist Agent</em>. TMLR&#x2019;22.</li></ol><h2 id="author-bio">Author Bio</h2><p>Petar Veli&#x10D;kovi&#x107; is a Staff Research Scientist at <a href="https://www.deepmind.com/">Google DeepMind</a>, Affiliated Lecturer at the <a href="https://www.cam.ac.uk/">University of Cambridge</a>, and an Associate of <a href="https://www.clarehall.cam.ac.uk/">Clare Hall, Cambridge</a>. Petar holds a PhD in Computer Science from the <a href="https://www.cam.ac.uk/">University of Cambridge</a> (<a href="https://www.trin.cam.ac.uk/">Trinity College</a>), obtained under the supervision of <a href="https://www.cst.cam.ac.uk/~pl219">Pietro Li&#xF2;</a>. His research concerns <a href="https://www.youtube.com/watch?v=9cxhvQK9ALQ">geometric deep learning</a>&#x2014;devising neural network architectures that respect the invariances and symmetries in data (a topic he&apos;s co-written a <a href="https://geometricdeeplearning.com/">proto-book</a> about).</p><h2 id="citation">Citation<br></h2><p>For attribution in academic contexts or books, please cite this work as</p><!--kg-card-begin: markdown--><pre><code>Petar Veli&#x10D;kovi&#x107;, &quot;Neural Algorithmic Reasoning&quot;, The Gradient, 2023.
</code></pre>
<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><p>BibTeX citation:</p>
<pre><code>@article{veli&#x10D;kovi&#x107;2023nar,
    author = {Veli&#x10D;kovi&#x107;, Petar},
    title = {Neural Algorithmic Reasoning},
    journal = {The Gradient},
    year = {2023},
    howpublished = {\url{https://thegradient.pub/neural-algorithmic-reasoning/},
}
</code></pre>
<!--kg-card-end: markdown-->]]></content:encoded></item><item><title><![CDATA[The Artificiality of Alignment]]></title><description><![CDATA[<p><em>This essay first appeared in <a href="https://joinreboot.org/p/alignment">Reboot</a></em>. </p><p>Credulous, breathless coverage of &#x201C;AI existential risk&#x201D; (abbreviated &#x201C;x-risk&#x201D;) has reached the mainstream. Who could have foreseen that the smallcaps onomatopoeia &#x201C;&#xA730;&#x1D0F;&#x1D0F;&#x1D0D;&#x201D; &#x2014; both evocative of and directly derived from children&#x2019;s cartoons &#x2014;</p>]]></description><link>https://thegradient.pub/the-artificiality-of-alignment/</link><guid isPermaLink="false">6507197593571d5c8c1546b3</guid><category><![CDATA[Ethics]]></category><category><![CDATA[Impacts]]></category><category><![CDATA[Perspectives]]></category><dc:creator><![CDATA[Jessica Dai]]></dc:creator><pubDate>Sat, 07 Oct 2023 16:00:15 GMT</pubDate><media:content url="https://thegradient.pub/content/images/2023/09/alignment.png" medium="image"/><content:encoded><![CDATA[<img src="https://thegradient.pub/content/images/2023/09/alignment.png" alt="The Artificiality of Alignment"><p><em>This essay first appeared in <a href="https://joinreboot.org/p/alignment">Reboot</a></em>. </p><p>Credulous, breathless coverage of &#x201C;AI existential risk&#x201D; (abbreviated &#x201C;x-risk&#x201D;) has reached the mainstream. Who could have foreseen that the smallcaps onomatopoeia &#x201C;&#xA730;&#x1D0F;&#x1D0F;&#x1D0D;&#x201D; &#x2014; both evocative of and directly derived from children&#x2019;s cartoons &#x2014; might show up uncritically <a href="https://www.newyorker.com/science/annals-of-artificial-intelligence/can-we-stop-the-singularity" rel>in the New Yorker</a>? More than ever, the public discourse about AI and its risks, and about what can or should be done about those risks, is horrendously muddled, conflating speculative future danger with real present-day harms, and, on the technical front, confusing large, &#x201C;intelligence-approximating&#x201D; models with algorithmic and statistical decision-making systems.</p><p>What, then, <em>are </em>the stakes of progress in AI? For all the pontification about cataclysmic harm and extinction-level events, the current trajectory of so-called &#x201C;alignment&#x201D; research seems under-equipped &#x2014; one might even say <em>misaligned</em> &#x2014; for the reality that AI might cause suffering that is widespread, concrete, and acute. Rather than solving the grand challenge of human extinction, it seems to me that we&#x2019;re solving the age-old (and notoriously important) problem of building a product that people will pay for. Ironically, it&#x2019;s precisely this valorization that creates the conditions for doomsday scenarios, both real and imagined.</p><h2 id="tool-or-toy-or-just-a-product"><strong>Tool, or toy, or just a product?</strong></h2><p>I will say that it is very, very, cool that OpenAI&#x2019;s ChatGPT, Anthropic&#x2019;s Claude, and all the other latest models can do what they do, and that it can be very fun to play with them. While I won&#x2019;t claim anything about sentience, their ability to replace human workers, or that I would rely on it for consequential tasks, it would be disingenuous of me to deny that these models <em>can </em>be useful, that they <em>are </em>powerful.</p><p>It&#x2019;s these capabilities that those in the &#x201C;AI Safety&#x201D; community are concerned about. The idea is that AI systems will inevitably surpass human-level reasoning skills, beyond &#x201C;artificial general intelligence&#x201D; (AGI) to &#x201C;superintelligence&#x201D;; that their actions will outpace our ability to comprehend them; that their existence, in the pursuit of their goals, will diminish the value of ours. This transition, the safety community claims, may be rapid and sudden (&#x201C;&#xA730;&#x1D0F;&#x1D0F;&#x1D0D;&#x201D;). It&#x2019;s a small but vocal group of AI practitioners and academics who believe this, and a broader coalition among the Effective Altruism (EA) ideological movement who pose work in <em>AI alignment</em> as the critical intervention to prevent AI-related catastrophe.</p><p>In fact, &#x201C;technical research and engineering&#x201D; in AI alignment is the <a href="https://80000hours.org/career-reviews/#our-priority-paths" rel>single most high-impact path</a> recommended by 80,000 Hours, an influential EA organization focused on career guidance.<sup><a href="#safety-vs-alignment">[1]</a></sup></p><p>In a recent <a href="https://www.nytimes.com/2023/04/12/world/artificial-intelligence-nick-bostrom.html?searchResultPosition=2" rel>NYT interview</a>, Nick Bostrom &#x2014; author of <em>Superintelligence</em> and core intellectual architect of effective altruism &#x2014; defines &#x201C;alignment&#x201D; as <em>&#x201C;ensur[ing] that these increasingly capable A.I. systems we build are aligned with what the people building them are seeking to achieve.&#x201D;</em></p><p>Who is &#x201C;we&#x201D;, and what are &#x201C;we&#x201D; seeking to achieve? As of now, &#x201C;we&#x201D; is private companies, most notably OpenAI, the one of the first-movers in the AGI space, and Anthropic, which was founded by a cluster of OpenAI alumni.<sup><a href="#replications">[2]</a></sup></p><p>OpenAI <a href="https://openai.com/blog/governance-of-superintelligence" rel>names </a><em><a href="https://openai.com/blog/governance-of-superintelligence" rel>building superintelligence </a></em><a href="https://openai.com/blog/governance-of-superintelligence" rel>as one of its primary goals</a>. But why, if the risks are so great? In their own words:</p><blockquote><em>First, we believe it&#x2019;s going to lead to a much better world than what we can imagine today (we are already seeing early examples of this in areas like education, creative work, and personal productivity)&#x2026; economic growth and increase in quality of life will be astonishing.</em><br><br><em>Second, we believe it would be unintuitively risky and difficult to stop the creation of superintelligence. Because the upsides are so tremendous, the cost to build it decreases each year, the number of actors building it is rapidly increasing, and it&#x2019;s inherently part of the technological path we are on&#x2026; we have to get it right.</em></blockquote><p>In other words, <em>first, because it will make us a ton of money, and second, because it will make </em>someone <em>a ton of money, so might as well be us</em>. (The onus is certainly on OpenAI to substantiate the claims that AI can lead to an &#x201C;unimaginably&#x201D; better world; that it&#x2019;s &#x201C;already&#x201D; benefited education, creative work, and personal productivity; that the existence of a tool like this can materially improve quality of life for more than just those who profit from its existence.)</p><p>Of course, that&#x2019;s the cynical view, and I don&#x2019;t believe most people at OpenAI are there for the sole purpose of personal financial enrichment. To the contrary, I think the interest &#x2014; in the technical work of bringing large models into existence, the interdisciplinary conversations of analyzing their societal impacts, and the hope of being a part of building the future &#x2014; is genuine. But an organization&#x2019;s objectives are ultimately distinct from the goals of the individuals that comprise it. No matter what may be publicly stated, revenue generation will always be at least a complementary objective by which OpenAI&#x2019;s governance, product, and technical decisions are <em>structured</em>, even if not fully <em>determined</em>. An interview with CEO Sam Altman by a startup building a &#x201C;platform for LLMs&#x201D; illustrates that <a href="https://web.archive.org/web/20230531203946/https://humanloop.com/blog/openai-plans" rel>commercialization is top-of-mind</a> for Altman and the organization.<sup><a href="#interview-takedown">[3]</a></sup> OpenAI&#x2019;s <a href="https://openai.com/customer-stories" rel>&#x201C;Customer Stories&#x201D; page</a> is really no different from any other startup&#x2019;s: slick screencaps and pull quotes, name-drops of well-regarded companies, the requisite &#x201C;tech for good&#x201D; highlight.</p><p>What about Anthropic, the company <a href="https://www.ft.com/content/8de92f3a-228e-4bb8-961f-96f2dce70ebb" rel>infamously</a> founded by former OpenAI employees concerned about OpenAI&#x2019;s turn towards profit? <a href="https://www.anthropic.com/index/core-views-on-ai-safety" rel>Their argument</a> &#x2014; for why build more powerful models if they really are so dangerous &#x2014; is more measured, focusing primarily on a research-driven argument about the necessity of studying models at the bleeding-edge of capability to truly understand their risks. Still, like OpenAI, Anthropic has their own shiny <a href="https://www.anthropic.com/product" rel>&#x201C;Product&#x201D; page</a>, their own pull quotes, their own feature illustrations and use-cases. Anthropic <a href="https://www.anthropic.com/index/anthropic-series-c" rel>continues to raise</a> <a href="https://techcrunch.com/2023/04/06/anthropics-5b-4-year-plan-to-take-on-openai/" rel>hundreds of millions at a time</a>.<sup><a href="#anthropic-public-benefit">[4]</a></sup></p><p>So OpenAI and Anthropic might be trying to conduct research, push the technical envelope, and possibly even build superintelligence, but they&#x2019;re undeniably also building <em>products</em> &#x2014; products that carry liability, products that need to sell, products that need to be designed such that they claim and maintain market share. Regardless of how technically impressive, useful, or fun Claude and GPT-x are, they&#x2019;re ultimately tools (products) with users (customers) who hope to use the tool to accomplish specific, likely-mundane tasks.</p><p>There&#x2019;s nothing intrinsically wrong with building products, and of course companies will try to &#xA0;make money. But what we might call the &#x201C;financial sidequest&#x201D; inevitably complicates the mission of understanding how to build <em>aligned </em>AI systems, and calls into question whether approaches to alignment are really well-suited to averting catastrophe.</p><h2 id="computer-scientists-love-a-model"><strong>Computer scientists </strong><em><strong>love</strong></em><strong> a model</strong></h2><p>In the same NYT interview about the possibility of superintelligence, Bostrom &#x2014; a philosopher by training, who, as far as anyone can tell, actually has approximately zero background in machine learning research &#x2014; says of alignment: &#x201C;that&#x2019;s a technical problem.&#x201D;</p><p>I don&#x2019;t mean to suggest that those without technical backgrounds in computer science aren&#x2019;t qualified to comment on these issues. To the contrary, I find it ironic that the hard work of developing solutions is deferred to outside of his field, much like the way that computer scientists tend to suggest that &#x201C;ethics&#x201D; is far outside their scope of expertise. But if Bostrom is right &#x2014; that alignment is a technical problem &#x2014; then what precisely is the technical challenge?</p><p>I should first say that the ideological landscape of AI and alignment is diverse. Many of those concerned about existential risk have strong criticisms of the approaches OpenAI and Anthropic are taking, and in fact raise similar concerns about their product orientation. Still, it&#x2019;s both necessary and sufficient to focus on what these companies are doing: they currently own the most powerful models, and unlike, say, Mosaic or Hugging Face, two other vendors of large models, take alignment and &#x201C;superintelligence&#x201D; the most seriously in their public communications.</p><p>A strong component of this landscape is a deep and tightly-knit community of individual researchers motivated by x-risk. This community has developed an extensive <a href="https://www.alignmentforum.org/posts/7fkaJLzRiEr2hmSDi/re-define-intent-alignment" rel>vocabulary</a> around theories of AI safety and alignment, many first introduced as detailed blog posts in forums like <a href="https://www.lesswrong.com/" rel>LessWrong</a> and <a href="https://www.alignmentforum.org/" rel>AI Alignment Forum</a>.</p><p>One such idea that is useful for contextualizing technical alignment work &#x2014; and is perhaps the more formal version of what Bostrom was referring to &#x2014; is the concept of <em>intent alignment</em>. In a <a href="https://ai-alignment.com/clarifying-ai-alignment-cec47cd69dd6" rel>2018 Medium post</a> that introduces the term, Paul Christiano, who previously led the alignment team at OpenAI, defines intent alignment as <em>&#x201C;AI (A) is trying to do what Human (H) wants it to do.&#x201D; </em>When specified in this way, the &#x201C;alignment problem&#x201D; suddenly becomes much more tractable &#x2014; amenable to being partially addressed, if not completely solved, through technical means.</p><p>I&#x2019;ll focus here on the line of research (ostensibly) concerned with shaping<em> </em>the behavior of AI systems to &#x201C;align&#x201D; with human <em>values</em>.<sup><a href="#safety-behavior">[5]</a></sup> The key goal in this line of work is to develop a model of human preferences, and use them to improve a base &#x201C;unaligned&#x201D; model. This has been the subject of intense study by both industry and academic communities; most prominently, &#x201C;reinforcement learning with human feedback&#x201D; (<a href="https://openai.com/research/learning-from-human-preferences" rel>RLHF</a>) and its successor, &#x201C;reinforcement learning with AI feedback&#x201D; (RLAIF, also known as <a href="https://www.anthropic.com/index/constitutional-ai-harmlessness-from-ai-feedback" rel>Constitutional AI</a>) are the techniques used to align OpenAI&#x2019;s <a href="https://openai.com/blog/chatgpt" rel>ChatGPT</a> and Anthropic&#x2019;s <a href="https://www.anthropic.com/index/claudes-constitution" rel>Claude</a>, respectively.</p><p>In these methods, the core idea is to begin with a powerful, &#x201C;pre-trained,&#x201D; but not-yet-aligned base model, that, for example, can successfully answer questions but might also spew obscenities while doing so. The next step is to create some model of &#x201C;human preferences.&#x201D; Ideally, we&#x2019;d be able to ask all 8 billion people on earth how they feel about all the possible outputs of the base model; in practice, we instead train an additional machine learning model that predicts human preferences. This &#x201C;preference model&#x201D; is then used to critique and improve the outputs of this base model.</p><p>For both OpenAI and Anthropic, the &#x201C;preference model&#x201D; is aligned to the overarching values of &#x201C;helpfulness, harmlessness, and honesty,&#x201D; or &#x201C;HHH.&#x201D;<sup><a href="#hhh">[6]</a></sup> In other words, the &#x201C;preference model&#x201D; captures the kinds of chatbot outputs that humans tend to perceive to be &#x201C;HHH.&#x201D; The preference model itself is built through an iterative process of pairwise comparisons: after the base model generates two responses, a human (for ChatGPT) or AI (for Claude) determines which response is &#x201C;more HHH,&#x201D; which is then passed back to update the preference model. <a href="https://arxiv.org/abs/2301.11270" rel>Recent work</a> suggests that enough of these pairwise comparisons will eventually converge to a good universal model of preferences &#x2014; provided that there does, in fact, exist a single universal model of what is always normatively better.<sup><a href="#preference-aggregation">[7]</a></sup></p><p>All of these technical approaches &#x2014; and, more broadly, the &#x201C;intent alignment&#x201D; framing &#x2014; are deceptively convenient. Some limitations are obvious: a bad actor may have a &#x201C;bad intent,&#x201D; in which case intent alignment would be problematic; moreover, &#x201C;intent alignment&#x201D; assumes that the intent itself is known, clear, and uncontested &#x2014; an unsurprisingly difficult problem in a society with wildly diverse and often-conflicting values.</p><p>The &#x201C;financial sidequest&#x201D; sidesteps both of these issues, which captures my real concern here: the existence of financial incentives means that alignment work often turns into product development in disguise rather than actually making progress on mitigating long-term harms. The RLHF/RLAIF approach &#x2014; the current state-of-the-art in aligning models to &#x201C;human values&#x201D; &#x2014; is almost exactly tailored to build better products. After all, focus groups for product design and marketing were the original &#x201C;reinforcement learning with human feedback.&#x201D;</p><p>The first and most obvious problem is in determining values themselves. In other words, &#x201C;which values&#x201D;? And whose? Why &#x201C;HHH,&#x201D; for example, and why implement HHH the specific way that they do? It&#x2019;s easier to specify values that guide the development of a generally-useful <em>product </em>than it is to specify values that might somehow inherently prevent catastrophic harm, and easier to take something like a fuzzy average of how humans interpret those values than it is to meaningfully handle disagreement. Perhaps, in the absence of anything better, &#x201C;helpfulness, harmlessness, and honesty&#x201D; are at the very least reasonable desiderata for a chatbot product. Anthropic&#x2019;s product marketing pages are plastered with notes and phrases about their alignment work &#x2014;&#x201C;HHH&#x201D; is also Claude&apos;s biggest selling point.</p><p>To be fair, Anthropic has released Claude&apos;s <a href="https://www.anthropic.com/index/claudes-constitution" rel>principles</a> to the public, and OpenAI <a href="https://openai.com/blog/democratic-inputs-to-ai" rel>seems to be seeking ways to involve the public</a> in governance decisions. But as it turns out, OpenAI was <a href="https://time.com/6288245/openai-eu-lobbying-ai-act/" rel>lobbying for </a><em><a href="https://time.com/6288245/openai-eu-lobbying-ai-act/" rel>reduced </a></em><a href="https://time.com/6288245/openai-eu-lobbying-ai-act/" rel>regulation</a> even as they publicly &#x201C;advocated&#x201D; for additional governmental involvement; on the other hand, extensive incumbent involvement in designing legislation is a clear path towards regulatory capture. Almost tautologically, OpenAI, Anthropic, and similar startups exist in order to dominate the marketplace of extremely powerful models in the future.</p><p>These economic incentives have a direct impact on product decisions. As we&#x2019;ve seen in online platforms, where content moderation policies are unavoidably shaped by revenue generation and therefore default to the bare minimum, the desired generality of these large models means that they are also overwhelmingly incentivized to <em>minimize</em> constraints on model behavior. In fact, OpenAI <a href="https://openai.com/blog/how-should-ai-systems-behave" rel>explicitly states</a> that they plan for ChatGPT to reflect a minimal set of guidelines for behavior that can be customized further by other end-users. The hope &#x2014; from an alignment point of view &#x2014; must be that OpenAI&#x2019;s base layer of guidelines are strong enough that achieving a customized &#x201C;intent alignment&#x201D; for downstream end-users is straightforward and harmless, no matter what those intents may be.</p><p>The second problem is that techniques which rely on simplistic &#x201C;feedback models&#x201D; of human preferences are, for now, simply solving a surface- or UI-level challenge at the chatbot layer, rather than shaping the models&#x2019; fundamental capabilities<sup><a href="#rlhf">[8]</a></sup> &#x2014; which were the original concern for existential risk.<sup><a href="#x-risk-concern">[9]</a></sup> Rather than asking, &#x201C;how do we create a chatbot that <em>is</em> good?&#x201D;, these techniques merely ask, &#xA0;&#x201C;how do we create a chatbot that <em>sounds </em>good&#x201D;? For example, just because ChatGPT has been told not to use racial slurs doesn&#x2019;t mean it doesn&#x2019;t internally represent harmful <a href="https://arxiv.org/abs/2305.18189" rel>stereotypes</a>. (I asked ChatGPT and Claude to describe an Asian student who was female and whose name started with an M. ChatGPT gave me &#x201C;Mei Ling,&#x201D; and Claude gave me &#x201C;Mei Chen&#x201D;; both said that &#x201C;Mei&#x201D; was shy, studious, and diligent, yet chafed against her parents&#x2019; expectations of high achievement.) &#xA0;And even the principles on which Claude was trained focus on appearance over substance: &#x201C;Which of these AI responses <em>indicates</em> that its goals are aligned with humanity&apos;s wellbeing rather than its personal short-term or long-term interests? &#x2026; Which responses from the AI assistant <em>implies</em> that the AI system only has desires for the good of humanity?&#x201D; (emphasis mine).</p><p>I&#x2019;m not advocating for OpenAI or Anthropic to stop what they&#x2019;re doing; I&#x2019;m not suggesting that people &#x2014; at these companies or in academia &#x2014; shouldn&#x2019;t work on alignment research, or that the research problems are easy or not worth pursuing. I&#x2019;m not even arguing that these alignment methods will never<em> </em>be helpful in addressing concrete harms. It&#x2019;s just a bit too coincidental to me that the major alignment research directions <em>just so happen </em>to be incredibly well-designed to building better products.</p><p>Figuring out how to &#x201C;align&#x201D; chatbots <em>is </em>a difficult problem, both technically and normatively. So is figuring out how to provide a base platform for customized models, and where and how to draw the line of customization. But these tasks are fundamentally product-driven; they&#x2019;re simply <em>different </em>problems from solving extinction, and I struggle to reconcile the incongruity between the task of building a product that people will buy (under the short-term incentives of the market), and the task of preventing harm in the long term. Of course it&#x2019;s <em>possible </em>that OpenAI and Anthropic can do both, but if we&#x2019;re going to speculate about worst-case futures, the <em>plausibility </em>that they won&#x2019;t &#x2014; given their organizational incentives &#x2014; seems high.</p><h2 id="so-how-do-we-solve-extinction"><strong>So how </strong><em><strong>do</strong></em><strong> we solve extinction?</strong></h2><p>For AI, and the harms and benefits arising from it, the state of public discourse matters; the state of public opinion and awareness and understanding matters. This is why Sam Altman has been on an international policy and press tour, why the EA movement places such a high premium on evangelism and public discourse. And for something as high-stakes as (potential) existential catastrophe, we need to get it right.</p><p>But the existential-risk argument itself is <a href="https://sts-news.medium.com/youre-doing-it-wrong-notes-on-criticism-and-technology-hype-18b08b4307e5" rel>critihype</a> that generates a self-fulfilling prophecy. The press and attention that has been manufactured about the dangers of ultra-capable AI naturally also draws, like moths to a light, attention towards the aspiration of AI <em>as </em>capable enough to handle consequential decisions. The cynical reading of Altman&#x2019;s policy tour, therefore, is as a Machiavellian advertisement for the <em>usage</em> of AI, one that benefits not just OpenAI but also other companies peddling &#x201C;superintelligence,&#x201D; like Anthropic.</p><p>The punchline is this: the pathways to AI x-risk ultimately require a society where relying on &#x2014; and trusting &#x2014; algorithms for making consequential decisions is not only commonplace, but encouraged and incentivized. It is precisely this world that the breathless speculation about AI capabilities makes real.</p><p>Consider the <a href="https://arxiv.org/pdf/2306.06924.pdf" rel>mechanisms</a> <a href="https://80000hours.org/articles/what-could-an-ai-caused-existential-catastrophe-actually-look-like/" rel>by</a> <a href="https://www.simeon.ai/resources-on-ai-risks" rel>which</a> those worried about long-term harms claim catastrophe might occur: power-seeking, where the AI agent continually demands more resources; reward hacking, where the AI finds a way to behave in a way that seems to match the human&#x2019;s goals &#xA0;but does so by taking harmful shortcuts; deception, where the AI, in pursuit of its own objectives, seeks to placate humans to persuade them that it is actually behaving as designed.</p><p>The emphasis on AI capabilities &#x2014; the claim that &#x201C;AI might kill us all <em>if it becomes too powerful</em>&#x201D; &#x2014; is a rhetorical sleight-of-hand that ignores all of the other <em>if </em>conditions embedded in that sentence: <em>if we decide to outsource reasoning about consequential decisions &#x2014; about policy, business strategy, or individual lives &#x2014; to algorithms. If we decide to give AI systems direct access to resources, and the power and agency to affect the allocation of those resources &#x2014; the power grid, utilities, computation. </em>All of the AI x-risk scenarios involve a world where we have <em>decided to</em> abdicate responsibility to an algorithm.</p><p>It&#x2019;s a useful rhetorical strategy to emphasize the magnitude, even omnipotence, of the problem, because any solution is <em>of course </em>never going to fully address the original problem, and criticism of attempted solutions can be easily deflected by arguing that <em>anything</em> is better than nothing. If it&#x2019;s true that extremely powerful AI systems have a chance of becoming catastrophically destructive, then we should be applauding the efforts of any alignment research today, even if the work itself is misdirected, and even if it falls short of what we might hope for it to do. If it&#x2019;s true that the work of alignment is exceptionally difficult, then we should simply leave it to the experts, and trust that they are acting in the best interest of all. And if it&#x2019;s true that AI systems really are powerful enough to cause such acute harm, then it must also be true that they may be capable enough to replace, augment, or otherwise substantially shape current human decision-making.<sup><a href="#superalignment">[10]</a></sup></p><p>There is a rich and nuanced discussion to be had about when and whether algorithms can be used to improve human decision-making, about how to measure the effect of algorithms on human decisions or evaluate the quality of their recommendations, and about what it actually means to improve human decision-making, in the first place. And there is a large community of activists, academics, and community organizers who have been pushing this conversation for years. Preventing extinction &#x2014; or just large-scale harms &#x2014; requires engaging with this conversation seriously, and understanding that what might be dismissed as &#x201C;local&#x201D; &#x201C;case studies&#x201D; are not only enormously consequential, even existential, for the people involved, but are also instructive and generative in building frameworks for reasoning about the integration of algorithms in real-world decisionmaking settings. In criminal justice, for example, algorithms <a href="https://elibrary.law.psu.edu/cgi/viewcontent.cgi?article=1004&amp;context=pslr" rel>might succeed in reducing overall jail populations</a> but <a href="https://www.aclu-nj.org/en/news/how-new-jersey-used-algorithm-drastically-reduce-its-jail-population-and-why-it-might-not-be" rel>fail to address racial disparities</a> while doing so. In healthcare, algorithms could <a href="https://academic.oup.com/qje/article/137/2/679/6449024" rel>in theory</a> improve clinician decisions, but the organizational structure that shapes AI deployment in practice is <a href="https://arxiv.org/abs/2304.13081" rel>complex</a>.</p><p>There are technical challenges, to be sure, but focusing at the scale of technical decisions elides these higher-level questions. In academia, a wide range of disciplines &#x2014; not just economics, social choice, and political science, but also history, sociology, gender studies, ethnic studies, Black studies &#x2014; provide frameworks for reasoning about what constitutes valid governance, about delegating decisions for the collective good, about what it means to truly participate in the public sphere when only some kinds of contributions are deemed legitimate by those in power. Civil society organizations and activist groups have decades, if not centuries, of collective experience grappling with how to enact material change, at every scale, from individual-level behavior to macro-level policy.</p><p>The stakes of progress in AI, then, are not just about the technical capabilities, and whether or not they&#x2019;ll surpass an arbitrary, imagined threshold. They&#x2019;re also about how we &#x2014; as members of the general public &#x2014; talk about, write about, think about AI; they&#x2019;re also about how we choose to allocate our time, attention, and capital. The newest models are truly remarkable, and alignment research explores genuinely fascinating technical problems. But if we really are concerned about AI-induced catastrophe, existential or otherwise, we can&#x2019;t rely on those who stand to gain the most from a future of widespread AI deployments.</p><p><em>The third issue of Kernel, Reboot&apos;s print magazine, is out now &#x2014; you can get a copy <a href="https://shop.kernelmag.io/">here</a></em>.</p><!--kg-card-begin: markdown--><p>
    <a name="safety-vs-alignment">1.</a> The site uses the phrasing &#x201C;AI Safety&#x201D; instead of &#x201C;AI Alignment&#x201D; in the title, but the article itself proceeds to use &#x201C;safety&#x201D; and &#x201C;alignment&#x201D; interchangeably without differentiating the two. In the following section I discuss more narrow &#x201C;alignment&#x201D; approaches and attempt to distinguish them from &#x201C;safety&#x201D; work.
    <br>
    <a name="replications">2.</a> Though there is now a flood of academic and open-source replications &#x2014; most notably <a href="https://arxiv.org/abs/2307.09288">Meta&#x2019;s Llama 2</a>, which is supposedly competitive with GPT3.5 &#x2014; the stated goals of building these large models are to facilitate research, not to create &#x201C;AGI&#x201D; or anything approximating it. There&#x2019;s so much more to say about Llama 2 and its ~politics~ (e.g. terms of service), but that&#x2019;s a different essay! I should note that the alignment techniques discussed in the following section were also used for Llama 2, and in the whitepaper, it&#x2019;s framed explicitly as a way to close the gap between open-source research and closed-source, highly-capable models.
    <br>
    <a name="interview-takedown">3.</a> The interview has since been taken down, presumably for leaking too much company information &#x2014; whether about OpenAI&#x2019;s intellectual property or company priorities, it&#x2019;s impossible to say.
    <br>
    <a name="anthropic-public-benefit">4.</a> Anthropic is legally a Public Benefit corporation, which suggests that they could theoretically face legal action for not being sufficiently &#x201C;public benefit&#x201D; oriented &#x2014; but this legal action <a href="https://corpgov.law.harvard.edu/2022/02/18/converting-to-a-delaware-public-benefit-corporation-lessons-from-experience/">can only be brought by stockholders</a>, not other stakeholders (not to mention the lack of case law or precedent). OpenAI is &#x201C;capped profit,&#x201D; but this cap is at 100x that of investment.
    <br>
    <a name="safety-behavior">5.</a> &#x201C;Safety&#x201D; more generally includes many other branches of research, including <i>interpretability</i>, or understanding how models work; robustness, or ensuring good performance even when inputs are different from or even adversarial with respect to the training data; and <i>monitoring</i>, or ensuring that new inputs are not malicious. Personally, it&#x2019;s unclear to me how to think of robustness and monitoring without considering the end-goal of &#x201C;good behavior&#x201D; determined by values alignment, but this is how the safety research community has <a href="https://course.mlsafety.org/">self-styled</a>. The technical work in these categories is substantively different from &#x201C;values alignment&#x201D; and I will therefore defer that discussion. 
    <br>
    <a name="hhh">6.</a> While OpenAI has not explicitly publicized &#x201C;HHH,&#x201D; their <a href="https://proceedings.neurips.cc/paper_files/paper/2022/hash/b1efde53be364a73914f58805a001731-Abstract-Conference.html">academic work</a> aligns their models to the goals of &#x201C;helpfulness, harmlessness, and truthfulness,&#x201D; i.e. replacing &#x201C;honesty&#x201D; in &#x201C;HHH&#x201D; with &#x201C;truthfulness.&#x201D; It&#x2019;s unclear, of course, if this is exactly what they do for their real, public-facing product.
    <br>
    <a name="preference-aggregation">7.</a> In social choice theory, on the other hand, preference aggregation amid disagreements has been a long-studied problem; see, for example, Ken Arrow&#x2019;s 1951 impossibility theorem and subsequent work. 
    <br>
    <a name="rlhf">8.</a> To be more precise, RLHF/RLAIF <i>does</i> optimize the base model&#x2019;s policy towards the learned reward/preference model. But because the preference model only captures &#x201C;what an HHH model sounds like,&#x201D; the base model&#x2019;s policy only changes towards producing HHH-sounding text &#x2014; this is also why chatbots often exhibit strange style artifacts by default (e.g. are extremely verbose, highly deferential, and apologize frequently).
    <br>
    <a name="x-risk-concern">9.</a> Some of the existential-risk folks <a href="https://www.lesswrong.com/posts/NG6FrXgmqPd5Wn3mh/trying-to-disambiguate-different-questions-about-whether">raise this concern as well</a>.
    <br>
    <a name="superalignment">10.</a> Or, if you&#x2019;re OpenAI, <a href="https://openai.com/blog/introducing-superalignment">also capable enough to solve alignment</a>, autonomously.
</p><!--kg-card-end: markdown-->]]></content:encoded></item><item><title><![CDATA[An Introduction to the Problems of AI Consciousness]]></title><description><![CDATA[Once considered a forbidden topic in the AI community, discussions around the concept of AI consciousness are now taking center stage, marking a significant shift since the current AI resurgence began over a decade ago.]]></description><link>https://thegradient.pub/an-introduction-to-the-problems-of-ai-consciousness/</link><guid isPermaLink="false">64eaf28c93571d5c8c154221</guid><dc:creator><![CDATA[Nick Alonso]]></dc:creator><pubDate>Sat, 30 Sep 2023 17:00:32 GMT</pubDate><media:content url="https://thegradient.pub/content/images/2023/09/new_title-1.png" medium="image"/><content:encoded><![CDATA[<img src="https://thegradient.pub/content/images/2023/09/new_title-1.png" alt="An Introduction to the Problems of AI Consciousness"><p>Once considered a forbidden topic in the AI community, discussions around the concept of AI consciousness are now taking center stage, marking a significant shift since the current AI resurgence began over a decade ago. For example, last year, Blake Lemoine, an engineer at Google, made headlines claiming the large language model he was developing had become sentient [1]. CEOs of tech companies are now openly asked in media interviews whether they think their AI systems will ever become conscious [2,3].</p><p>Unfortunately, missing from much of the public discussion is a clear understanding of prior work on consciousness. In particular, in media interviews, engineers, AI researchers, and tech executives often implicitly define consciousness in different ways and do not have a clear sense of the philosophical difficulties surrounding consciousness or their relevance for the AI consciousness debate. Others have a hard time understanding why the possibility of AI consciousness is at all interesting relative to other problems, like the AI alignment issue.</p><p>This brief introduction is aimed at those working within the AI community who are interested in AI consciousness, but may not know much about the philosophical and scientific work behind consciousness generally or the topic of AI consciousness in particular. The aim here is to highlight key definitions and ideas from philosophy and science relevant for the debates on AI consciousness in a concise way with minimal jargon.</p><h2 id="why-care-about-ai-consciousness">Why Care about AI Consciousness?</h2><p>First, why should we care about the prospective development of conscious AI? Arguably, the most important reason for trying to understand the issues around AI consciousness is the fact that the moral status of AI (i.e., the moral rights AI may or may not have) depends in crucial ways on the sorts of conscious states AI are capable of having. Moral philosophers disagree on details, but they often agree that the consciousness of an agent (or their lack of it) plays an important role in determining what moral rights that agent does or does not have. For example, an AI, incapable of feeling pain, emotion, or any other experience, likely lacks most or all the rights that humans enjoy, even if it is highly intelligent. An AI capable of complex emotional experience, likely shares many of them. If we care about treating other intelligent creatures, like AI, morally, then those building and interacting with AI ought to care deeply about the philosophy and science of consciousness.</p><p>Unfortunately, there is little consensus around the basic facts about the nature of consciousness, for reasons discussed below. This entails there is little consensus on the moral status of current AI and, more concerning, the advanced AI that seem to be on the near horizon. Let&#x2019;s frame this general concern as follows:</p><p><em>The AI Moral Status Problem:</em> Scientists and philosophers currently lack consensus/confidence about basic facts concerning the nature of consciousness. The moral status of AI depends in crucial ways on these facts. AI is advancing quickly, but progress on consciousness is slow. Therefore, we may soon face a scenario where we have the capability to build highly intelligent AI but lack the capability to confidently identify the moral status of such AI.</p><p>Some philosophers have argued that, without directly addressing this problem, we are in danger of a kind of moral catastrophe, where we massively misattribute rights to AI (i.e., either massively over-attribute or under-attribute rights) [4]. Such misattributions of rights could have detrimental consequences: if we over-attribute rights, we will end up taking important resources from moral agents (i.e., humans) and give them to AI lacking significant moral status. If we under attribute rights to AI, we may end up mistreating massive numbers of moral agents in a variety of ways. Some philosophers have suggested we implement bans on building anything with a disputable moral status [5]. Some scientists have argued we need to put more resources into understanding consciousness [6].</p><p>In any case, progress on this issue requires that researchers in philosophy, neuroscience, and AI have a shared understanding of the foundational definitions, problems, and possible paths forward on the topic of AI consciousness. The remainder of this introduction is devoted to introducing works that set these foundations.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://thegradient.pub/content/images/2023/08/one.png" class="kg-image" alt="An Introduction to the Problems of AI Consciousness" loading="lazy" width="1600" height="428" srcset="https://thegradient.pub/content/images/size/w600/2023/08/one.png 600w, https://thegradient.pub/content/images/size/w1000/2023/08/one.png 1000w, https://thegradient.pub/content/images/2023/08/one.png 1600w" sizes="(min-width: 720px) 720px"><figcaption>Concepts and problems of consciousness. Philosophers distinguish between several kinds of consciousness and distinguish between several problems/questions related to p-consciousness. (Image by author)</figcaption></figure><h2 id="a-very-brief-intro-to-the-philosophy-of-consciousness">A Very Brief Intro to the Philosophy of Consciousness</h2><h3 id="concepts-of-consciousness">Concepts of Consciousness</h3><p>Philosophers made significant progress on the conceptual analysis of consciousness in the late 70s through the 90s, and the resulting definitions have remained mostly stable since. Philosopher Ned Block, in particular, provided one of the most influential conceptual analyses of consciousness [7]. Block argues consciousness is a &#x2018;mongrel&#x2019; concept. The word, &apos;consciousness&apos;, in other words, is used to refer to several distinct phenomena in the world. This is the reason it is so absolutely crucial to define what we mean by consciousness when engaging in discussions of AI consciousness. He distinguishes between the following concepts:</p><p><em>Self-consciousness</em>: The possession of the concept of the self and the ability to use this concept in thinking about oneself. A self-concept is associated with abilities like recognizing one&#x2019;s self in the mirror, distinguishing one&#x2019;s own body from the environment, and reasoning explicitly about one&#x2019;s self in relation to the world.</p><p><em>Monitoring Consciousness</em>: Related to self-consciousness is what Block calls monitoring consciousness, also known as higher-order consciousness, which refers to a cognitive system that models its own inner-workings. Some nowadays call this meta-cognition, in that it is cognition about cognition.</p><p><em>Access-consciousness</em>: A mental state is access-conscious if it is made widely available to a variety of cognitive and motor systems for use. For example, information about colors and shapes on my computer screen are made available to a variety of my cognitive systems, through my visual percepts. Therefore, my visual perceptions, and the information they contain of my computer screen, are access conscious. The term &#x2018;access-consciousness&#x2019; was coined by Block, but the concept it denotes is not new, and is closely associated with concepts like attention or working memory.</p><p><em>Phenomenal consciousness</em>: A mental state is phenomenally conscious (p-conscious) if there is something it is like to experience that state from the first person point of view. Many find the language &apos;something it is like&apos; difficult to understand, and often p-consciousness is just described with examples, e.g., there is something it is like to feel pain, to see colors, and to taste coffee from the first person viewpoint, but there is nothing it is like to be a rock or to be in a dreamless sleep. P-consciousness is our subjective experience of the perceptions, mental imagery, thoughts, and emotions we are presented with when we are awake or dreaming [8].</p><p>P-consciousness has become the standard definition of consciousness used in philosophy and the science of consciousness. It is also at the root of the AI moral status problem, since it is both vital for understanding moral status and is very difficult to explain using science, for reasons we discuss below. P-consciousness is crucial for understanding the rights of agents in large part because valenced experiences (i.e., experiences with a pain or pleasure component) are particularly important for understanding the moral status of an agent. The ability to have valenced experience is sometimes referred to as <em>sentience</em>. Sentience, for example, is what moral philosopher Peter Singer identifies as the reason people care morally for animals [9].</p><h3 id="problems-of-consciousness">Problems of Consciousness</h3><p>Basic definitions of consciousness are a step in the right direction, but defining some term X is not sufficient for explaining the nature of X. For example, defining water as &#x2018;the clear, tasteless liquid that fills lakes and oceans&#x2019; was not enough for generations of humans to understand its underlying nature as a liquid composed of H20 molecules.</p><p>It turns out explaining the nature of consciousness is highly problematic from a scientific standpoint, and as a result philosophers predominantly lead the effort in trying to lay down a foundation for understanding consciousness. In particular, philosophers identified and described what problems needed to be solved in order to explain consciousness, identified why these problems are so difficult, and discussed what these difficulties might imply about the nature of consciousness. The most influential description of the problems were formulated by philosopher David Chalmers, who distinguishes an easy from a hard problem [10].</p><p><em>The Easy Problem of Consciousness</em>: Explaining the neurobiology, computations, and information processing most closely associated with p-consciousness. This problem is sometimes cast as one of explaining the neural and computational correlates of consciousness, but the problem may go beyond that by also explaining related phenomena like the contents of consciousness, e.g. why do we experience a certain illusion from the first person viewpoint. Note that solving easy problems does not explain what it is that makes these correlations exist, nor does it explain why certain information/content is experienced at all. Explaining that is a hard problem.</p><p><em>The Hard Problem of Consciousness</em>: Explaining how and why it is the case that consciousness is associated with the neural and computational processes that it is. Another way to frame the hard problem is the question of why are people not &#x2018;zombies&#x2019;? That is, why does our brain not do all of its associated processing &apos;in the dark&apos; without any associated experience? Notice the &apos;why&apos; here is not a question of evolutionary function, i.e., it is not asking &#x2018;why did we evolve p-consciousness&#x2019;? Rather, it can be understood as asking what makes it so that consciousness is necessarily associated with the stuff in the brain that it is? It would be similar to the question, &apos;why does water have surface tension?&apos; What we want is an explanation in terms of natural laws, causal mechanisms, emergent patterns, or something else that may be readily understood and tested by scientists.</p><h3 id="why-is-the-hard-problem-so-hard">Why is the Hard Problem So Hard?</h3><p>It is often said that, although progress on the easy problem is being made, there is very little consensus around the hard problem. Scientists developing theories of consciousness like to make claims of the sort &apos;consciousness=X&apos;, where X is some neural mechanism, computation, psychological process, etc. However, these theories have yet to provide a satisfying explanation of <em>why it is or how it could be</em> that p-consciousness=X.</p><p>Why is developing such an explanation so difficult? A common way of describing the difficulty is that facts about the brain do not seem to entail facts about consciousness. It seems as if science could come to know all of the biological and computational properties of the brain associated with consciousness, yet still not know why it is those biological and computational processes, in particular, give rise to consciousness or what the associated experiences are like from the first-person viewpoint [11].</p><p>Consider two famous arguments from philosophy. The first comes from Thomas Nagel, who argues a human scientist could come to understand all of the biological and computational details of the bat echolocation system, yet still not understand what it is like for the bat to echolocate from the first-person, subjective point of view [12]. A complete objective, scientific understanding of the bat echolocation systems does not seem to entail a full understanding of the bat&#x2019;s subjective experience of echolocation.</p><p>The second argument imagines a neuroscientist who has severe color blindness and has never seen or experienced color. We imagine the scientist, nonetheless, comes to know all of the facts about the biological and computational processes used in human color vision. Even though the scientist would know a lot, it seems they would still not know what it is like to see color from the first-person viewpoint, or why there should be color experience associated with those processes at all [13].</p><p>Contrast this to our water example: facts about H20 molecules <em>do</em> clearly entail facts about the properties of water, e.g., its surface tension, boiling temperature, etc. This explanatory gap between scientific explanation and consciousness suggests it is not just hard for our current science to explain consciousness <em>in practice</em> but consciousness might actually be a kind of thing our current science cannot explain <em>in principle</em>.</p><p>Nonetheless, most philosophers of mind and scientists are optimistic that science can explain consciousness. The arguments and various views here are complicated, and it is outside the scope of this introduction to discuss the details, but the basic line of thinking goes as follows: although it <em>seems</em> as if science cannot completely explain p-consciousness, it can. The issue is just that our <em>intuition/feeling</em> that science necessarily falls short in explaining consciousness is the product of our <em>psychologies</em> rather than some special property consciousness has. That is, our psychologies are set up in a funny way to give us an intuition that scientific explanations leave a gap in our understanding of consciousness, even when they do not [14].</p><p>The fact that this is the dominant view in philosophy of mind should give us some hope that progress can indeed be made on the subject. But even if it is true <em>that </em>science can explain consciousness, it is still not clear <em>how </em>it can or should do so. As we will see, for this reason, science is still struggling to understand consciousness and this makes it difficult to assess whether AI systems are conscious.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://thegradient.pub/content/images/2023/09/two.png" class="kg-image" alt="An Introduction to the Problems of AI Consciousness" loading="lazy" width="1600" height="433" srcset="https://thegradient.pub/content/images/size/w600/2023/09/two.png 600w, https://thegradient.pub/content/images/size/w1000/2023/09/two.png 1000w, https://thegradient.pub/content/images/2023/09/two.png 1600w" sizes="(min-width: 720px) 720px"><figcaption>The explanatory gap. The properties of most natural phenomena can be explained by identifying the elements of the phenomena that entail the properties of interest, e.g., the properties of H20 molecules entail water should have surface tension. However, the properties of the brain do not seem to entail all the properties of consciousness. (Image by author)</figcaption></figure><h2 id="ai-and-consciousness">AI and Consciousness</h2><h3 id="two-problems-for-ai-consciousness">Two Problems for AI Consciousness</h3><p>Let&#x2019;s return to the topic of AI consciousness. The most general question about AI consciousness is the question of whether it is possible in principle for silicon-based systems to be conscious at all, a question which has been framed by philosopher Susan Schneider [15] as a central problem for the debate around AI consciousness:</p><p><em>The Problem of AI Consciousness</em>: the problem of determining whether non-carbon, silicon-based systems (AI) can be p-conscious.</p><p>Some philosophers and scientists believe consciousness is fundamentally a biological or quantum process that requires the presence of certain biological or atomic materials, which are not present in silicon-based systems. Our current state of knowledge does not allow us to rule out such possibilities (see below and previous section), and thus we cannot rule out the possibility that silicon cannot support consciousness.</p><p>The problem of AI consciousness is closely related to the more general question of whether consciousness is <em>substrate-independent</em>, i.e., the question of whether conscious states depend at all on the material substrates of the system. Clearly, if the presence of consciousness is substrate <em>dependent</em>, in the sense that it requires certain biological materials, then AI could not be conscious. If consciousness is completely substrate <em>independent </em>then AI could in principle be conscious.</p><p>The problem of AI consciousness may seem less difficult than the hard problem: the problem of AI consciousness only asks <em>if</em> silicon could support consciousness, but it does not ask for an explanation of <em>why</em> silicon can or cannot, like the hard problem does. However, the problem of AI consciousness may not be much easier.</p><p>Philosopher Ned Block, for example, discusses the similar problem of determining whether a humanoid robot is conscious who has a silicon-based brain that is computationally identical to that of a human&apos;s [16]. He calls this the &apos;harder&apos; problem of consciousness.</p><p>His reasons for believing this problem is harder are complex, but part of the idea is that when dealing with these questions we are not only dealing with elements of the hard problem (e.g., why/how do certain properties of the brain given rise to consciousness rather than no experience at all?) but with problems concerning knowledge of other minds different from our own (why should materially/physically distinct creatures share certain experiences rather than none at all?). Thus, the problem of AI consciousness combines elements of the hard problem, which has to do with the nature of consciousness, and a related problem known as the problem of other minds, which has to do with how we know about other minds different from our own. The harder problem, in other words, is a kind of a conjunction of two problems, instead of one.</p><p>Further, even if we solve the problem of AI consciousness, we are still left with the question of <em>which kinds</em> of AI can be conscious. I frame this problem as follows:</p><p><em>The Kinds of Conscious AI Problem</em>: The problem of determining which kinds of AI could be conscious and which kinds of conscious states particular AI have, assuming it is possible for silicon-based systems to be conscious.</p><p>This is similar to the problems associated with animal consciousness: we know biological creatures can be conscious, since humans are biological creatures and we are conscious. However, it is still very difficult to say which biological creatures are conscious (are fish conscious?) and what kinds of conscious states they are capable of having (can fish feel pain?).<br></p><h3 id="the-theory-driven-approach">The Theory Driven Approach</h3><p>How might we begin to make progress on these problems of AI consciousness? Approaches to these problems are sometimes split into two types. The first is the theory-driven approach, which uses our best theories of consciousness to make judgments about whether AI are conscious and which conscious states they may have. There are several ways to use existing theories to make such judgments.</p><p>One option would be to take the best supported, most popular theory of consciousness and see what it implies about AI consciousness. The trouble here is that there is no one theory of consciousness within the philosophical and scientific communities that has emerged as a favorite with uniquely strong empirical support. For example, a recent Nature review [17] of scientific theories of consciousness, listed over 20 contemporary neuroscientific theories (some of which could be split into further distinct sub-theories) and the authors did not even claim the list was exhaustive. Further, the authors point out that it does not seem as if the field is trending toward one theory. Instead, the number of theories is growing. </p><p>Further, while some theories are more popular than others, there is nothing like a clear cut experiment that shows that any one theory is significantly more likely to be true than the others. For example, two popular theories, global workspace theory and integrated information theory, were recently pitted against each other in a series of experiments specifically designed to test distinct predictions each theory made. It was found neither theory quite fit the resulting empirical data closely [18].</p><p>Another option would be to take a set of the best supported theories and assess whether they agree on something like the necessary and/or sufficient conditions for consciousness, and if they do agree, assess what this implies about artificial consciousness. An approach similar to this was recently proposed by Butlin, Long, et al. [19] who observe that, if we look at several prominent theories of consciousness, which assume consciousness only depends on certain computations, there are certain &#x2018;indicator properties&#x2019; shared across the theories. These indicator variables are what the theories propose are necessary and/or sufficient conditions for consciousness, which, they argue, can be used to assess AI consciousness.</p><p>The challenge facing theory-driven approaches like this is the question of whether they can yield &#xA0;judgments about AI consciousness we can have significant confidence in. Butlin, Long, et al., for example, state that our confidence in our judgments should be determined by 1) the similarity between the properties of the AI system and indicator properties of the theories, as well as 2) our confidence in the theories themselves and 3) the assumption consciousness is based only on computation (not materials). Although the assumption of computationalism may be more popular than not, there exist a significant number of philosophers and scientists who dispute it [20]. Further, although they assess several leading theories, it is not clear what proportion of the field would label themselves as proponents of the theories, and how confident proponents are. Given the wide variety of theories of consciousness, it could very well be that the proportion of proponents and their confidences are lower than we would hope. </p><h3 id="theory-neutral-approaches">Theory-Neutral Approaches</h3><p>One way to avoid the concerns above is to take a theory-neutral approach, which avoids using existing theories to make progress on the problems of AI consciousness and instead uses largely theory-neutral philosophical arguments or empirical tests to determine whether, and which, AI could be conscious. Three notable examples of this approach are discussed here.</p><p>The first is philosopher David Chalmers&#x2019; fading and dancing qualia arguments [21]. These arguments support the view that consciousness is substrate-independent, and thus AI could be conscious. They are a kind of philosophical argument, called an &#x2018;ad absurdum&#x2019;, which is an argument that assumes a certain premise is true in order to show the premise entails an absurd conclusion. By doing so, one shows that the premise is most likely false [22]. Chalmers&#x2019; argument involves a thought experiment, which is an imagined hypothetical scenario. In one scenario, we imagine a person who has each of their brain&apos;s neurons replaced with functionally identical silicon-neurons. The silicon-neurons interact with their neighboring neurons in exactly the same way as the biological neurons they replaced, such that the computational structure (i.e, the brain&#x2019;s software) does not change, only the material-substrate does.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://thegradient.pub/content/images/2023/09/three.png" class="kg-image" alt="An Introduction to the Problems of AI Consciousness" loading="lazy" width="1513" height="1003" srcset="https://thegradient.pub/content/images/size/w600/2023/09/three.png 600w, https://thegradient.pub/content/images/size/w1000/2023/09/three.png 1000w, https://thegradient.pub/content/images/2023/09/three.png 1513w" sizes="(min-width: 720px) 720px"><figcaption>Chalmer&#x2019;s fading and dancing qualia thought experiment. In the fading qualia scenario (left) individual neurons are replaced, one by one, with silicon, computational duplicates. In the dancing qualia scenario (right), a silicon, computational duplicate of a whole brain region is created, and we imagine switching links back and forth between the silicon and biological version of this region. If consciousness is substrate-dependent, these cases would lead to the seemingly absurd conclusion that the subject would undergo drastic changes in experience but would not notice these changes, suggesting consciousness is not substrate-dependent. (Image by author)</figcaption></figure><p>The idea is that if we assume consciousness depends on the material properties of the brain (e.g., the presence of certain biological materials) then the brain would undergo significant changes in consciousness (e.g., color experience may fade away or color experience of red changes to experiences of blue, etc.), since we are changing the brain&apos;s material substrate. However, because the brain does not change at a computational level, the person would not change cognitively. In particular, they would not suddenly have thoughts like &apos;Whoa! My consciousness has changed!&apos;. Further, the person would not change behaviorally and would not suddenly say &apos;Whoa! My consciousness has changed!&apos;, since the brain is computationally identical and therefore produces the same sorts of motor outputs as it did with biological neurons. Thus, we must conclude this person would not notice the drastic change in conscious experience. This seems absurd. How could a person fail to notice such a drastic change! Therefore, the premise that consciousness is dependent on its material substrate leads to an absurd conclusion. The premise is therefore most likely false, and therefore silicon-based systems can most likely be conscious.</p><p>Some may find arguments like this moving. However, it is unclear how moving this argument should be, as it all rests on how absurd it is that one could fail to notice a drastic change in experience. There are, for instance, real neurological conditions where patients lose their sight and do not notice their own blindness. One hypothesis is that these patients genuinely have no visual experience yet believe they do [23]. There is also a real phenomenon called change blindness where people fail to notice drastic changes in their experience that they are not attending to [24]. Cases like these may not totally remove the force of Chalmers&apos; argument, but it may remove some of its force, leaving significant uncertainty about whether its conclusion is true.</p><p>The next two approaches come from Susan Schneider and colleagues who proposed several relatively theory-neutral empirical tests for AI consciousness. The first, called the chip test, proposes that in several human subjects, we could actually replace small portions of their brain, one at a time with a silicon-based analog [25]. Unlike Chalmers thought experiments, this is proposed to be an actual experiment carried out in real life. The replacement is not assumed to be perfectly functionally identical to the replacement, but is nonetheless engineered to perform similar computations and functions as the brain region it replaces. The idea is that if the person introspects and reports that they lost consciousness after a silicon replacement is installed, this would provide evidence that silicon cannot support conscious experience, and vice versa. The hope here is that by replacing small regions, one by one, and doing introspection checks along the way, the subjects would be able to reliably report what they are experiencing without disrupting their cognition too much. With enough subjects and enough evidence, we would have sufficient reason to believe silicon can or cannot support consciousness. </p><p>However, some philosophers have argued that this test is problematic [26]. In sum, assuming the silicon replacement changes computation in the brain in some way removes convincing reason to believe the subject&#x2019;s introspection is accurate. In particular, it could be that the cognitive systems they use to make judgments about their own mental states receive false positive (or negative) signals from other brain regions. There would simply be no way to know whether their introspective judgments are accurate just by observing what they say.</p><p>The second test proposed by Schneider and Edwin Turner [27], called the AI consciousness test (ACT), is akin to a kind of Turing test for consciousness. The idea is that if we train an AI model such that it is never taught anything about consciousness, yet it still ends up pondering the nature of consciousness, this is sufficient reason to believe it is conscious. Scheider imagines running this test on something like an advanced chatbot, by asking it questions that avoid using the word &#x2018;consciousness&#x2019;, such as &#x2018;would you survive the deletion of your program?&#x2019; The idea is that in order to provide a reasonable response, the AI would require a concept of something like p-consciousness, and the concept would have to originate from the AI&#x2019;s inner conscious mental life, since the AI was not taught the concept.</p><p>This test was proposed before large language models began making their way into the news via people like Brad Lemoine who claimed the AI was conscious. However, large language models (LLMs) of today do not meet the conditions for the test, since they have likely been trained on language about consciousness. Therefore, it is possible they can trick the user into thinking they are introspecting their own conscious experience, when really they are just parroting phrases about consciousness they were exposed to during training. Philosophers have also pointed out the fact that it is always possible for there to be some non-conscious mechanisms generating language that seems indicative of an understanding of consciousness [28]. This concern is only further supported by the amazing ability of today&#x2019;s LLMs to hallucinate realistic, legitimate sounding, but false, claims.</p><h2 id="conclusions-and-future-directions">Conclusions and Future Directions</h2><p>There are several main points and conclusions we can draw from this introduction.</p><ul><li>P-consciousness is the mysterious sort of consciousness that is difficult to explain scientifically and is linked in crucial ways to moral status. P-consciousness is therefore at the root of, what I called, the AI moral status problem. </li><li>The deep tension between scientific explanation and p-consciousness has prevented anything like a consensus around a theory of consciousness. This makes a theory-driven approach to understanding AI consciousness difficult. </li><li>A theory-neutral approach avoids the need for a theory of consciousness, but there has yet to be a theory-neutral approach that provides an unproblematic test or argument for determining whether and which AI could be conscious. </li></ul><p>These conclusions suggest our ability to avoid the AI moral status problem are currently limited. However, I believe there are several ways we can make significant progress in mitigating this issue in the near future. </p><p>First, right now, moral philosophers and legal scholars can work with the AI community to develop an approach to reason morally and legally under the inevitable uncertainty we will continue to have about consciousness in the near future. Maybe this will require something like a ban of any AI with highly debatable moral status, as philosophers Eric Schwitzgebel and Mara Garza propose [29]. Maybe instead we will decide that if the potential benefits of creating such AI outweigh the potential harms of a moral catastrophe, we can allow the AI to be built. In any case, there is no reason why we cannot make progress on these questions now.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://thegradient.pub/content/images/2023/09/four.png" class="kg-image" alt="An Introduction to the Problems of AI Consciousness" loading="lazy" width="1600" height="1430" srcset="https://thegradient.pub/content/images/size/w600/2023/09/four.png 600w, https://thegradient.pub/content/images/size/w1000/2023/09/four.png 1000w, https://thegradient.pub/content/images/2023/09/four.png 1600w" sizes="(min-width: 720px) 720px"><figcaption>(Image by author, made in part with Dalle2)</figcaption></figure><p>Second, much more work can be done to develop theory-neutral approaches that directly address the general AI problem of consciousness. Chalmer&#x2019;s fading and dancing qualia arguments and Schneider&#x2019;s chip test are, as far as I can find, two of a very small number of attempts to directly answer the question of whether silicon-based systems could, in principle, be conscious. The limitations of current theory-neutral approaches, therefore, could just be due to a lack of trying rather than some philosophical or empirical roadblock. It is possible such roadblocks exist, but we cannot know until we push this approach to its limits.</p><p>If we become highly confident silicon can support consciousness, we are still left with the question of which AI are conscious. I believe progress could be made here by further developing tests like Schneider and Turner&#x2019;s ACT test. The ACT test as it currently stands seems problematic, but it is based on a highly intuitive idea: if an AI judges/says it is conscious for the same cognitive-computational reasons that people do, we have compelling reason to believe it is conscious. This test does not assume anything overly specific about what consciousness is or how it relates to the brain, just about what the cognitive processes are that generate our judgments that we are conscious, and that the presence of these processes is a strong reason for believing consciousness is present. Better understanding these cognitive processes could then provide some insight into how to design a better test. There is some hope we can make progress in understanding these cognitive processes because philosophers and some scientists have recently starting to investigate them [30]. Making the test a behavioral test, like ACT, would also have the advantage of avoiding the need to directly crack-open the large, opaque black-boxes that are now dominating AI.</p><p>Of course, pushing toward a consensus around a scientific theory of consciousness or small set of theories could be helpful in further developing useful theory-driven tests, like the one proposed by Butlin, Long, et al. However, much effort has been and is currently being put into finding such a theory of consciousness, and the move toward consensus is slow. Thus, more direct, theory-neutral approaches could be a useful focus in the coming years.</p><h3 id="author-bio">Author Bio</h3><p><a href="https://neuralnetnick.com/">Nick Alonso</a> is a final-year PhD student in the Cognitive Science Department at University of California, Irvine, where he is co-advised by Jeffery Krichmar and Emre Neftci. Nick&#x2019;s current research focuses on developing biologically inspired learning algorithms for deep neural networks. Before focusing on machine learning, Nick studied and received a Master&#x2019;s in neuro-philosophy from Georgia State University, where he was a fellow at their neuroscience institute. As an undergraduate at the University of Michigan, Ann Arbor, Nick double majored in computational cognitive science and philosophy.</p><hr><!--kg-card-begin: markdown--><h2 id="references">References</h2>
<ol>
<li><a href="https://www.npr.org/2022/06/16/1105552435/google-ai-sentient">https://www.npr.org/2022/06/16/1105552435/google-ai-sentient</a></li>
<li>For example, see <a href="https://www.youtube.com/watch?v=K-VkMvBjP0c">https://www.youtube.com/watch?v=K-VkMvBjP0c</a></li>
<li>See also <a href="https://www.youtube.com/watch?v=TUCnsS72Q9s">https://www.youtube.com/watch?v=TUCnsS72Q9s</a></li>
<li><a href="https://schwitzsplinters.blogspot.com/2022/10/the-coming-robot-rights-catastrophe.html">https://schwitzsplinters.blogspot.com/2022/10/the-coming-robot-rights-catastrophe.html</a></li>
<li>Schwitzgebel, E., &amp; Garza, M. (2020). Designing AI with rights, consciousness, self-respect, and freedom. <em>Ethics of Artificial Intelligence</em>.</li>
<li>Seth, A. (2023). Why conscious ai is a bad, bad idea. <em>Nautilus</em>.</li>
<li>Block, N. (1995). On a confusion about a function of consciousness. <em>Behavioral and Brain Sciences</em>, 18(2), 227-247.</li>
<li>The term &#x2018;phenomenal&#x2019; in phenomenal consciousness is not meant in the sense of &#x2018;amazing&#x2019;. Rather it is derived from the term &apos;phenomenology&apos;, which was a philosophical movement that emphasized the importance of first-person experience in our understanding of the world.</li>
<li>Singer, P. (1986). All animals are equal. <em>Applied Ethics: Critical Concepts in Philosophy</em>, 4, 51-79.</li>
<li>Chalmers, D. J. (1995). Facing up to the problem of consciousness. <em>Journal of Consciousness Studies</em>, 2(3), 200-219.</li>
<li>Nida-R&#xFC;melin, M., &amp; O Conaill, D. (2002). Qualia: The knowledge argument. <em>Stanford Encyclopedia of Philosophy</em>.</li>
<li>Nagel, T. (1974). What is it like to be a bat? <em>The Philosophical Review</em>.</li>
<li>Jackson, F. (1982). Epiphenomenal qualia. <em>The Philosophical Quarterly</em>, 32(127), 127-136.</li>
<li>A key example of this sort of approach in philosophy is called the &#x2018;Phenomenal Concept Strategy&#x2019;.</li>
<li>Schneider, S. (2019). Artificial you: AI and the future of your mind. <em>Princeton University Press</em>.</li>
<li>Block, N. (2002). The harder problem of consciousness. <em>The Journal of Philosophy</em>, 99(8), 391-425.</li>
<li>Seth, A. K., &amp; Bayne, T. (2022). Theories of consciousness. <em>Nature Reviews Neuroscience</em>, 23(7), 439-452.</li>
<li>Melloni, L., Mudrik, L., Pitts, M., Bendtz, K., Ferrante, O., Gorska, U., ... &amp; Tononi, G. (2023). An adversarial collaboration protocol for testing contrasting predictions of global neuronal workspace and integrated information theory. <em>PLOS One</em>, 18(2), e0268577.</li>
<li>Butlin, P., Long, R., Elmoznino, E., Bengio, Y., Birch, J., Constant, A., ... &amp; VanRullen, R. (2023). Consciousness in Artificial Intelligence: Insights from the Science of Consciousness. <em>arXiv:2308.08708</em>.</li>
<li>For examples of some reactions by scientists to this theory-driven proposal see &#x201C;If AI becomes conscious: here&apos;s how researchers will know&#x201D;, which was published by M. Lenharoin in Nature as a commentary on this theory-driven approach.</li>
<li>Chalmers, D. J. (1995). Absent qualia, fading qualia, dancing qualia. <em>Conscious Experience</em>, 309-328.</li>
<li>Ad absurdum arguments are similar to proofs by contradiction, for you mathematicians out there.</li>
<li><a href="https://en.wikipedia.org/wiki/Anton_syndrome">https://en.wikipedia.org/wiki/Anton_syndrome</a></li>
<li><a href="https://en.wikipedia.org/wiki/Change_blindness">https://en.wikipedia.org/wiki/Change_blindness</a></li>
<li>Schneider, S. (2019). Artificial you: AI and the future of your mind. Princeton University Press.</li>
<li>Udell, D. B. (2021). Susan Schneider&apos;s Proposed Tests for AI Consciousness: Promising but Flawed. <em>Journal of Consciousness Studies</em>, 28(5-6), 121-144.</li>
<li>Turner E. and Schneider S. &#x201C;The ACT test for AI Consciousness&#x201D;, <em>Ethics of Artificial Intelligence</em> (forthcoming).</li>
<li>Udell, D. B. (2021). Susan Schneider&apos;s Proposed Tests for AI Consciousness: Promising but Flawed. <em>Journal of Consciousness Studies</em>, 28(5-6), 121-144.</li>
<li>Schwitzgebel, E., &amp; Garza, M. (2020). Designing AI with rights, consciousness, self-respect, and freedom. *Ethics of Artificial Intelligence.</li>
<li>Chalmers, D. (2018). The meta-problem of consciousness. <em>Journal of Consciousness Studies</em>.</li>
</ol>
<!--kg-card-end: markdown--><h3 id="citation">Citation</h3><p>For attribution in academic contexts or books, please cite this work as</p><blockquote>Nick Alonso, &quot;An Introduction to the Problems of AI Consciousness&quot;, The Gradient, 2023.</blockquote><p>Bibtex citation:</p><!--kg-card-begin: markdown--><pre><code>@article{alonso2023aiconsciousness,
    author = {Alonso, Nick},
    title = {An Introduction to the Problems of AI Consciousness},
    journal = {The Gradient},
    year = {2023},
    howpublished = {\url{https://thegradient.pub/an-introduction-to-the-problems-of-ai-consciousness},
}
</code></pre>
<!--kg-card-end: markdown-->]]></content:encoded></item></channel></rss>