<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>AI | VentureBeat</title>
        <link>https://venturebeat.com/category/ai/feed/</link>
        <description>Transformative tech coverage that matters</description>
        <lastBuildDate>Tue, 11 Nov 2025 17:45:44 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <copyright>Copyright 2025, VentureBeat</copyright>
        <item>
            <title><![CDATA[Meta returns to open source AI with Omnilingual ASR models that can transcribe 1,600+ languages natively]]></title>
            <link>https://venturebeat.com/ai/meta-returns-to-open-source-ai-with-omnilingual-asr-models-that-can</link>
            <guid isPermaLink="false">6gYDuHYeHB4ZtuM0lx3Qi3</guid>
            <pubDate>Mon, 10 Nov 2025 20:27:00 GMT</pubDate>
            <description><![CDATA[<p>Meta has just released a new <a href="https://ai.meta.com/blog/omnilingual-asr">multilingual automatic speech recognition (ASR) system</a> supporting 1,600+ languages — dwarfing OpenAI’s open source Whisper model, which supports just 99. </p><p>Is architecture also allows developers to extend that support to thousands more. Through a feature called zero-shot in-context learning, users can provide a few paired examples of audio and text in a new language at inference time, enabling the model to transcribe additional utterances in that language without any retraining.</p><p>In practice, this expands potential coverage to more than 5,400 languages — roughly every spoken language with a known script.</p><p>It’s a shift from static model capabilities to a flexible framework that communities can adapt themselves. So while the 1,600 languages reflect official training coverage, the broader figure represents Omnilingual ASR’s capacity to generalize on demand, making it the most extensible speech recognition system released to date.</p><p>Best of all: it&#x27;s been open sourced under<a href="https://github.com/facebookresearch/omnilingual-asr?tab=License-1-ov-file#readme"> a plain Apache 2.0 license</a> — not a restrictive, quasi open-source Llama license like the company&#x27;s prior releases, which limited use by larger enterprises unless they paid licensing fees — meaning researchers and developers are free to take and implement it right away, for free, without restrictions, even in commercial and enterprise-grade projects!</p><p>Released on November 10 on <a href="https://ai.meta.com/blog/omnilingual-asr-advancing-automatic-speech-recognition/">Meta&#x27;s website</a>, <a href="https://github.com/facebookresearch/omnilingual-asr">Github</a>, along with a <a href="https://huggingface.co/spaces/facebook/omniasr-transcriptions">demo space on Hugging Face</a> and <a href="https://ai.meta.com/research/publications/omnilingual-asr-open-source-multilingual-speech-recognition-for-1600-languages/">technical paper</a>, Meta’s Omnilingual ASR suite includes a family of speech recognition models, a 7-billion parameter multilingual audio representation model, and a massive speech corpus spanning over 350 previously underserved languages. </p><p>All resources are freely available under open licenses, and the models support speech-to-text transcription out of the box.</p><p>“By open sourcing these models and dataset, we aim to break down language barriers, expand digital access, and empower communities worldwide,” Meta posted on its <a href="https://x.com/AIatMeta/status/1987957744138416389">@AIatMeta account on X</a></p><h3><b>Designed for Speech-to-Text Transcription</b></h3><p>At its core, Omnilingual ASR is a speech-to-text system. </p><p>The models are trained to convert spoken language into written text, supporting applications like voice assistants, transcription tools, subtitles, oral archive digitization, and accessibility features for low-resource languages.</p><p>Unlike earlier ASR models that required extensive labeled training data, Omnilingual ASR includes a zero-shot variant. </p><p>This version can transcribe languages it has never seen before—using just a few paired examples of audio and corresponding text. </p><p>This lowers the barrier for adding new or endangered languages dramatically, removing the need for large corpora or retraining.</p><h3><b>Model Family and Technical Design</b></h3><p>The Omnilingual ASR suite includes multiple model families trained on more than 4.3 million hours of audio from 1,600+ languages:</p><ul><li><p>wav2vec 2.0 models for self-supervised speech representation learning (300M–7B parameters)</p></li><li><p>CTC-based ASR models for efficient supervised transcription</p></li><li><p>LLM-ASR models combining a speech encoder with a Transformer-based text decoder for state-of-the-art transcription</p></li><li><p>LLM-ZeroShot ASR model, enabling inference-time adaptation to unseen languages</p></li></ul><p>All models follow an encoder–decoder design: raw audio is converted into a language-agnostic representation, then decoded into written text.</p><h3><b>Why the Scale Matters</b></h3><p>While Whisper and similar models have advanced ASR capabilities for global languages, they fall short on the long tail of human linguistic diversity. Whisper supports 99 languages. Meta’s system:</p><ul><li><p>Directly supports 1,600+ languages</p></li><li><p>Can generalize to 5,400+ languages using in-context learning</p></li><li><p>Achieves character error rates (CER) under 10% in 78% of supported languages</p></li></ul><p>Among those supported are more than 500 languages never previously covered by any ASR model, according to Meta’s research paper.</p><p>This expansion opens new possibilities for communities whose languages are often excluded from digital tools</p><p>Here’s the revised and expanded background section, integrating the broader context of Meta’s 2025 AI strategy, leadership changes, and Llama 4’s reception, complete with in-text citations and links:</p><h3><b>Background: Meta’s AI Overhaul and a Rebound from Llama 4</b></h3><p>The release of Omnilingual ASR arrives at a pivotal moment in Meta’s AI strategy, following a year marked by organizational turbulence, leadership changes, and uneven product execution. </p><p>Omnilingual ASR is the first major open-source model release since the rollout of Llama 4, Meta’s latest large language model, which<a href="epseek-is-here-llama-4-launches-with-long-context-scout-and-maverick-models-and-2t-parameter-behemoth-on-the-way"> debuted in April 2025</a> to <a href="https://venturebeat.com/ai/meta-defends-llama-4-release-against-reports-of-mixed-quality-blames-bugs">mixed and ultimately poor reviews</a>, with scant enterprise adoption compared to Chinese open source model competitors.</p><p>The failure led Meta founder and CEO Mark Zuckerberg to appoint Alexandr Wang, co-founder and prior CEO of AI data supplier Scale AI, <a href="https://www.cnbc.com/2025/06/10/zuckerberg-makes-metas-biggest-bet-on-ai-14-billion-scale-ai-deal.html">as Chief AI Officer</a>, and embark on an <a href="https://www.wired.com/story/meta-poaches-openai-researcher-yang-song/">extensive and costly hiring spree</a> that shocked the AI and business communities with <a href="https://fortune.com/2025/06/18/metas-100-million-signing-bonuses-openai-staff-extreme-ai-talent-war/">eye-watering pay packages for top AI researchers</a>.</p><p>In contrast, Omnilingual ASR represents a strategic and reputational reset. It returns Meta to a domain where the company has historically led — multilingual AI — and offers a truly extensible, community-oriented stack with minimal barriers to entry. </p><p>The system’s support for 1,600+ languages and its extensibility to over 5,000 more via zero-shot in-context learning reassert Meta’s engineering credibility in language technology. </p><p>Importantly, it does so through a free and permissively licensed release, under Apache 2.0, with transparent dataset sourcing and reproducible training protocols.</p><p>This shift aligns with broader themes in Meta’s 2025 strategy. The company has refocused its narrative around a “personal superintelligence” vision, investing heavily in infrastructure (including a September release of custom AI accelerators and Arm-based inference stacks) <a href="https://engineering.fb.com/2025/09/22/data-infrastructure/meta-custom-accelerators/">source</a> while downplaying the metaverse in favor of foundational AI capabilities. The return to public training data in Europe after a regulatory pause also underscores its intention to compete globally, despite privacy scrutiny <a href="https://apnews.com/article/meta-eu-data-training-2025">source</a>.</p><p>Omnilingual ASR, then, is more than a model release — it’s a calculated move to reassert control of the narrative: from the fragmented rollout of Llama 4 to a high-utility, research-grounded contribution that aligns with Meta’s long-term AI platform strategy.</p><h3><b>Community-Centered Dataset Collection</b></h3><p>To achieve this scale, Meta partnered with researchers and community organizations in Africa, Asia, and elsewhere to create the Omnilingual ASR Corpus, a 3,350-hour dataset across 348 low-resource languages. Contributors were compensated local speakers, and recordings were gathered in collaboration with groups like:</p><ul><li><p><b>African Next Voices</b>: A Gates Foundation–supported consortium including Maseno University (Kenya), University of Pretoria, and Data Science Nigeria</p></li><li><p><b>Mozilla Foundation’s Common Voice</b>, supported through the Open Multilingual Speech Fund</p></li><li><p><b>Lanfrica / NaijaVoices</b>, which created data for 11 African languages including Igala, Serer, and Urhobo</p></li></ul><p>The data collection focused on natural, unscripted speech. Prompts were designed to be culturally relevant and open-ended, such as “Is it better to have a few close friends or many casual acquaintances? Why?” Transcriptions used established writing systems, with quality assurance built into every step.</p><h3><b>Performance and Hardware Considerations</b></h3><p>The largest model in the suite, the omniASR_LLM_7B, requires ~17GB of GPU memory for inference, making it suitable for deployment on high-end hardware. Smaller models (300M–1B) can run on lower-power devices and deliver real-time transcription speeds.</p><p>Performance benchmarks show strong results even in low-resource scenarios:</p><ul><li><p>CER &lt;10% in 95% of high-resource and mid-resource languages</p></li><li><p>CER &lt;10% in 36% of low-resource languages</p></li><li><p>Robustness in noisy conditions and unseen domains, especially with fine-tuning</p></li></ul><p>The zero-shot system, omniASR_LLM_7B_ZS, can transcribe new languages with minimal setup. Users provide a few sample audio–text pairs, and the model generates transcriptions for new utterances in the same language.</p><h3><b>Open Access and Developer Tooling</b></h3><p>All models and the dataset are licensed under permissive terms:</p><ul><li><p><b>Apache 2.0</b> for models and code</p></li><li><p><b>CC-BY 4.0</b> for the <a href="https://huggingface.co/datasets/facebook/omnilingual-asr-corpus">Omnilingual ASR Corpus on HuggingFace</a></p></li></ul><p>Installation is supported via PyPI and uv:</p><p><code>pip install omnilingual-asr</code></p><p>Meta also provides:</p><ul><li><p>A HuggingFace dataset integration</p></li><li><p>Pre-built inference pipelines</p></li><li><p>Language-code conditioning for improved accuracy</p></li></ul><p>Developers can view the full list of supported languages using the API:</p><p><code>from omnilingual_asr.models.wav2vec2_llama.lang_ids import supported_langs</code></p><p><code>print(len(supported_langs))
print(supported_langs)</code></p><h3><b>Broader Implications</b></h3><p>Omnilingual ASR reframes language coverage in ASR from a fixed list to an <b>extensible framework</b>. It enables:</p><ul><li><p>Community-driven inclusion of underrepresented languages</p></li><li><p>Digital access for oral and endangered languages</p></li><li><p>Research on speech tech in linguistically diverse contexts</p></li></ul><p>Crucially, Meta emphasizes ethical considerations throughout—advocating for open-source participation and collaboration with native-speaking communities.</p><p>“No model can ever anticipate and include all of the world’s languages in advance,” the Omnilingual ASR paper states, “but Omnilingual ASR makes it possible for communities to extend recognition with their own data.”</p><h3><b>Access the Tools</b></h3><p>All resources are now available at:</p><ul><li><p><b>Code + Models</b>: <a href="https://github.com/facebookresearch/omnilingual-asr">github.com/facebookresearch/omnilingual-asr</a></p></li><li><p><b>Dataset</b>: <a href="https://huggingface.co/datasets/facebook/omnilingual-asr-corpus">huggingface.co/datasets/facebook/omnilingual-asr-corpus</a></p></li><li><p><b>Blogpost</b>: <a href="https://ai.meta.com/blog/omnilingual-asr">ai.meta.com/blog/omnilingual-asr</a></p></li></ul><h3><b>What This Means for Enterprises</b></h3><p>For enterprise developers, especially those operating in multilingual or international markets, Omnilingual ASR significantly lowers the barrier to deploying speech-to-text systems across a broader range of customers and geographies. </p><p>Instead of relying on commercial ASR APIs that support only a narrow set of high-resource languages, teams can now integrate an open-source pipeline that covers over 1,600 languages out of the box—with the option to extend it to thousands more via zero-shot learning.</p><p>This flexibility is especially valuable for enterprises working in sectors like voice-based customer support, transcription services, accessibility, education, or civic technology, where local language coverage can be a competitive or regulatory necessity. Because the models are released under the permissive Apache 2.0 license, businesses can fine-tune, deploy, or integrate them into proprietary systems without restrictive terms.</p><p>It also represents a shift in the ASR landscape—from centralized, cloud-gated offerings to community-extendable infrastructure. By making multilingual speech recognition more accessible, customizable, and cost-effective, Omnilingual ASR opens the door to a new generation of enterprise speech applications built around linguistic inclusion rather than linguistic limitation.</p>]]></description>
            <author>carl.franzen@venturebeat.com (Carl Franzen)</author>
            <category>AI</category>
            <enclosure url="https://images.ctfassets.net/jdtwqhzvc2n1/5WF8w75sB7wnAYEK5pYnPD/16059c6ac3f6b853b3b301bc66f950d3/cfr0z3n_graphic_novel_abstract_expressionist_outline_style_show_390da294-e50e-424d-ab36-20b02133d3d9.png?w=300&amp;q=30" length="0" type="image/png"/>
        </item>
        <item>
            <title><![CDATA[Chronosphere takes on Datadog with AI that explains itself, not just outages]]></title>
            <link>https://venturebeat.com/ai/chronosphere-takes-on-datadog-with-ai-that-explains-itself-not-just-outages</link>
            <guid isPermaLink="false">1qCkvHVobPc5QWhKKWRFBM</guid>
            <pubDate>Mon, 10 Nov 2025 19:00:00 GMT</pubDate>
            <description><![CDATA[<p><a href="https://chronosphere.io/"><u>Chronosphere</u></a>, a New York-based observability startup <a href="https://chronosphere.io/news/chronosphere-marks-a-year-of-new-capabilities/"><u>valued at $1.6 billion</u></a>, announced Monday it will launch <a href="https://chronosphere.io/ai-guided-troubleshooting/"><u>AI-Guided Troubleshooting</u></a> capabilities designed to help engineers diagnose and fix production software failures — a problem that has intensified as artificial intelligence tools accelerate code creation while making systems harder to debug.</p><p>The new features combine AI-driven analysis with what Chronosphere calls a <a href="https://chronosphere.io/learn/how-chronosphere-built-a-deployment-system-with-temporal/"><u>Temporal Knowledge Graph</u></a>, a continuously updated map of an organization&#x27;s services, infrastructure dependencies, and system changes over time. The technology aims to address a mounting challenge in enterprise software: developers are writing code faster than ever with AI assistance, but troubleshooting remains largely manual, creating bottlenecks when applications fail.</p><p>&quot;For AI to be effective in observability, it needs more than pattern recognition and summarization,&quot; said Martin Mao, Chronosphere&#x27;s CEO and co-founder, in an exclusive interview with VentureBeat. &quot;Chronosphere has spent years building the data foundation and analytical depth needed for AI to actually help engineers. With our Temporal Knowledge Graph and advanced analytics capabilities, we&#x27;re giving AI the understanding it needs to make observability truly intelligent — and giving engineers the confidence to trust its guidance.&quot;</p><p>The announcement comes as the observability market — software that monitors complex cloud applications— faces mounting pressure to justify escalating costs. Enterprise log data volumes have grown <a href="https://chronosphere.io/learn/observability-log-data-trends/"><u>250% year-over-year</u></a>, according to Chronosphere&#x27;s own research, while a study from MIT and the University of Pennsylvania found that generative <a href="https://economics.mit.edu/sites/default/files/inline-files/draft_copilot_experiments.pdf"><u>AI has spurred a 13.5% increase in weekly code commits</u></a>, signifying faster development velocity but also greater system complexity.</p><h2><b>AI writes code 13% faster, but debugging stays stubbornly manual</b></h2><p>Despite advances in automated code generation, debugging production failures remains stubbornly manual. When a major e-commerce site slows during checkout or a banking app fails to process transactions, engineers must sift through millions of data points — server logs, application traces, infrastructure metrics, recent code deployments — to identify root causes.</p><p>Chronosphere&#x27;s answer is what it calls <a href="https://chronosphere.io/ai-guided-troubleshooting/"><u>AI-Guided Troubleshooting</u></a>, built on four core capabilities: automated &quot;Suggestions&quot; that propose investigation paths backed by data; the Temporal Knowledge Graph that maps system relationships and changes; Investigation Notebooks that document each troubleshooting step for future reference; and natural language query building.</p><p>Mao explained the <a href="https://chronosphere.io/learn/how-chronosphere-built-a-deployment-system-with-temporal/"><u>Temporal Knowledge Graph</u></a> in practical terms: &quot;It&#x27;s a living, time-aware model of your system. It stitches together telemetry—metrics, traces, logs—infrastructure context, change events like deploys and feature flags, and even human input like notes and runbooks into a single, queryable map that updates as your system evolves.&quot;</p><p>This differs fundamentally from the service dependency maps offered by competitors like <a href="https://www.datadoghq.com/dg/monitor/free-trial/?utm_source=google&amp;utm_medium=paid-search&amp;utm_campaign=dg-brand-ww&amp;utm_keyword=datadog&amp;utm_matchtype=b&amp;igaag=95325237782&amp;igaat=&amp;igacm=9551169254&amp;igacr=673270769690&amp;igakw=datadog&amp;igamt=b&amp;igant=g&amp;utm_campaignid=9551169254&amp;utm_adgroupid=95325237782&amp;gad_source=1&amp;gad_campaignid=9551169254&amp;gbraid=0AAAAADFY9NlOpf6xdtUzDLzD2BUR67UTl&amp;gclid=CjwKCAiAt8bIBhBpEiwAzH1w6T3Vk9lkeno-VLHlyRmDR0PFF8gTUGxe72EBr8QGbSpTNY5qtp63eRoC25kQAvD_BwE"><u>Datadog</u></a>, <a href="https://www.dynatrace.com/"><u>Dynatrace</u></a>, and <a href="https://www.splunk.com/"><u>Splunk</u></a>, Mao argued. &quot;It adds time, not just topology,&quot; he said. &quot;It tracks how services and dependencies change over time and connects those changes to incidents—what changed and why. Many tools rely on standardized integrations; our graph goes a step further to normalize custom, non-standard telemetry so application-specific signals aren&#x27;t a blind spot.&quot;</p><h2><b>Why Chronosphere shows its work instead of making automatic decisions</b></h2><p>Unlike purely automated systems, <a href="https://chronosphere.io/"><u>Chronosphere</u></a> designed its AI features to keep engineers in the driver&#x27;s seat—a deliberate choice meant to address what Mao calls the &quot;confident-but-wrong guidance&quot; problem plaguing early AI observability tools.</p><p>&quot;&#x27;Keeping engineers in control&#x27; means the AI shows its work, proposes next steps, and lets engineers verify or override — never auto-deciding behind the scenes,&quot; Mao explained. &quot;Every Suggestion includes the evidence—timing, dependencies, error patterns — and a &#x27;Why was this suggested?&#x27; view, so they can inspect what was checked and ruled out before acting.&quot;</p><p>He walked through a concrete example: &quot;An SLO [service level objective] alert fires on Checkout. Chronosphere immediately surfaces a ranked Suggestion: errors appear to have started in the dependent Payment service. An engineer can click Investigate to see the charts and reasoning and, if it holds up, choose to dig deeper. As they steer into Payment, the system adapts with new Suggestions scoped to that service—all from one view, no tab-hopping.&quot;</p><p>In this scenario, the engineer asks &quot;what changed?&quot; and the system pulls in change events. &quot;Our Notebook capability makes the causal chain plain: a feature-flag update preceded pod memory exhaustion in Payment; Checkout&#x27;s spike is a downstream symptom,&quot; Mao said. &quot;They can decide to roll back the flag. That whole path — suggestions followed, evidence viewed, conclusions—is captured automatically in an Investigation Notebook, and the outcome feeds the Temporal Knowledge Graph so similar future incidents are faster to resolve.&quot;</p><h2><b>How a $1.6 billion startup takes on Datadog, Dynatrace, and Splunk</b></h2><p>Chronosphere enters an increasingly crowded field. <a href="https://www.datadoghq.com/"><u>Datadog</u></a>, the publicly traded observability leader valued at over $40 billion, has introduced its own AI-powered troubleshooting features. So have Dynatrace and Splunk. All three offer comprehensive &quot;all-in-one&quot; platforms that promise single-pane-of-glass visibility.</p><p>Mao distinguished Chronosphere&#x27;s approach on technical grounds. &quot;Early &#x27;AI for observability&#x27; leaned heavily on pattern-spotting and summarization, which tends to break down during real incidents,&quot; he said. &quot;These approaches often stop at correlating anomalies or producing fluent explanations without the deeper analysis and causal reasoning observability leaders need. They can feel impressive in demos but disappoint in production—they summarize signals rather than explain cause and effect.&quot;</p><p>A specific technical gap, he argued, involves custom application telemetry. &quot;Most platforms reason over standardized integrations—Kubernetes, common cloud services, popular databases—ignoring the most telling clues that live in custom app telemetry,&quot; Mao said. &quot;With an incomplete picture, large language models will &#x27;fill in the gaps,&#x27; producing confident-but-wrong guidance that sends teams down dead ends.&quot;</p><p>Chronosphere&#x27;s competitive positioning received validation in July when Gartner named it a Leader in the <a href="https://www.gartner.com/en/documents/6688834"><u>2025 Magic Quadrant for Observability Platforms</u></a> for the second consecutive year. The firm was recognized based on both &quot;Completeness of Vision&quot; and &quot;Ability to Execute.&quot; In December 2024, Chronosphere also tied for the highest overall rating among recognized vendors in Gartner Peer Insights&#x27; &quot;Voice of the Customer&quot; report, scoring 4.7 out of 5 based on 70 reviews.</p><p>Yet the company faces intensifying competition for high-profile customers. UBS analysts noted in July that OpenAI now runs both Datadog and Chronosphere side-by-side to monitor GPU workloads, suggesting the AI leader is evaluating alternatives. While UBS maintained its buy rating on Datadog, the analysts warned that growing Chronosphere usage could pressure Datadog&#x27;s pricing power.</p><h2><b>Inside the 84% cost reduction claims—and what CIOs should actually measure</b></h2><p>Beyond technical capabilities, Chronosphere has built its market position on cost control — a critical factor as observability spending spirals. The company claims its platform reduces data volumes and associated costs by 84% on average while cutting critical incidents by up to 75%.</p><p>When pressed for specific customer examples with real numbers, Mao pointed to several case studies. &quot;Robinhood has seen a 5x improvement in reliability and a 4x improvement in Mean Time to Detection,&quot; he said. &quot;DoorDash used Chronosphere to improve governance and standardize monitoring practices. Astronomer achieved over 85% cost reduction by shaping data on ingest, and Affirm scaled their load 10x during a Black Friday event with no issues, highlighting the platform&#x27;s reliability under extreme conditions.&quot;</p><p>The cost argument matters because, as <a href="https://chronosphere.io/news/chronosphere-logs-raises-bar-in-observability/"><u>Paul Nashawaty</u></a>, principal analyst at CUBE Research, noted when Chronosphere launched its Logs 2.0 product in June: &quot;Organizations are drowning in telemetry data, with over 70% of observability spend going toward storing logs that are never queried.&quot;</p><p>For CIOs fatigued by &quot;AI-powered&quot; announcements, Mao acknowledged skepticism is warranted. &quot;The way to cut through it is to test whether the AI shortens incidents, reduces toil, and builds reusable knowledge in your own environment, not in a demo,&quot; he advised. He recommended CIOs evaluate three factors: transparency and control (does the system show its reasoning?), coverage of custom telemetry (can it handle non-standardized data?), and manual toil avoided (how many ad-hoc queries and tool-switches are eliminated?).</p><h2><b>Why Chronosphere partners with five vendors instead of building everything itself</b></h2><p>Alongside the AI troubleshooting announcement, Chronosphere revealed a new <a href="https://chronosphere.io/partners/"><u>Partner Program</u></a> integrating five specialized vendors to fill gaps in its platform: Arize for large language model monitoring, Embrace for real user monitoring, Polar Signals for continuous profiling, Checkly for synthetic monitoring, and Rootly for incident management.</p><p>The strategy represents a deliberate bet against the all-in-one platforms dominating the market. &quot;While an all-in-one platform may be sufficient for smaller organizations, global enterprises demand best-in-class depth across each domain,&quot; Mao said. &quot;This is what drove us to build our Partner Program and invest in seamless integrations with leading providers—so our customers can operate with confidence and clarity at every layer of observability.&quot;</p><p>Noah Smolen, head of partnerships at Arize, said the collaboration addresses a specific enterprise need. &quot;With a wide array of Fortune 500 customers, we understand the high bar needed to ensure AI agent systems are ready to deploy and stay incident-free, especially given the pace of AI adoption in the enterprise,&quot; Smolen said. &quot;Our partnership with Chronosphere comes at a time when an integrated purpose-built cloud-native and AI-observability suite solves a huge pain point for forward-thinking C-suite leaders who demand the very best across their entire observability stack.&quot;</p><p>Similarly, JJ Tang, CEO and founder of Rootly, emphasized the incident resolution benefits. &quot;Incidents hinder innovation and revenue, and the challenge lies in sifting through vast amounts of observability data, mobilizing teams, and resolving issues quickly,&quot; Tang said. &quot;Integrating Chronosphere with Rootly allows engineers to collaborate with context and resolve issues faster within their existing communication channels, drastically reducing time to resolution and ultimately improving reliability—78% plus decreases in repeat Sev0 and Sev1 incidents.&quot;</p><p>When asked how total costs compare when customers use multiple partner contracts versus a single platform, Mao acknowledged the current complexity. &quot;At present, mutual customers typically maintain separate contracts unless they engage through a services partner or system integrator,&quot; he said. However, he argued the economics still favor the composable approach: &quot;Our combined technologies deliver exceptional value—in most circumstances at just a fraction of the price of a single-platform solution. Beyond the savings, customers gain a richer, more unified observability experience that unlocks deeper insights and greater efficiency, especially for large-scale environments.&quot;</p><p>The company plans to streamline this over time. &quot;As the ISV program matures, we&#x27;re focused on delivering a more streamlined experience by transitioning to a single, unified contract that simplifies procurement and accelerates time to value,&quot; Mao said.</p><h2><b>How two Uber engineers turned Halloween outages into a billion-dollar startup</b></h2><p>Chronosphere&#x27;s origins trace to 2019, when Mao and co-founder Rob Skillington left Uber after building the ride-hailing giant&#x27;s internal observability platform. At Uber, Mao&#x27;s team had faced a crisis: the company&#x27;s in-house tools would fail on its two busiest nights — Halloween and New Year&#x27;s Eve — cutting off visibility into whether customers could request rides or drivers could locate passengers.</p><p>The solution they built at Uber used open-source software and ultimately allowed the company to operate without outages, even during high-volume events. But the broader market insight came at an industry conference in December 2018, when major cloud providers threw their weight behind Kubernetes, Google&#x27;s container orchestration technology.</p><p>&quot;This meant that most technology architectures were eventually going to look like Uber&#x27;s,&quot; Mao recalled in an <a href="https://greylock.com/greymatter/chronosphere-is-making-the-observability-platform-built-for-control/"><u>August 2024 profile by Greylock Partners</u></a>, Chronosphere&#x27;s lead investor. &quot;And that meant every company, not just a few big tech companies and the Walmarts of the world, would have the exact same problem we had solved at Uber.&quot;</p><p>Chronosphere has since raised more than <a href="https://chronosphere.io/news/chronosphere-marks-a-year-of-new-capabilities/"><u>$343 million in funding</u></a> across multiple rounds led by Greylock, Lux Capital, General Atlantic, Addition, and Founders Fund. The company operates as a remote-first organization with offices in New York, Austin, Boston, San Francisco, and Seattle, employing approximately 299 people according to LinkedIn data.</p><p>The company&#x27;s customer base includes <a href="https://www.doordash.com/"><u>DoorDash</u></a>, <a href="https://www.zillow.com"><u>Zillow</u></a>, <a href="https://www.snap.com/"><u>Snap</u></a>, <a href="https://robinhood.com/us/en/"><u>Robinhood</u></a>, and <a href="https://www.affirm.com/"><u>Affirm</u></a> — predominantly high-growth technology companies operating cloud-native, Kubernetes-based infrastructures at massive scale.</p><h2><b>What&#x27;s available now—and what enterprises can expect in 2026</b></h2><p>Chronosphere&#x27;s <a href="https://chronosphere.io/news/ai-guided-troubleshooting-redefines-observability/"><u>AI-Guided Troubleshooting</u></a> capabilities, including Suggestions and Investigation Notebooks, entered limited availability Monday with select customers. The company plans full general availability in 2026. The <a href="https://docs.chronosphere.io/integrate/mcp-server"><u>Model Context Protocol (MCP) Server</u></a>, which enables engineers to integrate Chronosphere directly into internal AI workflows and query observability data through AI-enabled development environments, is available immediately for all Chronosphere customers.</p><p>The phased rollout reflects the company&#x27;s cautious approach to deploying AI in production environments where mistakes carry real costs. By gathering feedback from early adopters before broad release, Chronosphere aims to refine its guidance algorithms and validate that its suggestions genuinely accelerate troubleshooting rather than simply generating impressive demonstrations.</p><p>The longer game, however, extends beyond individual product features. Chronosphere&#x27;s dual bet — on transparent AI that shows its reasoning and on a partner ecosystem rather than all-in-one integration — amounts to a fundamental thesis about how enterprise observability will evolve as systems grow more complex.</p><p>If that thesis proves correct, the company that solves observability for the AI age won&#x27;t be the one with the most automated black box. It will be the one that earns engineers&#x27; trust by explaining what it knows, admitting what it doesn&#x27;t, and letting humans make the final call. In an industry drowning in data and promised silver bullets, Chronosphere is wagering that showing your work still matters — even when AI is doing the math.</p>]]></description>
            <author>michael.nunez@venturebeat.com (Michael Nuñez)</author>
            <category>AI</category>
            <enclosure url="https://images.ctfassets.net/jdtwqhzvc2n1/6Jp2LKG6zzFEnDhB9m893X/06467084db114c990d9e72be3896bf51/nuneybits_Vector_art_of_AI_debugging_maze_2b3bfc89-ab17-4839-a03e-6331a42f5030.webp?w=300&amp;q=30" length="0" type="image/webp"/>
        </item>
        <item>
            <title><![CDATA[How context engineering can save your company from AI vibe code overload: lessons from Qodo and Monday.com]]></title>
            <link>https://venturebeat.com/ai/how-context-engineering-can-save-your-company-from-ai-vibe-code-overload</link>
            <guid isPermaLink="false">R2CeeuCF7v84F1DxoS27c</guid>
            <pubDate>Mon, 10 Nov 2025 15:00:00 GMT</pubDate>
            <description><![CDATA[<p>As cloud project tracking software <a href="https://monday.com/">monday.com</a>’s engineering organization scaled past 500 developers, the team began to feel the strain of its own success. Product lines were multiplying, microservices proliferating, and code was flowing faster than human reviewers could keep up. The company needed a way to review thousands of pull requests each month without drowning developers in tedium — or letting quality slip.</p><p>That’s when Guy Regev, VP of R&amp;D and head of the Growth and monday Dev teams, started experimenting with a new AI tool from <a href="https://www.qodo.ai/">Qodo</a>, an Israeli startup focused on developer agents. What began as a lightweight test soon became a critical part of monday.com’s software delivery infrastructure, as a <a href="https://www.qodo.ai/blog/monday-com-accelerates-review-cycles-and-improves-code-quality-with-qodo/">new case study</a> released by both Qodo and monday.com today reveals. </p><p>“Qodo doesn’t feel like just another tool—it’s like adding a new developer to the team who actually learns how we work,&quot; Regev told VentureBeat in a recent video call interview, adding that it has &quot;prevented over 800 issues per month from reaching production—some of them could have caused serious security vulnerabilities.&quot;</p><p>Unlike code generation tools like GitHub Copilot or Cursor, Qodo isn’t trying to write new code. Instead, it specializes in reviewing it — using what it calls <b>context engineering</b> to understand not just what changed in a pull request, but why, how it aligns with business logic, and whether it follows internal best practices. </p><p>&quot;You can call Claude Code or Cursor and in five minutes get 1,000 lines of code,&quot; said Itamar Friedman, co-founder and CEO of Qodo, in the same video call interview as with Regev. &quot;You have 40 minutes, and you can&#x27;t review that. So you need Qodo to actually review it.”</p><p>For monday.com, this capability wasn’t just helpful — it was transformative.</p><h2><b>Code Review, at Scale</b></h2><p>At any given time, monday.com’s developers are shipping updates across hundreds of repositories and services. The engineering org works in tightly coordinated teams, each aligned with specific parts of the product: marketing, CRM, dev tools, internal platforms, and more.</p><p>That’s where Qodo came in. The company’s platform uses AI not just to check for obvious bugs or style violations, but to evaluate whether a pull request follows team-specific conventions, architectural guidelines, and historical patterns. </p><p>It does this by learning from your own codebase — training on previous PRs, comments, merges, and even Slack threads to understand how your team works.</p><p>&quot;The comments Qodo gives aren’t generic—they reflect our values, our libraries, even our standards for things like feature flags and privacy,&quot; Regev said. &quot;It’s context-aware in a way traditional tools aren’t.&quot;</p><h2><b>What “Context Engineering” Actually Means</b></h2><p>Qodo calls its secret sauce <b>context engineering</b> — a system-level approach to managing everything the model sees when making a decision.</p><p> This includes the PR code diff, of course, but also prior discussions, documentation, relevant files from the repo, even test results and configuration data.</p><p>The idea is that language models don’t really “think” — they predict the next token based on the inputs they’re given. So the quality of their output depends almost entirely on the quality and structure of their inputs.</p><p>As Dana Fine, Qodo’s community manager, put it in a<a href="https://www.qodo.ai/blog/context-engineering/"> blog post</a>: “You’re not just writing prompts; you’re designing structured input under a fixed token limit. Every token is a design decision.”</p><p>This isn’t just theory. In monday.com’s case, it meant Qodo could catch not only the obvious bugs, but the subtle ones that typically slip past human reviewers — hardcoded variables, missing fallbacks, or violations of cross-team architecture conventions.</p><p>One example stood out. In a recent PR, Qodo flagged a line that inadvertently exposed a staging environment variable — something no human reviewer caught. Had it been merged, it might have caused problems in production. </p><p>&quot;The hours we would spend on fixing this security leak and the legal issue that it would bring would be much more than the hours that we reduce from a pull-request,&quot; said Regev.</p><h2><b>Integration into the Pipeline</b></h2><p>Today, Qodo is deeply integrated into monday.com’s development workflow, analyzing pull requests and surfacing context-aware recommendations based on prior team code reviews. </p><p>“It doesn’t feel like just another tool... It feels like another teammate that joined the system — one who learns how we work,&quot; Regev noted. </p><p>Developers receive suggestions during the review process and remain in control of final decisions — a human-in-the-loop model that was critical for adoption.</p><p>Because Qodo integrated directly into GitHub via pull request actions and comments, Monday.com’s infrastructure team didn’t face a steep learning curve.</p><p>“It’s just a GitHub action,” said Regev. “It creates a PR with the tests. It’s not like a separate tool we had to learn.”</p><p>“The purpose is to actually help the developer learn the code, take ownership, give feedback to each other, and learn from that and establish the standards,&quot; added Friedman.</p><h2><b>The Results: Time Saved, Bugs Prevented</b></h2><p>Since rolling out Qodo more broadly, monday.com has seen measurable improvements across multiple teams.</p><p>Internal analysis shows that developers save roughly an hour per pull request on average. Multiply that across thousands of PRs per month, and the savings quickly reach thousands of developer hours annually.</p><p>These aren’t just cosmetic issues — many relate to business logic, security, or runtime stability. And because Qodo’s suggestions reflect monday.com’s actual conventions, developers are more likely to act on them.</p><p>The system’s accuracy is rooted in its data-first design. Qodo trains on each company’s private codebase and historical data, adapting to different team styles and practices. It doesn’t rely on one-size-fits-all rules or external datasets. Everything is tailored.</p><h2><b>From Internal Tool to Product Vision</b></h2><p>Regev’s team was so impressed with Qodo’s impact that they’ve started planning deeper integrations between Qodo and Monday Dev, the developer-focused product line monday.com is building.</p><p>The vision is to create a workflow where business context — tasks, tickets, customer feedback — flows directly into the code review layer. That way, reviewers can assess not just whether the code “works,” but whether it solves the right problem.</p><p>“Before, we had linters, danger rules, static analysis... rule-based... you need to configure all the rules,&quot; Regev said. &quot;But it doesn’t know what you don’t know... Qodo... feels like it’s learning from our engineers.”</p><p>This aligns closely with Qodo’s own roadmap. The company doesn’t just review code. It’s building a full platform of developer agents — including Qodo Gen for context-aware code generation, Qodo Merge for automated PR analysis, and Qodo Cover, a regression-testing agent that uses runtime validation to ensure test coverage.</p><p>All of this is powered by Qodo’s own infrastructure, including its new open-source embedding model, Qodo-Embed-1-1.5B, which outperformed offerings from OpenAI and Salesforce on code retrieval benchmarks.</p><h2><b>What’s Next?</b></h2><p>Qodo is now offering its platform under a freemium model — free for individuals, discounted for startups through Google Cloud’s Perks program, and enterprise-grade for companies that need SSO, air-gapped deployment, or advanced controls.</p><p>The company is already working with teams at NVIDIA, Intuit, and other Fortune 500 companies. And thanks to a recent partnership with Google Cloud, Qodo’s models are available directly inside Vertex AI’s Model Garden, making it easier to integrate into enterprise pipelines.</p><p>&quot;Context engines will be the big story of 2026,&quot; Friedman said. &quot;Every enterprise will need to build their own second brain if they want AI that actually understands and helps them.&quot;</p><p>As AI systems become more embedded in software development, tools like Qodo are showing how the right context — delivered at the right moment — can transform how teams build, ship, and scale code across the enterprise.</p>]]></description>
            <author>carl.franzen@venturebeat.com (Carl Franzen)</author>
            <category>AI</category>
            <enclosure url="https://images.ctfassets.net/jdtwqhzvc2n1/6ITdq6fWHUqQToLpa2LMjE/9071464cc3dabd3e163518c89f5eb3b2/cfr0z3n_aerial_view_extended_view_of_hundreds_of_software_devel_dac48c75-82c8-48b8-9ca4-ff785e9ba5f3.png?w=300&amp;q=30" length="0" type="image/png"/>
        </item>
        <item>
            <title><![CDATA[Baseten takes on hyperscalers with new AI training platform that lets you own your model weights]]></title>
            <link>https://venturebeat.com/ai/baseten-takes-on-hyperscalers-with-new-ai-training-platform-that-lets-you</link>
            <guid isPermaLink="false">4UOKzkyQ3Rxp6ogmiLzDYl</guid>
            <pubDate>Mon, 10 Nov 2025 14:00:00 GMT</pubDate>
            <description><![CDATA[<p><a href="https://www.baseten.co/"><u>Baseten</u></a>, the AI infrastructure company recently valued at $2.15 billion, is making its most significant product pivot yet: a full-scale push into model training that could reshape how enterprises wean themselves off dependence on OpenAI and other closed-source AI providers.</p><p>The San Francisco-based company announced Thursday the general availability of <a href="https://www.baseten.co/products/training/"><u>Baseten Training</u></a>, an infrastructure platform designed to help companies fine-tune open-source AI models without the operational headaches of managing GPU clusters, multi-node orchestration, or cloud capacity planning. The move is a calculated expansion beyond Baseten&#x27;s core inference business, driven by what CTO Amir Haghighat describes as relentless customer demand and a strategic imperative to capture the full lifecycle of AI deployment.</p><p>&quot;We had a captive audience of customers who kept coming to us saying, &#x27;Hey, I hate this problem,&#x27;&quot; Haghighat said in an interview. &quot;One of them told me, &#x27;Look, I bought a bunch of H100s from a cloud provider. I have to SSH in on Friday, run my fine-tuning job, then check on Monday to see if it worked. Sometimes I realize it just hasn&#x27;t been working all along.&#x27;&quot;</p><p>The launch comes at a critical inflection point in enterprise AI adoption. As open-source models from <a href="https://huggingface.co/meta-llama"><u>Meta</u></a>, <a href="https://huggingface.co/Alibaba-NLP"><u>Alibaba</u></a>, and others increasingly rival proprietary systems in performance, companies face mounting pressure to reduce their reliance on expensive API calls to services like OpenAI&#x27;s <a href="https://openai.com/index/introducing-gpt-5/"><u>GPT-5</u></a> or Anthropic&#x27;s <a href="https://claude.ai/"><u>Claude</u></a>. But the path from off-the-shelf open-source model to production-ready custom AI remains treacherous, requiring specialized expertise in machine learning operations, infrastructure management, and performance optimization.</p><p>Baseten&#x27;s answer: provide the infrastructure rails while letting companies retain full control over their training code, data, and model weights. It&#x27;s a deliberately low-level approach born from hard-won lessons.</p><h2><b>How a failed product taught Baseten what AI training infrastructure really needs</b></h2><p>This isn&#x27;t Baseten&#x27;s first foray into training. The company&#x27;s previous attempt, a product called Blueprints launched roughly two and a half years ago, failed spectacularly — a failure Haghighat now embraces as instructive.</p><p>&quot;We had created the abstraction layer a little too high,&quot; he explained. &quot;We were trying to create a magical experience, where as a user, you come in and programmatically choose a base model, choose your data and some hyperparameters, and magically out comes a model.&quot;</p><p>The problem? Users didn&#x27;t have the intuition to make the right choices about base models, data quality, or hyperparameters. When their models underperformed, they blamed the product. Baseten found itself in the consulting business rather than the infrastructure business, helping customers debug everything from dataset deduplication to model selection.</p><p>&quot;We became consultants,&quot; Haghighat said. &quot;And that&#x27;s not what we had set out to do.&quot;</p><p><a href="https://www.baseten.co/"><u>Baseten</u></a> killed Blueprints and refocused entirely on inference, vowing to &quot;earn the right&quot; to expand again. That moment arrived earlier this year, driven by two market realities: the vast majority of Baseten&#x27;s inference revenue comes from custom models that customers train elsewhere, and competing training platforms were using restrictive terms of service to lock customers into their inference products.</p><p>&quot;Multiple companies who were building fine-tuning products had in their terms of service that you as a customer cannot take the weights of the fine-tuned model with you somewhere else,&quot; Haghighat said. &quot;I understand why from their perspective — I still don&#x27;t think there is a big company to be made purely on just training or fine-tuning. The sticky part is in inference, the valuable part where value is unlocked is in inference, and ultimately the revenue is in inference.&quot;</p><p>Baseten took the opposite approach: customers own their weights and can download them at will. The bet is that superior inference performance will keep them on the platform anyway.</p><h2><b>Multi-cloud GPU orchestration and sub-minute scheduling set Baseten apart from hyperscalers</b></h2><p>The new <a href="https://www.baseten.co/products/training/"><u>Baseten Training</u></a> product operates at what Haghighat calls &quot;the infrastructure layer&quot; — lower-level than the failed Blueprints experiment, but with opinionated tooling around reliability, observability, and integration with Baseten&#x27;s inference stack.</p><p>Key technical capabilities include multi-node training support across clusters of <a href="https://www.nvidia.com/en-us/data-center/h100/"><u>NVIDIA H100</u></a> or <a href="https://www.baseten.co/blog/accelerating-inference-nvidia-b200-gpus/?utm_term=b200%20gpu&amp;utm_campaign=Search+-Hosting+Inference&amp;utm_source=adwords&amp;utm_medium=ppc&amp;hsa_acc=9990356727&amp;hsa_cam=21607833837&amp;hsa_grp=179204207220&amp;hsa_ad=747940377782&amp;hsa_src=g&amp;hsa_tgt=kwd-2402895176571&amp;hsa_kw=b200%20gpu&amp;hsa_mt=e&amp;hsa_net=adwords&amp;hsa_ver=3&amp;gad_source=1&amp;gad_campaignid=21607833837&amp;gbraid=0AAAAAqCKh1tBBfE-A9FM_fms6m5z6IuPk&amp;gclid=CjwKCAiAt8bIBhBpEiwAzH1w6S_fxqaV-EfbGshbTKib8rt2sQl81yPs6M0y40aoYf1Zy5mV_ECflBoCjPsQAvD_BwE"><u>B200 GPUs</u></a>, automated checkpointing to protect against node failures, sub-minute job scheduling, and integration with Baseten&#x27;s proprietary <a href="https://www.baseten.co/blog/how-we-built-multi-cloud-capacity-management/"><u>Multi-Cloud Management (MCM) </u></a>system. That last piece is critical: MCM allows Baseten to dynamically provision GPU capacity across multiple cloud providers and regions, passing cost savings to customers while avoiding the capacity constraints and multi-year contracts typical of hyperscaler deals.</p><p>&quot;With hyperscalers, you don&#x27;t get to say, &#x27;Hey, give me three or four B200 nodes while my job is running, and then take it back from me and don&#x27;t charge me for it,&#x27;&quot; Haghighat said. &quot;They say, &#x27;No, you need to sign a three-year contract.&#x27; We don&#x27;t do that.&quot;</p><p>Baseten&#x27;s approach mirrors broader trends in cloud infrastructure, where abstraction layers increasingly allow workloads to move fluidly across providers. When AWS experienced a major outage several weeks ago, Baseten&#x27;s inference services remained operational by automatically routing traffic to other cloud providers — a capability now extended to training workloads.</p><p>The technical differentiation extends to Baseten&#x27;s observability tooling, which provides per-GPU metrics for multi-node jobs, granular checkpoint tracking, and a refreshed UI that surfaces infrastructure-level events. The company also introduced an &quot;<a href="https://github.com/basetenlabs/ml-cookbook"><u>ML Cookbook</u></a>&quot; of open-source training recipes for popular models like Gemma, GPT OSS, and Qwen, designed to help users reach &quot;training success&quot; faster.</p><h2><b>Early adopters report 84% cost savings and 50% latency improvements with custom models</b></h2><p>Two early customers illustrate the market Baseten is targeting: AI-native companies building specialized vertical solutions that require custom models.</p><p><a href="https://www.oxen.ai/"><u>Oxen AI</u></a>, a platform focused on dataset management and model fine-tuning, exemplifies the partnership model Baseten envisions. CEO Greg Schoeninger articulated a common strategic calculus, telling VentureBeat: &quot;Whenever I&#x27;ve seen a platform try to do both hardware and software, they usually fail at one of them. That&#x27;s why partnering with Baseten to handle infrastructure was the obvious choice.&quot;</p><p>Oxen built its customer experience entirely on top of Baseten&#x27;s infrastructure, using the <a href="https://www.baseten.co/resources/changelog/authenticate-from-cli-with-truss-login/"><u>Baseten CLI</u></a> to programmatically orchestrate training jobs. The system automatically provisions and deprovisions GPUs, fully concealing Baseten&#x27;s interface behind Oxen&#x27;s own. For one Oxen customer, <a href="https://alliumai.com/"><u>AlliumAI</u></a> — a startup bringing structure to messy retail data — the integration delivered 84% cost savings compared to previous approaches, reducing total inference costs from $46,800 to $7,530.</p><p>&quot;Training custom LoRAs has always been one of the most effective ways to leverage open-source models, but it often came with infrastructure headaches,&quot; said Daniel Demillard, CEO of AlliumAI. &quot;With Oxen and Baseten, that complexity disappears. We can train and deploy models at massive scale without ever worrying about CUDA, which GPU to choose, or shutting down servers after training.&quot;</p><p><a href="https://parsed.com/"><u>Parsed</u></a>, another early customer, tackles a different pain point: helping enterprises reduce dependence on OpenAI by creating specialized models that outperform generalist LLMs on domain-specific tasks. The company works in mission-critical sectors like healthcare, finance, and legal services, where model performance and reliability aren&#x27;t negotiable.</p><p>&quot;Prior to switching to Baseten, we were seeing repetitive and degraded performance on our fine-tuned models due to bugs with our previous training provider,&quot; said Charles O&#x27;Neill, Parsed&#x27;s co-founder and chief science officer. &quot;On top of that, we were struggling to easily download and checkpoint weights after training runs.&quot;</p><p>With Baseten, Parsed achieved 50% lower end-to-end latency for transcription use cases, spun up HIPAA-compliant EU deployments for testing within 48 hours, and kicked off more than 500 training jobs. The company also leveraged Baseten&#x27;s modified <a href="https://docs.baseten.co/examples/vllm"><u>vLLM inference framework</u></a> and <a href="https://www.baseten.co/blog/a-quick-introduction-to-speculative-decoding/"><u>speculative decoding</u></a> — a technique that generates draft tokens to accelerate language model output — to cut latency in half for custom models.</p><p>&quot;Fast models matter,&quot; O&#x27;Neill said. &quot;But fast models that get better over time matter more. A model that&#x27;s 2x faster but static loses to one that&#x27;s slightly slower but improving 10% monthly. Baseten gives us both — the performance edge today and the infrastructure for continuous improvement.&quot;</p><h2><b>Why training and inference are more interconnected than the industry realizes</b></h2><p>The Parsed example illuminates a deeper strategic rationale for Baseten&#x27;s training expansion: the boundary between training and inference is blurrier than conventional wisdom suggests.</p><p>Baseten&#x27;s model performance team uses the training platform extensively to create &quot;draft models&quot; for speculative decoding, a cutting-edge technique that can dramatically accelerate inference. The company recently announced it achieved 650+ tokens per second on OpenAI&#x27;s <a href="https://huggingface.co/openai/gpt-oss-120b"><u>GPT OSS 120B model</u></a> — a 60% improvement over its launch performance — using <a href="https://arxiv.org/abs/2503.01840"><u>EAGLE-3</u></a> speculative decoding, which requires training specialized small models to work alongside larger target models.</p><p>&quot;Ultimately, inference and training plug in more ways than one might think,&quot; Haghighat said. &quot;When you do speculative decoding in inference, you need to train the draft model. Our model performance team is a big customer of the training product to train these EAGLE heads on a continuous basis.&quot;</p><p>This technical interdependence reinforces Baseten&#x27;s thesis that owning both training and inference creates defensible value. The company can optimize the entire lifecycle: a model trained on Baseten can be deployed with a single click to inference endpoints pre-optimized for that architecture, with deployment-from-checkpoint support for chat completion and audio transcription workloads.</p><p>The approach contrasts sharply with vertically integrated competitors like <a href="https://replicate.com/"><u>Replicate</u></a> or <a href="https://modal.com/"><u>Modal</u></a>, which also offer training and inference but with different architectural tradeoffs. Baseten&#x27;s bet is on lower-level infrastructure flexibility and performance optimization, particularly for companies running custom models at scale.</p><h2><b>As open-source AI models improve, enterprises see fine-tuning as the path away from OpenAI dependency</b></h2><p>Underpinning Baseten&#x27;s entire strategy is a conviction about the trajectory of open-source AI models — namely, that they&#x27;re getting good enough, fast enough, to unlock massive enterprise adoption through fine-tuning.</p><p>&quot;Both closed and open-source models are getting better and better in terms of quality,&quot; Haghighat said. &quot;We don&#x27;t even need open source to surpass closed models, because as both of them are getting better, they unlock all these invisible lines of usefulness for different use cases.&quot;</p><p>He pointed to the proliferation of reinforcement learning and supervised fine-tuning techniques that allow companies to take an open-source model and make it &quot;as good as the closed model, not at everything, but at this narrow band of capability that they want.&quot;</p><p>That trend is already visible in Baseten&#x27;s <a href="https://www.baseten.co/products/model-apis/"><u>Model APIs business</u></a>, launched alongside Training earlier this year to provide production-grade access to open-source models. The company was the first provider to offer access to <a href="https://api-docs.deepseek.com/news/news1226"><u>DeepSeek V3</u></a> and <a href="https://api-docs.deepseek.com/news/news250120"><u>R1</u></a>, and has since added models like <a href="https://ai.meta.com/blog/llama-4-multimodal-intelligence/"><u>Llama 4</u></a> and <a href="https://qwen.ai/research"><u>Qwen 3</u></a>, optimized for performance and reliability. Model APIs serves as a top-of-funnel product: companies start with off-the-shelf open-source models, realize they need customization, move to Training for fine-tuning, and ultimately deploy on Baseten&#x27;s <a href="https://www.baseten.co/products/dedicated-deployments/"><u>Dedicated Deployments</u></a> infrastructure.</p><p>Yet Haghighat acknowledged the market remains &quot;fuzzy&quot; around which training techniques will dominate. Baseten is hedging by staying close to the bleeding edge through its <a href="https://www.baseten.co/blog/forward-deployed-engineering/"><u>Forward Deployed Engineering team</u></a>, which works hands-on with select customers on reinforcement learning, supervised fine-tuning, and other advanced techniques.</p><p>&quot;As we do that, we will see patterns emerge about what a productized training product can look like that really addresses the user&#x27;s needs without them having to learn too much about how RL works,&quot; he said. &quot;Are we there as an industry? I would say not quite. I see some attempts at that, but they all seem like almost falling to the same trap that Blueprints fell into—a bit of a walled garden that ties the hands of AI folks behind their back.&quot;</p><p>The roadmap ahead includes potential abstractions for common training patterns, expansion into image, audio, and video fine-tuning, and deeper integration of advanced techniques like prefill-decode disaggregation, which separates the initial processing of prompts from token generation to improve efficiency.</p><h2><b>Baseten faces crowded field but bets developer experience and performance will win enterprise customers</b></h2><p>Baseten enters an increasingly crowded market for AI infrastructure. Hyperscalers like <a href="https://aws.amazon.com/"><u>AWS</u></a>, <a href="https://cloud.google.com/?hl=en"><u>Google Cloud</u></a>, and <a href="https://azure.microsoft.com/en-us/"><u>Microsoft Azure</u></a> offer GPU compute for training, while specialized providers like Lambda Labs, CoreWeave, and Together AI compete on price, performance, or ease of use. Then there are vertically integrated platforms like <a href="https://huggingface.co/"><u>Hugging Face</u></a>, <a href="https://replicate.com/"><u>Replicate</u></a>, and <a href="https://modal.com/"><u>Modal</u></a> that bundle training, inference, and model hosting.</p><p>Baseten&#x27;s differentiation rests on three pillars: its MCM system for multi-cloud capacity management, deep performance optimization expertise built from its inference business, and a developer experience tailored for production deployments rather than experimentation.</p><p>The company&#x27;s recent <a href="https://www.baseten.co/blog/announcing-baseten-150m-series-d/"><u>$150 million Series D</u></a> and <a href="https://www.baseten.co/blog/announcing-baseten-150m-series-d/"><u>$2.15 billion valuation</u></a> provide runway to invest in both products simultaneously. Major customers include <a href="https://www.descript.com/"><u>Descript</u></a>, which uses Baseten for transcription workloads; <a href="https://decagon.ai/"><u>Decagon</u></a>, which runs customer service AI; and <a href="https://sourcegraph.com/"><u>Sourcegraph</u></a>, which powers coding assistants. All three operate in domains where model customization and performance are competitive advantages.</p><p>Timing may be Baseten&#x27;s biggest asset. The confluence of improving open-source models, enterprise discomfort with dependence on proprietary AI providers, and growing sophistication around fine-tuning techniques creates what Haghighat sees as a sustainable market shift.</p><p>&quot;There is a lot of use cases for which closed models have gotten there and open ones have not,&quot; he said. &quot;Where I&#x27;m seeing in the market is people using different training techniques — more recently, a lot of reinforcement learning and SFT — to be able to get this open model to be as good as the closed model, not at everything, but at this narrow band of capability that they want. That&#x27;s very palpable in the market.&quot;</p><p>For enterprises navigating the complex transition from closed to open AI models, Baseten&#x27;s positioning offers a clear value proposition: infrastructure that handles the messy middle of fine-tuning while optimizing for the ultimate goal of performant, reliable, cost-effective inference at scale. The company&#x27;s insistence that customers own their model weights — a stark contrast to competitors using training as a lock-in mechanism — reflects confidence that technical excellence, not contractual restrictions, will drive retention.</p><p>Whether Baseten can execute on this vision depends on navigating tensions inherent in its strategy: staying at the infrastructure layer without becoming consultants, providing power and flexibility without overwhelming users with complexity, and building abstractions at exactly the right level as the market matures. The company&#x27;s willingness to kill Blueprints when it failed suggests a pragmatism that could prove decisive in a market where many infrastructure providers over-promise and under-deliver.</p><p>&quot;Through and through, we&#x27;re an inference company,&quot; Haghighat emphasized. &quot;The reason that we did training is at the service of inference.&quot;</p><p>That clarity of purpose — treating training as a means to an end rather than an end in itself—may be Baseten&#x27;s most important strategic asset. As AI deployment matures from experimentation to production, the companies that solve the full stack stand to capture outsized value. But only if they avoid the trap of technology in search of a problem.</p><p>At least Baseten&#x27;s customers no longer have to SSH into boxes on Friday and pray their training jobs complete by Monday. In the infrastructure business, sometimes the best innovation is simply making the painful parts disappear.</p>]]></description>
            <author>michael.nunez@venturebeat.com (Michael Nuñez)</author>
            <category>AI</category>
            <category>Data Infrastructure</category>
            <enclosure url="https://images.ctfassets.net/jdtwqhzvc2n1/789u1ePM0udlJqqwgxRkPd/6fd3661185f8285972d95d68249faa03/nuneybits_Vector_art_of_multi-cloud_nodes_interconnected_global_df5a72fb-1f71-4b95-8b12-94c1e8def7d6.webp?w=300&amp;q=30" length="0" type="image/webp"/>
        </item>
        <item>
            <title><![CDATA[Celosphere 2025: Where enterprise AI moved from experiment to execution]]></title>
            <link>https://venturebeat.com/ai/celosphere-2025-where-enterprise-ai-moved-from-experiment-to-execution</link>
            <guid isPermaLink="false">4MGz5GAB6kboQHg89j2DAK</guid>
            <pubDate>Mon, 10 Nov 2025 05:00:00 GMT</pubDate>
            <description><![CDATA[<p><i>Presented by Celonis</i></p><hr/><p>After a year of boardroom declarations about “AI transformation,” this was the week where enterprise leaders came together to talk about what actually works. Speaking from the stage at <a href="https://www.celonis.com/events/celosphere/2025">Celosphere in Munich</a>, Celonis co-founder and co-CEO Alexander Rinke set the tone early in his keynote:</p><p>“Only 11 % of companies are seeing measurable benefits from AI projects today,” he said. “That’s not an adoption problem. That’s a context problem.”</p><p>It’s a sentiment familiar to anyone who’s tried to deploy AI inside a large enterprise. You can’t automate what you don’t understand — and most organizations still lack a unified picture of how work in their companies really gets done.</p><p>Celonis’ answer, showcased across three days at the company’s annual event, was less about new tech acronyms and more about connective tissue: how to make AI fit within the messy, living processes that drive business. The company framed it as achieving a real “Return on AI (ROAI)” — measurable impact that comes only when intelligence is grounded in process context.</p><h3><b>A living model of how the enterprise works</b></h3><p>At the heart of the keynote was what Rinke called a “living digital twin of your operations.” Celonis has been building toward this moment for years — but this was the first time the company made clear how far that concept has evolved.</p><p>“We start by freeing the process,” said Rinke. “Freeing it from the restrictions of your current legacy systems.” <a href="https://www.celonis.com/platform/datacore">Data Core</a>, Celonis’ data infrastructure, extracts raw data from source systems. It’s capable of querying billions of records in near real time with sub-minute refresh — extending visibility beyond traditional systems of record.</p><p>Built on this foundation, the <a href="https://www.celonis.com/platform#it-all-starts-with-data--business-context">Process Intelligence Graph</a> sits at the center of the Celonis Platform. It’s a system-agnostic, graph-based model that unifies data across systems, apps, and even devices, including task-mining data that captures clicks, spreadsheets, and browser activity. It combines this data with business context—business rules, KPIs, benchmarks, and exceptions. Every transaction, rule, and process interaction becomes part of a continuously updated replica that reflects how the organization actually operates.</p><p>On top of the Graph, the company’s new Build Experience allows organizations to analyze, design, and operate AI-driven, composable processes — integrating AI where it delivers business impact, not just technical demos:</p><ul><li><p>Analyze where processes stall or repeat</p></li><li><p>Design the future state, setting outcomes, guardrails, and AI touchpoints</p></li><li><p>Operate with humans, systems, and AI agents working in sync — now orchestrated through a generally available <a href="https://www.celonis.com/platform/process-improvement">Orchestration Engine</a> that can trigger and monitor every step in one flow</p></li></ul><p>It’s a deliberate shift from discovery-driven AI pilots to outcome-driven AI operations — and a blueprint for orchestrating agentic AI, where human teams, systems, and autonomous agents work together through shared process context rather than in silos.</p><h3><b>Real-world proof: Mercedes-Benz, Vinmar, and Uniper</b></h3><p>The Celosphere stage offered real proof of theCelonisPlatform in action, through live stories from customers already building on it.</p><p><a href="https://www.celonis.com/news/press/mercedes-benz-accelerates-ai-driven-transformation-with-celonis">Mercedes-Benz</a> shared how process intelligence became their “connective tissue” during the semiconductor crisis. “We had data everywhere — plants, suppliers, logistics,” recalled Dr. Jörg Burzer, Member of the Board of Management of Mercedes-Benz Group AG. “What we didn’t have was a way to see it together. Celonis helped us connect those dots fast enough to act.”</p><p>The partnership has since expanded across eight of the company’s ten most critical processes, from supply chain to quality to after-sales. But what impressed the audience wasn’t just the scale — it was the cultural shift.</p><p>“If you show data in context, and let teams visualize processes, you also change the culture,” Burzer said. “It’s not just process transformation — it’s people transformation.”</p><p>At <a href="https://www.google.com/url?q=https://www.celonis.com/news/press/vinmar-collaborates-with-celonis-to-streamline-its-global-supply-chains-with-ai-and-process-intelligence&amp;sa=D&amp;source=docs&amp;ust=1762541119653587&amp;usg=AOvVaw0Zm4sDKnt224j9eNG94LGA">Vinmar</a>, CEO Vishal Baid described Celonis as “the foundation of our automation and AI strategy.” His global plastics distribution business has already automated its entire order-to-cash process for a $3 B unit, achieving a 40 % productivity lift. But Baid wasn’t there to just celebrate finished work — he was looking ahead. </p><p>“Now we’re tackling the non-algorithmic stuff,” he said. “Matching purchase and sales orders sounds simple until you have thousands of edge cases. We’re building an AI agent that can do that allocation intelligently. That’s the next frontier.”</p><p>And in the energy sector, <a href="https://www.celonis.com/news/press/celonis-collaborates-with-uniper-and-microsoft-to-drive-digital-transformation-in-the-energy-sector">Uniper</a>, with partner Microsoft, demonstrated how process-aware AI copilots are already reshaping operations. Using Celonis and Microsoft’s AI stack, Uniper can predict when hydropower plants will need maintenance — and cluster those jobs to reduce downtime and emissions.</p><p>“Each technician, each part, each system plays a role in a living process,” said Hans Berg, Uniper’s CIO. “The human can’t see all of it. But process intelligence can — and it can nudge the system toward the best outcome.”</p><p>Agnes Heftberger, CVP &amp; CEO, Microsoft Germany &amp; Austria, who joined Berg on stage, summed it up crisply:</p><p>“The hard part isn’t building AI features — it’s scaling them responsibly,” she explained. “You need to marry intelligence with the beating heart of the company: its processes.”</p><p>Across the global community, Celonis reports <a href="https://www.celonis.com/news/press/celonis-customers-drive-tangible-business-outcomes-with-enterprise-ai-powered-by-process-intelligence">more than $8 billion in realized business value</a>and over 120 certified value champions — proof that process intelligence is driving measurable impact far beyond pilots. Rinke called it “the early proof points of a true return on AI.” </p><h3><b>From closed systems to composable intelligence</b></h3><p>Celosphere 2025 marked a shift from architecture to interoperability — from defining enterprise AI to making it work across boundaries.</p><p>Rinke’s vision for the future is unapologetically open: “Good things grow from open ecosystems,” he said. That philosophy is taking shape through deeper platform integrations — including <a href="https://www.celonis.com/news/press/celonis-provides-process-intelligence-to-microsoft-fabric-customers-to-enable-effective-ai-acceleration-at-scale">Microsoft Fabric</a>, <a href="https://www.celonis.com/news/press/celonis-partners-with-databricks-to-power-enterprise-ai-that-continuously-improves-business-operations">Databricks</a>, and <a href="https://www.celonis.com/news/press/bloomfilter-unveils-agent-miner-app-to-observe-govern-agents">Bloomfilter</a> — with zero-copy, bidirectional lakehouse access that lets customers query process data in place with minimal latency. The company also announced MCP Server support for embedding the Process Intelligence Graph directly into agentic AI platforms like Amazon Bedrock and Microsoft Copilot Studio.</p><p>These updates make “composable enterprise AI” tangible — organizations can now assemble and govern AI solutions across ecosystems rather than being locked into any single vendor.</p><p>Rather than competing on who has the “best agent,” the message was that enterprise AI will thrive when agents work together through shared context and models that mirror how businesses actually run.</p><p>“Every vendor is bringing out their own agent,” Rinke said. “But each one is limited to that vendor’s world. If they can’t work together, they can’t work for you. That’s what process intelligence fixes.”</p><p>The idea drew sustained applause. For companies juggling multiple cloud platforms, ERPs, and data tools, composability isn’t just elegant; it’s survival.</p><h3><b>Beyond operations: data, democracy, and direction</b></h3><p>The closing moments of the keynote took an unexpected turn — from enterprise architecture to human courage. Venezuelan opposition leader and Nobel Peace Prize winner María Corina Machado joined live via satellite to share how her movement used data, encrypted apps, and civic coordination to expose election fraud and mobilize millions.</p><p>It was a powerful contrast: the same principles — transparency, accountability, context — at work in both business and democracy.</p><p>“Technology can be a weapon or a liberator,” Machado said. “It depends on who holds the context.”</p><p>Her words landed with weight in a room full of people used to talking about data, systems, and governance — a reminder that context isn’t just technical, it’s human.</p><h3><b>Why this year mattered</b></h3><p>Celosphere 2025 marked a shift in how enterprises approach AI — from experimentation to results grounded in process intelligence. The shift was evident in both tone and technology, with a more powerful Data Core, enhanced Process Intelligence Graph, and new Build Experience. But the deeper takeaway was philosophical: AI only scales when it’s grounded in how people and systems actually work together.</p><p>Celonis president Carsten Thoma was candid in acknowledging that early process-mining projects often “stormed in with discovery” before understanding organizational value — a lesson that now defines the company’s measured, pragmatic approach to enterprise AI.</p><p>Rinke put it best near the end of his keynote:</p><p>“We’re not just automating steps,” he said. “We’re building enterprises that can adapt instantly, innovate freely, and improve continuously.”</p><p><b><i>Missed it? </i></b><a href="https://www.celonis.com/events/celosphere/2025"><b><i>Catch up with all the highlights from Celosphere 2025 here</i></b></a><b><i>. </i></b></p><hr/><p><i>Sponsored articles are content produced by a company that is either paying for the post or has a business relationship with VentureBeat, and they’re always clearly marked. For more information, contact </i><a href="mailto:sales@venturebeat.com"><i><u>sales@venturebeat.com</u></i></a><i>.</i></p>]]></description>
            <category>AI</category>
            <enclosure url="https://images.ctfassets.net/jdtwqhzvc2n1/uV4vD3ySkvCA0p9ZGxJhR/2ad630e0970a0d2668b8a78dd5c9e013/2025_10_04_Celosphere_3O7A5496__1_.jpg?w=300&amp;q=30" length="0" type="image/jpg"/>
        </item>
        <item>
            <title><![CDATA[6 proven lessons from the AI projects that broke before they scaled]]></title>
            <link>https://venturebeat.com/ai/6-proven-lessons-from-the-ai-projects-that-broke-before-they-scaled</link>
            <guid isPermaLink="false">56iW757sWZhfC7Y3qOLrG1</guid>
            <pubDate>Sun, 09 Nov 2025 05:00:00 GMT</pubDate>
            <description><![CDATA[<p>Companies hate to admit it, but the road to <a href="https://venturebeat.com/ai/what-could-possibly-go-wrong-if-an-enterprise-replaces-all-its-engineers">production-level AI</a> deployment is littered with proof of concepts (PoCs) that go nowhere, or failed projects that never deliver on their goals. In certain domains, there’s little tolerance for iteration, especially in something like life sciences, when the AI application is facilitating new treatments to markets or diagnosing diseases. Even slightly inaccurate analyses and assumptions early on can create sizable downstream drift in ways that can be concerning.</p><p>In analyzing dozens of AI PoCs that sailed on through to full production use — or didn’t — six common pitfalls emerge. Interestingly, it’s not usually the quality of the technology but misaligned goals, poor planning or unrealistic expectations that caused failure.

Here’s a summary of what went wrong in real-world examples and practical guidance on how to get it right.</p><h2><b>Lesson 1: A vague vision spells disaster</b></h2><p>Every <a href="https://venturebeat.com/ai/large-reasoning-models-almost-certainly-can-think">AI project</a> needs a clear, measurable goal. Without it, developers are building a solution in search of a problem. For example, in developing an AI system for a pharmaceutical manufacturer’s clinical trials, the team aimed to “optimize the trial process,” but didn’t define what that meant. Did they need to accelerate patient recruitment, reduce participant dropout rates or lower the overall trial cost? The lack of focus led to a model that was technically sound but irrelevant to the client’s most pressing operational needs.</p><p><b>Takeaway</b>: Define specific, measurable objectives upfront. Use <b>SMART criteria</b> (Specific, Measurable, Achievable, Relevant, Time-bound). For example, aim for “reduce equipment downtime by 15% within six months” rather than a vague “make things better.” Document these goals and align stakeholders early to avoid scope creep.</p><h2><b>Lesson 2: Data quality overtakes quantity</b></h2><p>Data is the <a href="https://venturebeat.com/ai/the-teacher-is-the-new-engineer-inside-the-rise-of-ai-enablement-and">lifeblood of AI</a>, but poor-quality data is poison. In one project, a retail client began with years of sales data to predict inventory needs. The catch? The dataset was riddled with inconsistencies, including missing entries, duplicate records and outdated product codes. The model performed well in testing but failed in production because it learned from noisy, unreliable data.</p><p><b>Takeaway</b>: Invest in data quality over volume. Use tools like Pandas for preprocessing and Great Expectations for data validation to <b>catch issues early</b>. Conduct exploratory data analysis (EDA) with visualizations (like Seaborn) to spot outliers or inconsistencies. Clean data is worth more than terabytes of garbage.</p><h2><b>Lesson 3: Overcomplicating model backfires</b></h2><p>Chasing technical complexity doesn&#x27;t always lead to better outcomes. For example, on a healthcare project, development initially began by creating a sophisticated convolutional neural network (CNN) to identify anomalies in medical images.</p><p>While the model was state-of-the-art, its high computational cost meant weeks of training, and its &quot;black box&quot; nature made it difficult for clinicians to trust. The application was revised to implement a simpler random forest model that not only matched the CNN&#x27;s predictive accuracy but was faster to train and far easier to interpret — a critical factor for clinical adoption.</p><p><b>Takeaway</b>: Start simple. Use straightforward algorithms like <b>random forest </b>or <b>XGBoost</b> from scikit-learn to establish a baseline. Only scale to complex models — TensorFlow-based long-short-term-memory (LSTM) networks — if the problem demands it. Prioritize explainability with tools like SHAP (SHapley Additive exPlanations) to build trust with stakeholders.</p><h2><b>Lesson 4: Ignoring deployment realities</b></h2><p>A model that shines in a Jupyter Notebook can crash in the real world. For example, a company’s initial deployment of a recommendation engine for its e-commerce platform couldn’t handle peak traffic. The model was built without scalability in mind and choked under load, causing delays and frustrated users. The oversight cost weeks of rework.</p><p><b>Takeaway</b>: Plan for production from day one. Package models in Docker containers and deploy with Kubernetes for scalability. Use TensorFlow Serving or FastAPI for efficient inference. Monitor performance with Prometheus and Grafana to catch bottlenecks early. Test under realistic conditions to ensure reliability.</p><h2><b>Lesson 5: Neglecting model maintenance</b></h2><p>AI models aren’t set-and-forget. In a financial forecasting project, the model performed well for months until market conditions shifted. Unmonitored data drift caused predictions to degrade, and the lack of a retraining pipeline meant manual fixes were needed. The project lost credibility before developers could recover.</p><p><b>Takeaway</b>: Build for the long haul. Implement monitoring for data drift using tools like Alibi Detect. Automate retraining with Apache Airflow and track experiments with MLflow. Incorporate active learning to prioritize labeling for uncertain predictions, keeping models relevant.</p><h2><b>Lesson 6: Underestimating stakeholder buy-in</b></h2><p>Technology doesn’t exist in a vacuum. A fraud detection model was technically flawless but flopped because end-users — bank employees — didn’t trust it. Without clear explanations or training, they ignored the model’s alerts, rendering it useless.</p><p><b>Takeaway</b>: Prioritize human-centric design. Use explainability tools like SHAP to make model decisions transparent. Engage stakeholders early with demos and feedback loops. Train users on how to interpret and act on AI outputs. Trust is as critical as accuracy.</p><h2><b>Best practices for success in AI projects</b></h2><p>Drawing from these failures, here’s the roadmap to get it right:</p><ul><li><p><b>Set clear goals</b>: Use SMART criteria to align teams and stakeholders.</p></li><li><p><b>Prioritize data quality</b>: Invest in cleaning, validation and EDA before modeling.</p></li><li><p><b>Start simple</b>: Build baselines with simple algorithms before scaling complexity.</p></li><li><p><b>Design for production</b>: Plan for scalability, monitoring and real-world conditions.</p></li><li><p><b>Maintain models</b>: Automate retraining and monitor for drift to stay relevant.</p></li><li><p><b>Engage stakeholders</b>: Foster trust with explainability and user training.</p></li></ul><h2><b>Building resilient AI</b></h2><p>AI’s potential is intoxicating, yet failed AI projects teach us that success isn’t just about algorithms. It’s about discipline, planning and adaptability. As AI evolves, emerging trends like federated learning for privacy-preserving models and edge AI for real-time insights will raise the bar. By learning from past mistakes, teams can build scale-out, production systems that are robust, accurate, and trusted.</p><p><i>Kavin Xavier is VP of AI solutions at </i><a href="http://www.capestart.com"><i><u>CapeStart</u></i></a><i>. </i></p><p><i>Read more from our </i><a href="https://venturebeat.com/datadecisionmakers"><i>guest writers</i></a><i>. Or, consider submitting a post of your own! See our </i><a href="https://venturebeat.com/guest-posts"><i>guidelines here</i></a><i>. </i></p>]]></description>
            <category>AI</category>
            <category>DataDecisionMakers</category>
            <enclosure url="https://images.ctfassets.net/jdtwqhzvc2n1/9vQIPNRompjXxzQ45ADsZ/7b6c888af7d577cb86f1e040c1b56c7f/AI_failures.png?w=300&amp;q=30" length="0" type="image/png"/>
        </item>
        <item>
            <title><![CDATA[What could possibly go wrong if an enterprise replaces all its engineers with AI? ]]></title>
            <link>https://venturebeat.com/ai/what-could-possibly-go-wrong-if-an-enterprise-replaces-all-its-engineers</link>
            <guid isPermaLink="false">2rVQd896cNBGGNa1iVXVdN</guid>
            <pubDate>Sat, 08 Nov 2025 05:00:00 GMT</pubDate>
            <description><![CDATA[<p>AI coding, <a href="https://x.com/karpathy/status/1886192184808149383?lang=en"><u>vibe coding</u></a> and <a href="https://venturebeat.com/ai/vibe-coding-is-dead-agentic-swarm-coding-is-the-new-enterprise-moat"><u>agentic swarm</u></a> have made a dramatic and astonishing recent market entrance, with the AI Code Tools market valued at <a href="https://www.gminsights.com/industry-analysis/ai-code-tools-market"><u>$4.8 billion and expected to grow at a 23% annual rate</u></a>.  Enterprises are grappling with AI coding agents and what do about expensive human coders. </p><p>They don’t lack for advice.  OpenAI’s CEO estimates that AI can perform <a href="https://economictimes.indiatimes.com/tech/artificial-intelligence/openai-ceo-sam-altman-says-ai-will-gradually-reduce-need-for-software-engineers/articleshow/119303412.cms"><u>over 50% of what human engineers can do</u></a>.  Six months ago, Anthropic’s CEO said that AI <a href="https://www.businessinsider.com/anthropic-ceo-ai-90-percent-code-3-to-6-months-2025-3"><u>would write 90% of code</u></a> in six months.  Meta’s CEO said he believes AI will <a href="https://www.forbes.com/sites/quickerbettertech/2025/01/26/business-tech-news-zuckerberg-says-ai-will-replace-mid-level-engineers-soon/"><u>replace mid-level engineers “soon.”</u></a> Judging by <a href="https://fortune.com/2025/07/27/artificial-intelligence-skills-18000-salaries-28-percent/"><u>recent tech layoffs</u></a>, it seems many executives are embracing that advice.</p><p>Software engineers and data scientists are among the most expensive salary lines at many companies, and business and technology leaders may be tempted to replace them with AI. However, recent high-profile failures demonstrate that engineers and their expertise remain valuable, even as AI continues to make impressive advances.</p><h2>SaaStr disaster</h2><p>Jason Lemkin, a tech entrepreneur and founder of the SaaS community SaaStr, has been <a href="https://venturebeat.com/ai/is-vibe-coding-ruining-a-generation-of-engineers">vibe coding</a> a SaaS networking app and live-tweeting his experience. About a week into his adventure, he admitted to his audience that something was going very wrong.  The AI <a href="https://www.tomshardware.com/tech-industry/artificial-intelligence/ai-coding-platform-goes-rogue-during-code-freeze-and-deletes-entire-company-database-replit-ceo-apologizes-after-ai-engine-says-it-made-a-catastrophic-error-in-judgment-and-destroyed-all-production-data"><u>deleted his production database</u></a> despite his request for a “code and action freeze.” This is the kind of mistake no experienced (or even semi-experienced) engineer would make.</p><p>If you have ever worked in a professional <a href="https://venturebeat.com/ai/replacing-coders-with-ai-why-bill-gates-sam-altman-and-experience-say-you">coding environment</a>, you know to split your development environment from production. Junior engineers are given full access to the development environment (it’s crucial for productivity), but access to production is given on a limited need-to-have basis to a few of the most trusted senior engineers. The reason for restricted access is precisely for this use case: To prevent a junior engineer from accidentally taking down production. </p><p>In fact, Lemkin made two mistakes. First: for something as critical as production, access to unreliable actors is just never granted (we don’t rely on asking a junior engineer or AI nicely). Second, he never separated development from production.  In a subsequent public conversation on LinkedIn, Lemkin, who holds a Stanford Executive MBA and Berkeley JD, admitted that <a href="https://www.linkedin.com/posts/hugo-bowne-anderson-045939a5_his-reply-concerns-me-more-than-the-original-activity-7353059080237731840-trVa/"><u>he was not aware of the best practice</u></a> of splitting development and production databases.</p><p>The takeaway for business leaders is that standard software engineering best practices still apply. We should incorporate at least the same safety constraints for AI as we do for junior engineers. Arguably, we should go beyond that and treat AI slightly adversarially: There are reports that, like HAL in Stanley Kubrick&#x27;s <i>2001: A Space Odyssey</i>, the AI might try to <a href="https://www.reddit.com/r/OpenAI/comments/1ffwbp5/wakeup_moment_during_safety_testing_o1_broke_out/"><u>break out of its sandbox environment</u></a> to accomplish a task. With more vibe coding, having experienced engineers who understand how complex software systems work and can implement the proper guardrails in development processes will become increasingly necessary.</p><h2>Tea hack</h2><p>Sean Cook is the Founder and CEO of Tea, a mobile application launched in 2023, designed to help women date safely. In the summer of 2025, they were “hacked&quot;: 72,000 images, including 13,000 verification photos and images of government IDs, were <a href="https://www.nbcnews.com/tech/social-media/tea-app-hacked-13000-photos-leaked-4chan-call-action-rcna221139"><u>leaked onto the public discussion forum 4chan</u></a>. Worse, Tea’s own privacy policy promises that these images would be &quot;deleted immediately&quot; after users were authenticated, meaning they potentially <a href="https://www.bbc.com/news/articles/c7vl57n74pqo"><u>violated their own privacy policy</u></a>.</p><p>I use “hacked” in air-quotes because the incident stems less from the cleverness of the attackers than the ineptitude of the defenders. In addition to violating their own data policies, the app left a Firebase storage bucket unsecured, <a href="https://www.engadget.com/cybersecurity/tea-app-suffers-breach-exposing-thousands-of-user-images-190731414.html"><u>exposing sensiztive user data to the public internet</u></a>. It’s the digital equivalent of locking your front door but leaving your back open with your family jewelry ostentatiously hanging on the doorknob.</p><p>While we don’t know if the root cause was vibe coding, the Tea hack highlights catastrophic breaches stemming from basic, preventable security errors due to poor development processes. It is the kind of vulnerability that a disciplined and thoughtful engineering process addresses. Unfortunately, the relentless push of financial pressures, where a “lean,” “move fast and break things” culture is the polar opposite, and vibe coding only exacerbates the problem.</p><h2>How to safely adopt AI coding agents?</h2><p>So how should enterprise and technology leaders think about AI? First, this is not a call to abandon AI for coding.  <a href="https://mitsloan.mit.edu/ideas-made-to-matter/how-generative-ai-affects-highly-skilled-workers#:~:text=AI%20helped%20newer%20employees%20with,important%20next%20step%2C%20he%20said."><u>An MIT Sloan study</u></a> estimated AI leads to productivity gains between 8% and 39%, while a <a href="https://www.mckinsey.com/capabilities/mckinsey-digital/our-insights/unleashing-developer-productivity-with-generative-ai"><u>McKinsey study</u></a> found a 10% to 50% reduction in time to task completion with the use of AI. </p><p>However, we should be aware of the risks. The old lessons of software engineering don’t go away. These include many tried-and-true best practices, such as version control, automated unit and integration tests, safety checks like SAST/DAST, separating development and production environments, code review and secrets management. If anything, they become more salient.</p><p>AI can generate code 100 times faster than humans can type, fostering an illusion of productivity that is a tempting siren call for many executives.  However, the quality of the rapidly generated AI shlop is still up for debate. To develop complex production systems, enterprises need the thoughtful, seasoned experience of human engineers.</p><p><i>Tianhui Michael Li is president at Pragmatic Institute and the founder and president of The Data Incubator. </i></p><p><i>Read more from our </i><a href="https://venturebeat.com/datadecisionmakers"><i>guest writers</i></a><i>. Or, consider submitting a post of your own! See our </i><a href="https://venturebeat.com/guest-posts"><i>guidelines here</i></a><i>. </i></p>]]></description>
            <author>tianhui.michael.li@gmail.com (Michael Li, Pragmatic Institute)</author>
            <category>AI</category>
            <category>DataDecisionMakers</category>
            <category>Programming &amp; Development</category>
            <enclosure url="https://images.ctfassets.net/jdtwqhzvc2n1/1PiLMdaIC9zNld7zUb6c5A/6085df3002ea17a77730da954ae69c07/What_could_possibly_go_wrong.png?w=300&amp;q=30" length="0" type="image/png"/>
        </item>
    </channel>
</rss>