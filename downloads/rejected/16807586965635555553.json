{
  "id": "16807586965635555553",
  "title": "Streaming datasets: 100x More Efficient",
  "url": "https://huggingface.co/blog/streaming-datasets",
  "published_date": "2025-10-27T00:00:00Z",
  "author": null,
  "summary": null,
  "source_type": "rss",
  "content_html": "We boosted loaddataset('dataset', streamingTrue), streaming datasets without downloading them with one line of code! Start training on multi-TB datasets immediately, without complex setups, downloading, no \"disk out of space\", or 429 stop requesting! errors.It's super fast! Outrunning our local SSDs when training on 64xH100 with 256 workers downloading data. We've improved streaming to have 100x fewer requests, 10 faster data resolution 2x sample/sec, 0 worker crashes at 256 concurrent workers. Loading data, especially at the terabyte scale, is a major pain in any machine learning workflow. We suffered this while training SmolLM3, at one point we had to wait 3 hours before each run to download enough data. Streaming has always been possible in the datasets library, but large scale training with massive datasets remained a challenge. That changes today . We spent a few months improving the backend, focusing on streaming datasets to make it faster and more efficient. First things first: our changes are backwards compatible. You can still stream any dataset from the Hub with the same simple streamingTrue flag. It's as easy as ever. Thousands of AI developers around the world use datasets daily; they should just get improved performance with zero extra work. Streaming was a lifesaver to quickly understand a dataset, but to train models, people were usually downloading the data locally, or using a cloud storage service such as S3. That's what we were doing for training SmolVLM, we had all of our data on S3 and were streaming directly from it. We wanted to change that, so we decided to use streaming from the Hub when we were developing nanoVLM. Soon we found a big issue: our test run generated over 100,000 requests in under a minute, which got our IP blocked by the Hub! This happened because every DataLoader worker was initializing the dataset independently. As we dug deeper, we found that this creates a storm of redundant requests, many of which are unnecessary. Our changes ultimately reduced startup requests by a factor of 100. In total, our improvements delivered: So, what changed? We focused on two phases: startup and streaming. 1. Startup The initial resolution of data files was creating a ton of requests. We made two major changes: 2. Streaming To improve throughput during streaming itself, we've introduced two new features: This is how we can increase the minimum request size when streaming from 32MiB (default) to 128MiB and configure prefetching: Together, these improvements can double your data throughput, allowing you to train faster and more efficiently. Hugging Face uses Xet: a dedupe-based storage which enables fast deduped uploads and downloads. Unlike traditional remote storage, data transfers are faster on Xet because duplicated data is only transferred once. For example: uploading a large scale dataset to Hugging Face leverages Xet which accelerates uploads. Once the dataset is uploaded, it can be streamed right away. Deduplication for Parquet is enabled through Parquet Content Defined Chunking (CDC). Thanks to Parquet CDC and Xet deduplication, uploading datasets on Hugging Face is faster than on any traditional remote storage. This is supported by our pysparkhuggingface package, a Spark Data Source to read/write HF datasets. It includes Parquet CDC and Xet support, accelerating data transfers on HF dramatically. Some data file formats are not supported in datasets, and sometimes there is a need for more control, so we made it easy to build custom streaming pipelines. This has been battle-tested in the LeRobot library to sample video frames, and in the WebDataset library to stream TAR archives. We improved the HfFileSystem in the huggingfacehub library to efficiently read files from remote Hugging Face dataset repositories and stream data: Passing a HfFileSystem to a torch DataLoader reuses the cached results from .ls() and .glob() which eliminates the need for additional requests when listing data files. We're now using these streaming enhancements in nanoVLM to train the next generation of SmolVLMs. With these tweaks, we achieve better performance from streaming than from training on our cluster's hierarchical hard disk setup. In fact, streaming is now as fast as reading the data from local SSDs! Previously, transferring data to local SSDs was the process that used to delay our trainings by three-hours. For more details, check out our GitHub. These powerful new features landed in the datasets and huggingfacehub libraries. To take advantage of them, simply update your libraries and check out the documentation: To celebrate this, we preconcatenated and shuffled all the data sources in FineVision into FineVisionMax. You can use this single combined dataset to train your VLM no need to handle multiple datasets manually! steaming is always the way to go since generally neural nets trainings are stateful any way",
  "content_text": "We boosted loaddataset('dataset', streamingTrue), streaming datasets without downloading them with one line of code! Start training on multi-TB datasets immediately, without complex setups, downloading, no \"disk out of space\", or 429 stop requesting! errors.It's super fast! Outrunning our local SSDs when training on 64xH100 with 256 workers downloading data. We've improved streaming to have 100x fewer requests, 10 faster data resolution 2x sample/sec, 0 worker crashes at 256 concurrent workers. Loading data, especially at the terabyte scale, is a major pain in any machine learning workflow. We suffered this while training SmolLM3, at one point we had to wait 3 hours before each run to download enough data. Streaming has always been possible in the datasets library, but large scale training with massive datasets remained a challenge. That changes today . We spent a few months improving the backend, focusing on streaming datasets to make it faster and more efficient. First things first: our changes are backwards compatible. You can still stream any dataset from the Hub with the same simple streamingTrue flag. It's as easy as ever. Thousands of AI developers around the world use datasets daily; they should just get improved performance with zero extra work. Streaming was a lifesaver to quickly understand a dataset, but to train models, people were usually downloading the data locally, or using a cloud storage service such as S3. That's what we were doing for training SmolVLM, we had all of our data on S3 and were streaming directly from it. We wanted to change that, so we decided to use streaming from the Hub when we were developing nanoVLM. Soon we found a big issue: our test run generated over 100,000 requests in under a minute, which got our IP blocked by the Hub! This happened because every DataLoader worker was initializing the dataset independently. As we dug deeper, we found that this creates a storm of redundant requests, many of which are unnecessary. Our changes ultimately reduced startup requests by a factor of 100. In total, our improvements delivered: So, what changed? We focused on two phases: startup and streaming. 1. Startup The initial resolution of data files was creating a ton of requests. We made two major changes: 2. Streaming To improve throughput during streaming itself, we've introduced two new features: This is how we can increase the minimum request size when streaming from 32MiB (default) to 128MiB and configure prefetching: Together, these improvements can double your data throughput, allowing you to train faster and more efficiently. Hugging Face uses Xet: a dedupe-based storage which enables fast deduped uploads and downloads. Unlike traditional remote storage, data transfers are faster on Xet because duplicated data is only transferred once. For example: uploading a large scale dataset to Hugging Face leverages Xet which accelerates uploads. Once the dataset is uploaded, it can be streamed right away. Deduplication for Parquet is enabled through Parquet Content Defined Chunking (CDC). Thanks to Parquet CDC and Xet deduplication, uploading datasets on Hugging Face is faster than on any traditional remote storage. This is supported by our pysparkhuggingface package, a Spark Data Source to read/write HF datasets. It includes Parquet CDC and Xet support, accelerating data transfers on HF dramatically. Some data file formats are not supported in datasets, and sometimes there is a need for more control, so we made it easy to build custom streaming pipelines. This has been battle-tested in the LeRobot library to sample video frames, and in the WebDataset library to stream TAR archives. We improved the HfFileSystem in the huggingfacehub library to efficiently read files from remote Hugging Face dataset repositories and stream data: Passing a HfFileSystem to a torch DataLoader reuses the cached results from .ls() and .glob() which eliminates the need for additional requests when listing data files. We're now using these streaming enhancements in nanoVLM to train the next generation of SmolVLMs. With these tweaks, we achieve better performance from streaming than from training on our cluster's hierarchical hard disk setup. In fact, streaming is now as fast as reading the data from local SSDs! Previously, transferring data to local SSDs was the process that used to delay our trainings by three-hours. For more details, check out our GitHub. These powerful new features landed in the datasets and huggingfacehub libraries. To take advantage of them, simply update your libraries and check out the documentation: To celebrate this, we preconcatenated and shuffled all the data sources in FineVision into FineVisionMax. You can use this single combined dataset to train your VLM no need to handle multiple datasets manually! steaming is always the way to go since generally neural nets trainings are stateful any way"
}